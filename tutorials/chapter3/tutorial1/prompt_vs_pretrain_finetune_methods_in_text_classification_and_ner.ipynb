{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 3 Tutorial #1: Prompt vs. Pre-Train/Fine-Tune Methods in Text Classification and NER\n",
    "\n",
    "This tutorial will show how prompt-based learning can achieve superior results compared to head-based fine-tuning in data-sparse training situations. While prompting may not consistently outperform fine-tuning with more significant amounts of data and longer training cycles, prompt learning has a significant advantage in its efficiency. It allows LLMs to be adapted to new tasks more quickly and at a lower cost. It is far less dependent on data volume and quality, and fewer data means less expensive computation.\n",
    "\n",
    "Our experiment will directly compare the zero-shot and few-shot capabilities of the pre-train/fine-tune and prompt-based learning approaches in their application to text classification and named-entity recognition. We adopt BERT as the basis for our fine-tuning exercises for this test. Using PyTorch, supplemented with OpenPrompt for the prompt-based portion, we will iteratively refine our BERT models with larger and larger subsets of the training data, predicting on the validation sets at regular intervals to show how the model is responding to few-shot learning. Finally, we will compare learning curves for the two tuning approaches for each NLP task and discuss the implications."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5Cp4yv_6iVW_"
   },
   "source": [
    "## Installation and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "PHC2u6uR6iWU"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting transformers==4.19.0\n",
      "  Downloading transformers-4.19.0-py3-none-any.whl.metadata (73 kB)\n",
      "Requirement already satisfied: filelock in /home/maruanottoni/miniconda3/envs/nn/lib/python3.8/site-packages (from transformers==4.19.0) (3.16.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.1.0 in /home/maruanottoni/miniconda3/envs/nn/lib/python3.8/site-packages (from transformers==4.19.0) (0.26.2)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/maruanottoni/miniconda3/envs/nn/lib/python3.8/site-packages (from transformers==4.19.0) (1.24.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/maruanottoni/miniconda3/envs/nn/lib/python3.8/site-packages (from transformers==4.19.0) (24.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/maruanottoni/miniconda3/envs/nn/lib/python3.8/site-packages (from transformers==4.19.0) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/maruanottoni/miniconda3/envs/nn/lib/python3.8/site-packages (from transformers==4.19.0) (2024.9.11)\n",
      "Requirement already satisfied: requests in /home/maruanottoni/miniconda3/envs/nn/lib/python3.8/site-packages (from transformers==4.19.0) (2.32.3)\n",
      "Collecting tokenizers!=0.11.3,<0.13,>=0.11.1 (from transformers==4.19.0)\n",
      "  Downloading tokenizers-0.12.1-cp38-cp38-manylinux_2_12_x86_64.manylinux2010_x86_64.whl.metadata (6.5 kB)\n",
      "Requirement already satisfied: tqdm>=4.27 in /home/maruanottoni/miniconda3/envs/nn/lib/python3.8/site-packages (from transformers==4.19.0) (4.66.5)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /home/maruanottoni/miniconda3/envs/nn/lib/python3.8/site-packages (from huggingface-hub<1.0,>=0.1.0->transformers==4.19.0) (2024.9.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/maruanottoni/miniconda3/envs/nn/lib/python3.8/site-packages (from huggingface-hub<1.0,>=0.1.0->transformers==4.19.0) (4.12.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/maruanottoni/miniconda3/envs/nn/lib/python3.8/site-packages (from requests->transformers==4.19.0) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/maruanottoni/miniconda3/envs/nn/lib/python3.8/site-packages (from requests->transformers==4.19.0) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/maruanottoni/miniconda3/envs/nn/lib/python3.8/site-packages (from requests->transformers==4.19.0) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/maruanottoni/miniconda3/envs/nn/lib/python3.8/site-packages (from requests->transformers==4.19.0) (2024.8.30)\n",
      "Downloading transformers-4.19.0-py3-none-any.whl (4.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.2/4.2 MB\u001b[0m \u001b[31m18.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading tokenizers-0.12.1-cp38-cp38-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (6.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.6/6.6 MB\u001b[0m \u001b[31m38.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: tokenizers, transformers\n",
      "  Attempting uninstall: tokenizers\n",
      "    Found existing installation: tokenizers 0.20.3\n",
      "    Uninstalling tokenizers-0.20.3:\n",
      "      Successfully uninstalled tokenizers-0.20.3\n",
      "  Attempting uninstall: transformers\n",
      "    Found existing installation: transformers 4.46.2\n",
      "    Uninstalling transformers-4.46.2:\n",
      "      Successfully uninstalled transformers-4.46.2\n",
      "Successfully installed tokenizers-0.12.1 transformers-4.19.0\n",
      "Requirement already satisfied: datasets in /home/maruanottoni/miniconda3/envs/nn/lib/python3.8/site-packages (3.1.0)\n",
      "Requirement already satisfied: filelock in /home/maruanottoni/miniconda3/envs/nn/lib/python3.8/site-packages (from datasets) (3.16.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/maruanottoni/miniconda3/envs/nn/lib/python3.8/site-packages (from datasets) (1.24.4)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /home/maruanottoni/miniconda3/envs/nn/lib/python3.8/site-packages (from datasets) (16.1.0)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /home/maruanottoni/miniconda3/envs/nn/lib/python3.8/site-packages (from datasets) (0.3.1.1)\n",
      "Requirement already satisfied: pandas in /home/maruanottoni/miniconda3/envs/nn/lib/python3.8/site-packages (from datasets) (2.0.3)\n",
      "Requirement already satisfied: requests>=2.32.2 in /home/maruanottoni/miniconda3/envs/nn/lib/python3.8/site-packages (from datasets) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.66.3 in /home/maruanottoni/miniconda3/envs/nn/lib/python3.8/site-packages (from datasets) (4.66.5)\n",
      "Requirement already satisfied: xxhash in /home/maruanottoni/miniconda3/envs/nn/lib/python3.8/site-packages (from datasets) (3.5.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in /home/maruanottoni/miniconda3/envs/nn/lib/python3.8/site-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2024.9.0,>=2023.1.0 in /home/maruanottoni/miniconda3/envs/nn/lib/python3.8/site-packages (from fsspec[http]<=2024.9.0,>=2023.1.0->datasets) (2024.9.0)\n",
      "Requirement already satisfied: aiohttp in /home/maruanottoni/miniconda3/envs/nn/lib/python3.8/site-packages (from datasets) (3.10.10)\n",
      "Requirement already satisfied: huggingface-hub>=0.23.0 in /home/maruanottoni/miniconda3/envs/nn/lib/python3.8/site-packages (from datasets) (0.26.2)\n",
      "Requirement already satisfied: packaging in /home/maruanottoni/miniconda3/envs/nn/lib/python3.8/site-packages (from datasets) (24.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/maruanottoni/miniconda3/envs/nn/lib/python3.8/site-packages (from datasets) (6.0.2)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /home/maruanottoni/miniconda3/envs/nn/lib/python3.8/site-packages (from aiohttp->datasets) (2.4.3)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /home/maruanottoni/miniconda3/envs/nn/lib/python3.8/site-packages (from aiohttp->datasets) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /home/maruanottoni/miniconda3/envs/nn/lib/python3.8/site-packages (from aiohttp->datasets) (24.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /home/maruanottoni/miniconda3/envs/nn/lib/python3.8/site-packages (from aiohttp->datasets) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /home/maruanottoni/miniconda3/envs/nn/lib/python3.8/site-packages (from aiohttp->datasets) (6.1.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.12.0 in /home/maruanottoni/miniconda3/envs/nn/lib/python3.8/site-packages (from aiohttp->datasets) (1.15.2)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0 in /home/maruanottoni/miniconda3/envs/nn/lib/python3.8/site-packages (from aiohttp->datasets) (4.0.3)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/maruanottoni/miniconda3/envs/nn/lib/python3.8/site-packages (from huggingface-hub>=0.23.0->datasets) (4.12.2)\n",
      "Collecting dill<0.3.9,>=0.3.0 (from datasets)\n",
      "  Using cached dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/maruanottoni/miniconda3/envs/nn/lib/python3.8/site-packages (from requests>=2.32.2->datasets) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/maruanottoni/miniconda3/envs/nn/lib/python3.8/site-packages (from requests>=2.32.2->datasets) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/maruanottoni/miniconda3/envs/nn/lib/python3.8/site-packages (from requests>=2.32.2->datasets) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/maruanottoni/miniconda3/envs/nn/lib/python3.8/site-packages (from requests>=2.32.2->datasets) (2024.8.30)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /home/maruanottoni/miniconda3/envs/nn/lib/python3.8/site-packages (from pandas->datasets) (2.9.0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/maruanottoni/miniconda3/envs/nn/lib/python3.8/site-packages (from pandas->datasets) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /home/maruanottoni/miniconda3/envs/nn/lib/python3.8/site-packages (from pandas->datasets) (2024.2)\n",
      "Requirement already satisfied: six>=1.5 in /home/maruanottoni/miniconda3/envs/nn/lib/python3.8/site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /home/maruanottoni/miniconda3/envs/nn/lib/python3.8/site-packages (from yarl<2.0,>=1.12.0->aiohttp->datasets) (0.2.0)\n",
      "Using cached dill-0.3.8-py3-none-any.whl (116 kB)\n",
      "Installing collected packages: dill\n",
      "  Attempting uninstall: dill\n",
      "    Found existing installation: dill 0.3.1.1\n",
      "    Uninstalling dill-0.3.1.1:\n",
      "      Successfully uninstalled dill-0.3.1.1\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "apache-beam 2.60.0 requires dill<0.3.2,>=0.3.1.1, but you have dill 0.3.8 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed dill-0.3.8\n",
      "Collecting openprompt\n",
      "  Downloading openprompt-1.0.1-py3-none-any.whl.metadata (10 kB)\n",
      "Requirement already satisfied: transformers>=4.10.0 in /home/maruanottoni/miniconda3/envs/nn/lib/python3.8/site-packages (from openprompt) (4.19.0)\n",
      "Collecting sentencepiece==0.1.96 (from openprompt)\n",
      "  Downloading sentencepiece-0.1.96-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (10 kB)\n",
      "Requirement already satisfied: tqdm>=4.62.2 in /home/maruanottoni/miniconda3/envs/nn/lib/python3.8/site-packages (from openprompt) (4.66.5)\n",
      "Collecting tensorboardX (from openprompt)\n",
      "  Using cached tensorboardX-2.6.2.2-py2.py3-none-any.whl.metadata (5.8 kB)\n",
      "Collecting nltk (from openprompt)\n",
      "  Downloading nltk-3.9.1-py3-none-any.whl.metadata (2.9 kB)\n",
      "Collecting yacs (from openprompt)\n",
      "  Downloading yacs-0.1.8-py3-none-any.whl.metadata (639 bytes)\n",
      "Requirement already satisfied: dill in /home/maruanottoni/miniconda3/envs/nn/lib/python3.8/site-packages (from openprompt) (0.3.8)\n",
      "Requirement already satisfied: datasets in /home/maruanottoni/miniconda3/envs/nn/lib/python3.8/site-packages (from openprompt) (3.1.0)\n",
      "Collecting rouge==1.0.0 (from openprompt)\n",
      "  Downloading rouge-1.0.0-py3-none-any.whl.metadata (4.0 kB)\n",
      "Requirement already satisfied: pyarrow in /home/maruanottoni/miniconda3/envs/nn/lib/python3.8/site-packages (from openprompt) (16.1.0)\n",
      "Requirement already satisfied: scipy in /home/maruanottoni/miniconda3/envs/nn/lib/python3.8/site-packages (from openprompt) (1.8.1)\n",
      "Requirement already satisfied: six in /home/maruanottoni/miniconda3/envs/nn/lib/python3.8/site-packages (from rouge==1.0.0->openprompt) (1.16.0)\n",
      "Requirement already satisfied: filelock in /home/maruanottoni/miniconda3/envs/nn/lib/python3.8/site-packages (from transformers>=4.10.0->openprompt) (3.16.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.1.0 in /home/maruanottoni/miniconda3/envs/nn/lib/python3.8/site-packages (from transformers>=4.10.0->openprompt) (0.26.2)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/maruanottoni/miniconda3/envs/nn/lib/python3.8/site-packages (from transformers>=4.10.0->openprompt) (1.24.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/maruanottoni/miniconda3/envs/nn/lib/python3.8/site-packages (from transformers>=4.10.0->openprompt) (24.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/maruanottoni/miniconda3/envs/nn/lib/python3.8/site-packages (from transformers>=4.10.0->openprompt) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/maruanottoni/miniconda3/envs/nn/lib/python3.8/site-packages (from transformers>=4.10.0->openprompt) (2024.9.11)\n",
      "Requirement already satisfied: requests in /home/maruanottoni/miniconda3/envs/nn/lib/python3.8/site-packages (from transformers>=4.10.0->openprompt) (2.32.3)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.13,>=0.11.1 in /home/maruanottoni/miniconda3/envs/nn/lib/python3.8/site-packages (from transformers>=4.10.0->openprompt) (0.12.1)\n",
      "Requirement already satisfied: pandas in /home/maruanottoni/miniconda3/envs/nn/lib/python3.8/site-packages (from datasets->openprompt) (2.0.3)\n",
      "Requirement already satisfied: xxhash in /home/maruanottoni/miniconda3/envs/nn/lib/python3.8/site-packages (from datasets->openprompt) (3.5.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in /home/maruanottoni/miniconda3/envs/nn/lib/python3.8/site-packages (from datasets->openprompt) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2024.9.0,>=2023.1.0 in /home/maruanottoni/miniconda3/envs/nn/lib/python3.8/site-packages (from fsspec[http]<=2024.9.0,>=2023.1.0->datasets->openprompt) (2024.9.0)\n",
      "Requirement already satisfied: aiohttp in /home/maruanottoni/miniconda3/envs/nn/lib/python3.8/site-packages (from datasets->openprompt) (3.10.10)\n",
      "Collecting click (from nltk->openprompt)\n",
      "  Downloading click-8.1.7-py3-none-any.whl.metadata (3.0 kB)\n",
      "Requirement already satisfied: joblib in /home/maruanottoni/miniconda3/envs/nn/lib/python3.8/site-packages (from nltk->openprompt) (1.4.2)\n",
      "Requirement already satisfied: protobuf>=3.20 in /home/maruanottoni/miniconda3/envs/nn/lib/python3.8/site-packages (from tensorboardX->openprompt) (4.25.5)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /home/maruanottoni/miniconda3/envs/nn/lib/python3.8/site-packages (from aiohttp->datasets->openprompt) (2.4.3)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /home/maruanottoni/miniconda3/envs/nn/lib/python3.8/site-packages (from aiohttp->datasets->openprompt) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /home/maruanottoni/miniconda3/envs/nn/lib/python3.8/site-packages (from aiohttp->datasets->openprompt) (24.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /home/maruanottoni/miniconda3/envs/nn/lib/python3.8/site-packages (from aiohttp->datasets->openprompt) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /home/maruanottoni/miniconda3/envs/nn/lib/python3.8/site-packages (from aiohttp->datasets->openprompt) (6.1.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.12.0 in /home/maruanottoni/miniconda3/envs/nn/lib/python3.8/site-packages (from aiohttp->datasets->openprompt) (1.15.2)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0 in /home/maruanottoni/miniconda3/envs/nn/lib/python3.8/site-packages (from aiohttp->datasets->openprompt) (4.0.3)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/maruanottoni/miniconda3/envs/nn/lib/python3.8/site-packages (from huggingface-hub<1.0,>=0.1.0->transformers>=4.10.0->openprompt) (4.12.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/maruanottoni/miniconda3/envs/nn/lib/python3.8/site-packages (from requests->transformers>=4.10.0->openprompt) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/maruanottoni/miniconda3/envs/nn/lib/python3.8/site-packages (from requests->transformers>=4.10.0->openprompt) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/maruanottoni/miniconda3/envs/nn/lib/python3.8/site-packages (from requests->transformers>=4.10.0->openprompt) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/maruanottoni/miniconda3/envs/nn/lib/python3.8/site-packages (from requests->transformers>=4.10.0->openprompt) (2024.8.30)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /home/maruanottoni/miniconda3/envs/nn/lib/python3.8/site-packages (from pandas->datasets->openprompt) (2.9.0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/maruanottoni/miniconda3/envs/nn/lib/python3.8/site-packages (from pandas->datasets->openprompt) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /home/maruanottoni/miniconda3/envs/nn/lib/python3.8/site-packages (from pandas->datasets->openprompt) (2024.2)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /home/maruanottoni/miniconda3/envs/nn/lib/python3.8/site-packages (from yarl<2.0,>=1.12.0->aiohttp->datasets->openprompt) (0.2.0)\n",
      "Downloading openprompt-1.0.1-py3-none-any.whl (146 kB)\n",
      "Downloading rouge-1.0.0-py3-none-any.whl (14 kB)\n",
      "Downloading sentencepiece-0.1.96-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nltk-3.9.1-py3-none-any.whl (1.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m33.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hUsing cached tensorboardX-2.6.2.2-py2.py3-none-any.whl (101 kB)\n",
      "Downloading yacs-0.1.8-py3-none-any.whl (14 kB)\n",
      "Downloading click-8.1.7-py3-none-any.whl (97 kB)\n",
      "Installing collected packages: sentencepiece, yacs, tensorboardX, rouge, click, nltk, openprompt\n",
      "  Attempting uninstall: sentencepiece\n",
      "    Found existing installation: sentencepiece 0.2.0\n",
      "    Uninstalling sentencepiece-0.2.0:\n",
      "      Successfully uninstalled sentencepiece-0.2.0\n",
      "Successfully installed click-8.1.7 nltk-3.9.1 openprompt-1.0.1 rouge-1.0.0 sentencepiece-0.1.96 tensorboardX-2.6.2.2 yacs-0.1.8\n",
      "Requirement already satisfied: accelerate in /home/maruanottoni/miniconda3/envs/nn/lib/python3.8/site-packages (1.0.1)\n",
      "Requirement already satisfied: numpy<3.0.0,>=1.17 in /home/maruanottoni/miniconda3/envs/nn/lib/python3.8/site-packages (from accelerate) (1.24.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/maruanottoni/miniconda3/envs/nn/lib/python3.8/site-packages (from accelerate) (24.1)\n",
      "Requirement already satisfied: psutil in /home/maruanottoni/miniconda3/envs/nn/lib/python3.8/site-packages (from accelerate) (6.0.0)\n",
      "Requirement already satisfied: pyyaml in /home/maruanottoni/miniconda3/envs/nn/lib/python3.8/site-packages (from accelerate) (6.0.2)\n",
      "Requirement already satisfied: torch>=1.10.0 in /home/maruanottoni/miniconda3/envs/nn/lib/python3.8/site-packages (from accelerate) (2.4.1)\n",
      "Requirement already satisfied: huggingface-hub>=0.21.0 in /home/maruanottoni/miniconda3/envs/nn/lib/python3.8/site-packages (from accelerate) (0.26.2)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /home/maruanottoni/miniconda3/envs/nn/lib/python3.8/site-packages (from accelerate) (0.4.5)\n",
      "Requirement already satisfied: filelock in /home/maruanottoni/miniconda3/envs/nn/lib/python3.8/site-packages (from huggingface-hub>=0.21.0->accelerate) (3.16.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /home/maruanottoni/miniconda3/envs/nn/lib/python3.8/site-packages (from huggingface-hub>=0.21.0->accelerate) (2024.9.0)\n",
      "Requirement already satisfied: requests in /home/maruanottoni/miniconda3/envs/nn/lib/python3.8/site-packages (from huggingface-hub>=0.21.0->accelerate) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in /home/maruanottoni/miniconda3/envs/nn/lib/python3.8/site-packages (from huggingface-hub>=0.21.0->accelerate) (4.66.5)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/maruanottoni/miniconda3/envs/nn/lib/python3.8/site-packages (from huggingface-hub>=0.21.0->accelerate) (4.12.2)\n",
      "Requirement already satisfied: sympy in /home/maruanottoni/miniconda3/envs/nn/lib/python3.8/site-packages (from torch>=1.10.0->accelerate) (1.13.3)\n",
      "Requirement already satisfied: networkx in /home/maruanottoni/miniconda3/envs/nn/lib/python3.8/site-packages (from torch>=1.10.0->accelerate) (2.8.8)\n",
      "Requirement already satisfied: jinja2 in /home/maruanottoni/miniconda3/envs/nn/lib/python3.8/site-packages (from torch>=1.10.0->accelerate) (3.0.3)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /home/maruanottoni/miniconda3/envs/nn/lib/python3.8/site-packages (from torch>=1.10.0->accelerate) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /home/maruanottoni/miniconda3/envs/nn/lib/python3.8/site-packages (from torch>=1.10.0->accelerate) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /home/maruanottoni/miniconda3/envs/nn/lib/python3.8/site-packages (from torch>=1.10.0->accelerate) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /home/maruanottoni/miniconda3/envs/nn/lib/python3.8/site-packages (from torch>=1.10.0->accelerate) (9.1.0.70)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /home/maruanottoni/miniconda3/envs/nn/lib/python3.8/site-packages (from torch>=1.10.0->accelerate) (12.1.3.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /home/maruanottoni/miniconda3/envs/nn/lib/python3.8/site-packages (from torch>=1.10.0->accelerate) (11.0.2.54)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /home/maruanottoni/miniconda3/envs/nn/lib/python3.8/site-packages (from torch>=1.10.0->accelerate) (10.3.2.106)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /home/maruanottoni/miniconda3/envs/nn/lib/python3.8/site-packages (from torch>=1.10.0->accelerate) (11.4.5.107)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /home/maruanottoni/miniconda3/envs/nn/lib/python3.8/site-packages (from torch>=1.10.0->accelerate) (12.1.0.106)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.20.5 in /home/maruanottoni/miniconda3/envs/nn/lib/python3.8/site-packages (from torch>=1.10.0->accelerate) (2.20.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /home/maruanottoni/miniconda3/envs/nn/lib/python3.8/site-packages (from torch>=1.10.0->accelerate) (12.1.105)\n",
      "Requirement already satisfied: triton==3.0.0 in /home/maruanottoni/miniconda3/envs/nn/lib/python3.8/site-packages (from torch>=1.10.0->accelerate) (3.0.0)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12 in /home/maruanottoni/miniconda3/envs/nn/lib/python3.8/site-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.10.0->accelerate) (12.6.77)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/maruanottoni/miniconda3/envs/nn/lib/python3.8/site-packages (from jinja2->torch>=1.10.0->accelerate) (2.1.5)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/maruanottoni/miniconda3/envs/nn/lib/python3.8/site-packages (from requests->huggingface-hub>=0.21.0->accelerate) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/maruanottoni/miniconda3/envs/nn/lib/python3.8/site-packages (from requests->huggingface-hub>=0.21.0->accelerate) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/maruanottoni/miniconda3/envs/nn/lib/python3.8/site-packages (from requests->huggingface-hub>=0.21.0->accelerate) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/maruanottoni/miniconda3/envs/nn/lib/python3.8/site-packages (from requests->huggingface-hub>=0.21.0->accelerate) (2024.8.30)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /home/maruanottoni/miniconda3/envs/nn/lib/python3.8/site-packages (from sympy->torch>=1.10.0->accelerate) (1.3.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers==4.19.0\n",
    "!pip install datasets\n",
    "!pip install openprompt\n",
    "!pip install accelerate -U"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "Tlj4ZSPjAXGe"
   },
   "outputs": [],
   "source": [
    "import copy\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from datasets import load_dataset, Dataset, concatenate_datasets\n",
    "\n",
    "from transformers import (\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    "    BertForTokenClassification,\n",
    "    BertForSequenceClassification,\n",
    "    DataCollatorWithPadding,\n",
    "    AutoTokenizer,\n",
    "    AdamW\n",
    ")\n",
    "\n",
    "from openprompt.data_utils import InputExample\n",
    "from openprompt.plms import load_plm\n",
    "from openprompt.prompts import ManualTemplate, ManualVerbalizer\n",
    "from openprompt import PromptDataLoader, PromptForClassification\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.ticker import MultipleLocator\n",
    "%matplotlib inline\n",
    "\n",
    "use_cuda = torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JMVHj_SFiVXF"
   },
   "source": [
    "## Text Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JvjqkCREiVXF"
   },
   "source": [
    "### Load and format data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yVpjgtn4iVXF",
    "outputId": "850a3144-2a6c-44b5-d038-5bab6b586c0d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['sentence', 'label', 'idx'],\n",
       "        num_rows: 1000\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['sentence', 'label', 'idx'],\n",
       "        num_rows: 872\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['sentence', 'label', 'idx'],\n",
       "        num_rows: 1821\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = load_dataset('glue', 'sst2')\n",
    "seed = 0\n",
    "\n",
    "## Keep only 1000 of the training data to speed up preprocessing/tokenization\n",
    "dataset['train'] = dataset['train'].shuffle(seed=seed).select(range(1000))\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "2clYqIvhiVXG"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Filter: 100%|██████████| 1000/1000 [00:00<00:00, 33557.92 examples/s]\n",
      "Filter: 100%|██████████| 1000/1000 [00:00<00:00, 39172.74 examples/s]\n"
     ]
    }
   ],
   "source": [
    "## Separate the positives and negatives in training to ensure balanced samples.\n",
    "## This could matter a lot in few-shot\n",
    "dataset['pos_train'] = dataset['train'].filter(lambda x: x['label']==1)\n",
    "dataset['neg_train'] = dataset['train'].filter(lambda x: x['label']==0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "p_RK9g2AiVXG",
    "outputId": "ea4181bf-b373-4108-9eef-883e511dcf7b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[16, 32, 64, 128, 256]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# we will try different samples evenly split between classes\n",
    "shot_increments = 5\n",
    "sample_sizes = [2**i for i in range(4, 4 + shot_increments)]\n",
    "sample_sizes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jYRGuYAviVXH"
   },
   "source": [
    "### Head-based fine-tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 66,
     "referenced_widgets": [
      "fc3675c8cdd34ea38ac3f9c07fd0f8fd",
      "43eddbbd7b994bf280259963d8b90a0e",
      "d3942f7ec3f44fb7b1cc36959b825420",
      "9f4371ee9536450e96dc2d404221e62f",
      "154f86fa79a24a878eab56c61be42a14",
      "471803e92edf48248a90d934549b2a89",
      "7e710035aecf4dcdb9a89f2a94706f45",
      "0b84311849f84b2f8ead61bee9fe76ae",
      "0958a86012ae4b8eb2bccc8773959acd",
      "ad433567c0dc4b16a4c7a78fe9d1d2d7",
      "b9bc96d64de94241a3727252642bf3ec"
     ]
    },
    "id": "5xe0_rM6iVXH",
    "outputId": "ba8979cc-20c0-4edc-8b1c-c37d68806d0f"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: 100%|██████████| 49.0/49.0 [00:00<00:00, 122kB/s]\n",
      "Downloading: 100%|██████████| 570/570 [00:00<00:00, 1.76MB/s]\n",
      "Downloading: 100%|██████████| 208k/208k [00:00<00:00, 739kB/s] \n",
      "Downloading: 100%|██████████| 426k/426k [00:00<00:00, 1.06MB/s]\n",
      "Map: 100%|██████████| 1000/1000 [00:00<00:00, 15960.55 examples/s]\n",
      "Map: 100%|██████████| 872/872 [00:00<00:00, 15879.44 examples/s]\n",
      "Map: 100%|██████████| 1821/1821 [00:00<00:00, 19241.14 examples/s]\n",
      "Map: 100%|██████████| 560/560 [00:00<00:00, 16541.73 examples/s]\n",
      "Map: 100%|██████████| 440/440 [00:00<00:00, 14664.58 examples/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['labels', 'input_ids', 'token_type_ids', 'attention_mask']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Instantiate a BERT tokenizer and tokenizer the dataset\n",
    "\n",
    "def tokenize_function(example):\n",
    "    return tokenizer(example[\"sentence\"], truncation=True)\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained('bert-base-cased')\n",
    "tokenized_datasets = dataset.map(tokenize_function, batched=True)\n",
    "\n",
    "# rename columns and convert tokenized dataset to pytorch format\n",
    "tokenized_datasets = tokenized_datasets.remove_columns([\"sentence\", \"idx\"])\n",
    "tokenized_datasets = tokenized_datasets.rename_column(\"label\", \"labels\")\n",
    "tokenized_datasets.set_format(\"torch\")\n",
    "tokenized_datasets[\"train\"].column_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "OWp8G0awiVXH"
   },
   "outputs": [],
   "source": [
    "## Define the accuracy metric\n",
    "\n",
    "def compute_acc(eval_preds):\n",
    "    preds = np.argmax(eval_preds.predictions, axis=-1)\n",
    "    labels = eval_preds.label_ids\n",
    "    acc = sum([int(i==j) for i,j in zip(preds, labels)])/len(labels)\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 245,
     "referenced_widgets": [
      "dd9e2008ea70466d8f763e59a6e71e81",
      "03a30e0f3629404fad68bca87cd7df83",
      "8ab2c16098b0433a8ee2bffc4533183a",
      "a2e31e39f305471f9ca3d12b7a5ca1bc",
      "d84a270ea3ea4156acc42a1fb79a233d",
      "bd09844151a54644b8a5c6cb7fc5ed29",
      "901f44c6889744859993aaeb6100056f",
      "91d59fe84a634ec293216cfaee9c333a",
      "e6781e07a6314e51b3fd91e3f6ed4ae8",
      "ae4146821f994598ab561b975ad2f9f2",
      "f31f50d874a046b294537c1fe6950fb7"
     ]
    },
    "id": "QMuLYiAXiVXH",
    "outputId": "ff88b13e-11a0-40fe-f117-386e8be32144"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: 100%|██████████| 416M/416M [00:17<00:00, 25.3MB/s] \n",
      "/home/maruanottoni/miniconda3/envs/nn/lib/python3.8/site-packages/transformers/modeling_utils.py:392: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  return torch.load(checkpoint_file, map_location=\"cpu\")\n",
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "***** Running Prediction *****\n",
      "  Num examples = 872\n",
      "  Batch size = 16\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='165' max='55' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [55/55 05:01]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.5091743119266054]\n"
     ]
    }
   ],
   "source": [
    "## \"Zero-shot\" i.e. the head has random weights and no training is done\n",
    "training_args = TrainingArguments(\"trainer\")\n",
    "finetune_model = BertForSequenceClassification.from_pretrained('bert-base-cased', num_labels=2)\n",
    "if use_cuda:\n",
    "    finetune_model = finetune_model.cuda()\n",
    "\n",
    "trainer = Trainer(\n",
    "    finetune_model,\n",
    "    training_args,\n",
    "    train_dataset=tokenized_datasets['train'],\n",
    "    tokenizer=tokenizer,\n",
    ")\n",
    "\n",
    "## Skip training of the Trainer object for zero-shot\n",
    "preds = trainer.predict(tokenized_datasets['validation'])\n",
    "acc = compute_acc(preds)\n",
    "finetune_scores = [acc]\n",
    "print(finetune_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "Xz92P171iVXI",
    "outputId": "81678075-1a7f-4c41-c27d-18637dae9b05"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "/home/maruanottoni/miniconda3/envs/nn/lib/python3.8/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 1000\n",
      "  Num Epochs = 3\n",
      "  Instantaneous batch size per device = 8\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 16\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 189\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='189' max='189' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [189/189 02:07, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "***** Running Prediction *****\n",
      "  Num examples = 872\n",
      "  Batch size = 16\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "***** Running training *****\n",
      "  Num examples = 1000\n",
      "  Num Epochs = 3\n",
      "  Instantaneous batch size per device = 8\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 16\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 189\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.5091743119266054, 0.8646788990825688]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='189' max='189' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [189/189 02:12, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "***** Running Prediction *****\n",
      "  Num examples = 872\n",
      "  Batch size = 16\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "***** Running training *****\n",
      "  Num examples = 1000\n",
      "  Num Epochs = 3\n",
      "  Instantaneous batch size per device = 8\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 16\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 189\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.5091743119266054, 0.8646788990825688, 0.8646788990825688]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='16' max='189' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 16/189 00:10 < 02:11, 1.31 it/s, Epoch 0.24/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## Now generate a learning curve. loop over different values of k (total samples)\n",
    "## and calculate accuracy for each\n",
    "\n",
    "for k in sample_sizes:\n",
    "    train_sample = concatenate_datasets([tokenized_datasets['pos_train'].select(range(k)),\n",
    "                                         tokenized_datasets['neg_train'].select(range(k))])\n",
    "    training_args = TrainingArguments(\"trainer\")\n",
    "    model = copy.deepcopy(finetune_model)\n",
    "    if use_cuda:\n",
    "        model = model.cuda()\n",
    "\n",
    "        trainer = Trainer(\n",
    "            model,\n",
    "            training_args,\n",
    "            train_dataset=train_sample,\n",
    "            # data_collator=data_collator,\n",
    "            tokenizer=tokenizer\n",
    "        )\n",
    "\n",
    "    trainer.train()\n",
    "    preds = trainer.predict(tokenized_datasets['validation'])\n",
    "    acc = compute_acc(preds)\n",
    "    finetune_scores.append(acc)\n",
    "    print(finetune_scores)\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "E_EJLfR4iVXI"
   },
   "source": [
    "### Prompt-based fine-tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gy6vCsrRiVXI",
    "outputId": "2abdddae-2dbd-4470-fa4e-0f2adb515ff4"
   },
   "outputs": [],
   "source": [
    "## Create a dataset of openprompt InputExamples from the training data\n",
    "prompt_dataset = {}\n",
    "for split in ['pos_train', 'neg_train', 'validation', 'test']:\n",
    "    prompt_dataset[split] = []\n",
    "    for data in dataset[split]:\n",
    "        input_example = InputExample(text_a = data['sentence'], label=int(data['label']), guid=data['idx'])\n",
    "        prompt_dataset[split].append(input_example)\n",
    "print(prompt_dataset['pos_train'][0])\n",
    "print(prompt_dataset['neg_train'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "P9ev2o-QiVXI",
    "outputId": "ad2d9e9b-e6a8-4b4e-8f04-0f7c971a1bc3"
   },
   "outputs": [],
   "source": [
    "## Load the BERT model\n",
    "plm, tokenizer, model_config, WrapperClass = load_plm(\"bert\", \"bert-base-cased\")\n",
    "\n",
    "## Create the prompt template\n",
    "template_text = '{\"placeholder\": \"text_a\"} it is {\"mask\"} .'\n",
    "template = ManualTemplate(tokenizer=tokenizer, text=template_text)\n",
    "\n",
    "## Create a wrapped tokenizer\n",
    "wrapped_tokenizer = WrapperClass(max_seq_length=128, decoder_max_length=3, tokenizer=tokenizer, truncate_method=\"head\")\n",
    "\n",
    "## Define your verbalizer with desired vocabulatary mapping to pos and neg\n",
    "verbalizer = ManualVerbalizer(tokenizer, num_classes=2,\n",
    "                              label_words=[['terrible'], ['great']])\n",
    "\n",
    "## Generate a testing dataloader\n",
    "val_dataloader = PromptDataLoader(prompt_dataset['validation'], template, tokenizer=tokenizer,\n",
    "                                  tokenizer_wrapper_class=WrapperClass, batch_size=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iW6S9sDXiVXJ"
   },
   "outputs": [],
   "source": [
    "## Define the accuracy metric\n",
    "\n",
    "def evaluate(model, val_dataloader):\n",
    "    model.eval()\n",
    "    allpreds = []\n",
    "    alllabels = []\n",
    "    with torch.no_grad():\n",
    "        for step, inputs in enumerate(val_dataloader):\n",
    "            if use_cuda:\n",
    "                inputs = inputs.cuda()\n",
    "            logits = model(inputs)\n",
    "            labels = inputs['label']\n",
    "            alllabels.extend(labels.cpu().tolist())\n",
    "            allpreds.extend(torch.argmax(logits, dim=-1).cpu().tolist())\n",
    "\n",
    "    acc = sum([int(i==j) for i,j in zip(allpreds, alllabels)])/len(allpreds)\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_jlgVoOKiVXJ",
    "outputId": "fd139e14-2401-4cd8-8eb3-52851d7afe45"
   },
   "outputs": [],
   "source": [
    "## ** Zero-shot testing **\n",
    "## Run the evalation set against the prompt model before any finetuning.\n",
    "## This is equivalent to normal prompt-based inference, relying just on\n",
    "## the weights tuned into the model and not on any modification.\n",
    "\n",
    "prompt_model = PromptForClassification(plm=copy.deepcopy(plm), template=template,\n",
    "                                       verbalizer=verbalizer)\n",
    "prompt_model = prompt_model.cuda()\n",
    "prompt_scores = [evaluate(prompt_model, val_dataloader)]\n",
    "prompt_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZFOS10MLiVXJ",
    "outputId": "99855407-0f70-4ee6-b2d2-1d248f7f2d3f"
   },
   "outputs": [],
   "source": [
    "## Now generate a learning curve. loop over different values of k (total samples)\n",
    "## and calculate accuracy for each\n",
    "\n",
    "for k in sample_sizes:\n",
    "    # they are already shuffled, we can simply select the first k examples each time\n",
    "    train_sample = prompt_dataset['pos_train'][:k] + prompt_dataset['neg_train'][:k]\n",
    "    train_dataloader = PromptDataLoader(train_sample, template, tokenizer=tokenizer,\n",
    "                                    tokenizer_wrapper_class=WrapperClass, shuffle=True,\n",
    "                                    batch_size=4, seed=seed)\n",
    "\n",
    "    prompt_model = PromptForClassification(plm=copy.deepcopy(plm), template=template,\n",
    "                                         verbalizer=verbalizer, freeze_plm=False)\n",
    "    prompt_model = prompt_model.cuda()\n",
    "\n",
    "    loss_func = torch.nn.CrossEntropyLoss()\n",
    "    no_decay = ['bias', 'LayerNorm.weight']\n",
    "    optimizer_grouped_parameters = [\n",
    "      {'params': [p for n, p in prompt_model.named_parameters() if not any(nd in n for nd in no_decay)], 'weight_decay': 0.01},\n",
    "      {'params': [p for n, p in prompt_model.named_parameters() if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n",
    "    ]\n",
    "\n",
    "    optimizer = AdamW(optimizer_grouped_parameters, lr=2e-5)\n",
    "\n",
    "    for epoch in range(5):\n",
    "        tot_loss = 0\n",
    "        for step, inputs in enumerate(train_dataloader):\n",
    "            if use_cuda:\n",
    "                inputs = inputs.cuda()\n",
    "            logits = prompt_model(inputs)\n",
    "            labels = inputs['label']\n",
    "            loss = loss_func(logits, labels)\n",
    "            loss.backward()\n",
    "            tot_loss += loss.item()\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "    accuracy = evaluate(prompt_model, val_dataloader)\n",
    "    prompt_scores.append(accuracy)\n",
    "    print(prompt_scores)\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Nm3SSIriiVXJ"
   },
   "source": [
    "### Plot the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "O4A0tlP3iVXJ"
   },
   "outputs": [],
   "source": [
    "## Scores for the plot in the paper\n",
    "\n",
    "# finetune_scores = [0.5091743119266054,\n",
    "#                    0.5068807339449541,\n",
    "#                    0.6548165137614679,\n",
    "#                    0.8486238532110092,\n",
    "#                    0.8623853211009175,\n",
    "#                    0.8738532110091743]\n",
    "\n",
    "# prompt_scores = [0.680045871559633,\n",
    "#                  0.6743119266055045,\n",
    "#                  0.786697247706422,\n",
    "#                  0.8474770642201835,\n",
    "#                  0.8520642201834863,\n",
    "#                  0.8658256880733946]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 490
    },
    "id": "31tU6QT0iVXJ",
    "outputId": "4e2ac156-0b96-4276-ae62-9c387a336f16"
   },
   "outputs": [],
   "source": [
    "# plot the prompt-based training and pre-train/finetuning learning curves\n",
    "# against each other\n",
    "\n",
    "fig1 = plt.figure()\n",
    "ax11 = fig1.add_subplot(111)\n",
    "\n",
    "x = [0] + sample_sizes\n",
    "ax11.plot(range(len(x)), finetune_scores, color='#cc0000', lw=2, ls='--', label='Head-based tuning')\n",
    "ax11.scatter(range(len(x)), finetune_scores, color='#cc0000')\n",
    "ax11.plot(range(len(x)), prompt_scores, color='#6d9eeb', lw=2, label='Prompt-based tuning')\n",
    "ax11.scatter(range(len(x)), prompt_scores, color='#6d9eeb')\n",
    "ax11.set_xticks(range(len(x)))\n",
    "ax11.set_xticklabels(x,fontsize=16)\n",
    "ax11.set_xlabel('# training examples (per class)',fontsize=20)\n",
    "\n",
    "ax11.set_yticks([0.5, 0.6, 0.7, 0.8, 0.9])\n",
    "ax11.set_yticklabels(['0.5', '0.6', '0.7', '0.8', '0.9'],fontsize=16)\n",
    "ax11.set_ylabel('Test Set Accuracy',fontsize=20)\n",
    "\n",
    "ax11.xaxis.set_ticks_position('both')\n",
    "ax11.tick_params(axis='x', which='major', direction='in')\n",
    "ax11.tick_params(axis='x', which='minor', direction='in')\n",
    "ax11.yaxis.set_ticks_position('both')\n",
    "ax11.tick_params(axis='y', which='major', direction='in')\n",
    "ax11.tick_params(axis='y', which='minor', direction='in')\n",
    "\n",
    "plt.gca().yaxis.set_minor_locator(MultipleLocator(0.02))\n",
    "\n",
    "ax11.legend(fontsize=15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CC43e6JLiVXK"
   },
   "source": [
    "## Named-entity Recognition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9FFCfAYMGlHg"
   },
   "source": [
    "### Head-based fine-tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lS0DTDXviVXK"
   },
   "source": [
    "#### Load and format data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "EQA2iXKA0x9z",
    "outputId": "42479984-cc4e-4832-d0b1-baf12556e7ce"
   },
   "outputs": [],
   "source": [
    "## Load CoNLL-2003 dataset from huggingface\n",
    "dataset = load_dataset('conll2003')\n",
    "\n",
    "## Create dictionary key for CoNLL-2003 NER-tags:\n",
    "id_to_label = {'O': 0, 'B-PER': 1, 'I-PER': 2, 'B-ORG': 3, 'I-ORG': 4, 'B-LOC': 5, 'I-LOC': 6, 'B-MISC': 7, 'I-MISC': 8}\n",
    "label_to_id = {0: 'O', 1: 'B-PER', 2: 'I-PER', 3: 'B-ORG', 4: 'I-ORG', 5: 'B-LOC', 6: 'I-LOC', 7: 'B-MISC', 8: 'I-MISC' }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PJ7pOexZ0zH6",
    "outputId": "c08eb168-bdc7-4845-e00c-e7109698e94e"
   },
   "outputs": [],
   "source": [
    "## Prepare the data using three steps.\n",
    "##   1) downsample train, test, val as desired.\n",
    "##   2) convert labels from dataset tokenization to BERT tokenization\n",
    "##   3) tokenize and pad the sentences.\n",
    "\n",
    "seed = 13\n",
    "\n",
    "TRN_SAMP = 1024\n",
    "VAL_SAMP = 500\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")\n",
    "SET_EXTRA_TOK_TO_MINUS_100 = False\n",
    "\n",
    "pad_length = 128\n",
    "\n",
    "#######\n",
    "\n",
    "## Tokenize sentences and return vectors necessary for the training loop\n",
    "def tokenize_function(tokens):\n",
    "    return tokenizer.encode_plus(tokens,\n",
    "                                 is_split_into_words = True,\n",
    "                                 pad_to_max_length = True,\n",
    "                                 max_length = pad_length,\n",
    "                                 return_attention_mask = True,\n",
    "                                 return_token_type_ids = True,\n",
    "                                 )\n",
    "\n",
    "def pad_label_example(tokenized_input, labels, label_all_tokens=True):\n",
    "    ## Look for tokens identified as \"None\" type -- set label to -100. Else,\n",
    "    ## use the label from the input label vector.\n",
    "    word_ids = tokenized_input.word_ids()\n",
    "    label_ids = []\n",
    "    for i, word_idx in enumerate(word_ids):\n",
    "        if word_idx is None:\n",
    "            label_ids.append(-100)\n",
    "        else:\n",
    "            label_ids.append(labels[i-1])\n",
    "    return label_ids\n",
    "\n",
    "## Determine for each word if BERT tokenization differs.\n",
    "def apply_bert_tok(og_tok_sent,og_lab):\n",
    "    newlab = []\n",
    "    ## Determine the tokenization for each word.\n",
    "    for i, t in enumerate(og_tok_sent):\n",
    "        ttok = tokenizer(' '+t+' ',add_special_tokens=False)['input_ids']\n",
    "        ## If the word splits to subwork tokens, extra spots in the labels.\n",
    "        for j in range(len(ttok)):\n",
    "            ## Can either set to -100 or set to same label as the original token.\n",
    "            if SET_EXTRA_TOK_TO_MINUS_100:\n",
    "                if j == 0: newlab.append(og_lab[i])\n",
    "                else: newlab.append(-100)\n",
    "            else:\n",
    "                newlab.append(og_lab[i])\n",
    "    return newlab\n",
    "\n",
    "## Convert labels to BERT tokenization.\n",
    "## Note the data comes pretokenized, but the split doesn't match BERT procedure.\n",
    "## So we tokenize every individual word to determine where word splitting occurs,\n",
    "## and create a new label vector to account for this.\n",
    "def bert_token_maker(dataset):\n",
    "    full_sents, tok_sents, pad_labs = [], [], []\n",
    "    input_ids, tkntype_ids, attn_masks = [], [], []\n",
    "    ## Update labels to reflect BERT tokenization\n",
    "    sents, labs = dataset['sentence'], dataset['labels']\n",
    "    new_labs = [apply_bert_tok(sents[i], labs[i]) for i in range(len(labs))]\n",
    "    ## Create encoded dictionaries for each sentence.\n",
    "    tok_dicts = [tokenize_function(s) for s in sents]\n",
    "\n",
    "    for td,l in zip(tok_dicts,new_labs):\n",
    "        ## Collect the original and tokenized sentence.\n",
    "        full_sents.append(tokenizer.decode(td['input_ids'],skip_special_tokens=True))\n",
    "        tok_sents.append([tokenizer.decode(td['input_ids'][i]) for i in range(len(td['input_ids']))])\n",
    "        ## Pad labels to align with tokenized sentences.\n",
    "        pad_labs.append(pad_label_example(td,l))\n",
    "        ## Collect other training requirements.\n",
    "        input_ids.append(td['input_ids'])\n",
    "        tkntype_ids.append(td['token_type_ids'])\n",
    "        attn_masks.append(td['attention_mask'])\n",
    "\n",
    "    dataset = Dataset.from_dict(\n",
    "            {'id': dataset['id'],\n",
    "             'sentence': full_sents,\n",
    "             'tokens': tok_sents,\n",
    "             'labels': pad_labs,\n",
    "             'input_ids': input_ids,\n",
    "             'token_type_ids': tkntype_ids,\n",
    "             'attention_mask': attn_masks\n",
    "             })\n",
    "\n",
    "    return dataset\n",
    "\n",
    "def data_prep(dataset):\n",
    "\n",
    "    ## Remove unneeded columns\n",
    "    dataset = dataset.remove_columns([\"pos_tags\", \"chunk_tags\"])\n",
    "    dataset = dataset.rename_column(\"tokens\", \"sentence\")\n",
    "    dataset = dataset.rename_column(\"ner_tags\", \"labels\")\n",
    "\n",
    "    ## Downsample to a (pseudo)random subset.\n",
    "    dataset['train'] = dataset['train'].shuffle(seed=seed).select(range(TRN_SAMP))\n",
    "    dataset['validation'] = dataset['validation'].shuffle(seed=seed).select(range(VAL_SAMP))\n",
    "\n",
    "    ## Tokenize sentences and align labels.\n",
    "    dataset['train'] = bert_token_maker(dataset['train'])\n",
    "    dataset['validation'] = bert_token_maker(dataset['validation'])\n",
    "\n",
    "    return dataset\n",
    "\n",
    "tokenized_datasets = data_prep(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "C8pnkofx0VgZ",
    "outputId": "c45a6566-e167-411a-9998-a1e046aa9d17"
   },
   "outputs": [],
   "source": [
    "## We will try different numbers of samples for the learning curve\n",
    "shot_increments = 7\n",
    "sample_sizes = [0,] + [2**i for i in range(3, 4 + shot_increments)]\n",
    "sample_sizes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kd95w6OsNBsR"
   },
   "source": [
    "#### Fine-tune the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "W5GD5muUNBXs"
   },
   "outputs": [],
   "source": [
    "## Define F1-score metric\n",
    "\n",
    "def compute_ptft_f1(preds,labs):\n",
    "    preds_clean = preds.argmax(axis=2)[labs != -100]\n",
    "    labs_clean = labs[labs != -100]\n",
    "    # precision\n",
    "    pmask = np.isin(preds_clean, [1,3,5,7])\n",
    "    p = (preds_clean[pmask] == labs_clean[pmask])\n",
    "    p = p.astype(float).sum()/sum(pmask)\n",
    "    # recall\n",
    "    rmask = np.isin(labs_clean, [1,3,5,7])\n",
    "    r = (preds_clean[rmask] == labs_clean[rmask])\n",
    "    r = r.astype(float).sum()/sum(rmask)\n",
    "    # f1\n",
    "    return 2.*p*r/(p+r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "-Pij4BAr_O-Q",
    "outputId": "7aa37a33-48bf-439b-ccff-2c8a63ff5f6b"
   },
   "outputs": [],
   "source": [
    "## Set parameters for training loop, and load BERT model\n",
    "\n",
    "tokenized_datasets.set_format(\"torch\")\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "training_args = TrainingArguments(\"trainer\")\n",
    "model = BertForTokenClassification.from_pretrained('bert-base-cased',\n",
    "                                                    num_labels=len(label_to_id))\n",
    "\n",
    "## Loop over different samples of k\n",
    "finetune_scores = []\n",
    "for i, k in enumerate(sample_sizes):\n",
    "    if k == 0: samps = range(0) ## zero-shot\n",
    "    else: samps = range(sample_sizes[i-1],k) ## iterative training\n",
    "    train_sample = tokenized_datasets['train'].select(samps)\n",
    "    training_args = TrainingArguments(\"trainer\")\n",
    "    if use_cuda:\n",
    "        model = model.cuda()\n",
    "\n",
    "    trainer = Trainer(\n",
    "        model,\n",
    "        training_args,\n",
    "        train_dataset=train_sample,\n",
    "        data_collator=data_collator,\n",
    "        tokenizer=tokenizer\n",
    "    )\n",
    "\n",
    "    ## If k==0, do zero-shot testing. Otherwise, activate training.\n",
    "    if k > 0:\n",
    "        trainer.train()\n",
    "    preds = trainer.predict(tokenized_datasets['validation'])\n",
    "    f1 = compute_ptft_f1(preds.predictions, preds.label_ids)\n",
    "    finetune_scores.append(f1)\n",
    "    print(finetune_scores)\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sCfwo7SINIcj"
   },
   "source": [
    "### Prompt-based fine-tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nuKksfuLiVXL"
   },
   "source": [
    "#### Load and format data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vFKjNUDyGnTm"
   },
   "outputs": [],
   "source": [
    "dataset = load_dataset('conll2003')\n",
    "\n",
    "id_to_label = {'O': 0, 'B-PER': 1, 'I-PER': 2, 'B-ORG': 3, 'I-ORG': 4, 'B-LOC': 5, 'I-LOC': 6, 'B-MISC': 7, 'I-MISC': 8}\n",
    "label_to_id = {0: 'O', 1: 'B-PER', 2: 'I-PER', 3: 'B-ORG', 4: 'I-ORG', 5: 'B-LOC', 6: 'I-LOC', 7: 'B-MISC', 8: 'I-MISC'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "w1lIqaBt7x8Y"
   },
   "outputs": [],
   "source": [
    "## Create mapping for entity ids to indexes 0-4.\n",
    "label_to_label = {0: 0, ## non-entity\n",
    "                  1: 1, ## person\n",
    "                  3: 2, ## organization\n",
    "                  5: 3, ## location\n",
    "                  7: 4} ## misc\n",
    "TRN_SAMP = 1024\n",
    "VAL_SAMP = 500\n",
    "seed = 13\n",
    "\n",
    "## Identify each entity in each sample, and determine if it is one or multiple\n",
    "## tokens long. this allows us to generate individual prompts for each entity.\n",
    "## Save each entity along with its NER tag.\n",
    "def gen_entity_tags(sample):\n",
    "    tokens, labels = sample['tokens'], sample['ner_tags']\n",
    "    total = len(tokens)\n",
    "    entities, tags = [], []\n",
    "    for idx, (t, l) in enumerate(zip(tokens, labels)):\n",
    "        ## Collect non-entities\n",
    "        if l == 0:\n",
    "            entities.append(t)\n",
    "            tags.append(label_to_label[l])\n",
    "            continue\n",
    "\n",
    "        ## Collect entites\n",
    "        if l in (1,3,5,7):\n",
    "            ## If entity is final token, save\n",
    "            if idx == total-1:\n",
    "                entities.append(t)\n",
    "                tags.append(label_to_label[l])\n",
    "                continue\n",
    "\n",
    "            ## If entity is one token, save.\n",
    "            if labels[idx+1] != l+1:\n",
    "                entities.append(t)\n",
    "                tags.append(label_to_label[l])\n",
    "                continue\n",
    "\n",
    "            ## If entity continues to the next token, find how long it goes.\n",
    "            cont = True\n",
    "            extra = 1\n",
    "            while(cont):\n",
    "                ## Check if we've reached the end\n",
    "                if idx+extra == total-1:\n",
    "                    cont = False\n",
    "                elif labels[idx+extra+1] == l+1:\n",
    "                    extra += 1\n",
    "                else:\n",
    "                    cont = False\n",
    "            whole_ent = ' '.join(tokens[idx:idx+extra+1])\n",
    "            entities.append(whole_ent)\n",
    "            tags.append(label_to_label[l])\n",
    "\n",
    "    return entities, tags\n",
    "\n",
    "\n",
    "## Create a dictionary-based dataset to save each entity, its tag, and its\n",
    "## source sentence.\n",
    "def prompt_data_prep(dataset):\n",
    "    ## Downsample\n",
    "    dataset['train'] = dataset['train'].shuffle(seed=seed).select(range(TRN_SAMP))\n",
    "    dataset['validation'] = dataset['validation'].shuffle(seed=seed).select(range(VAL_SAMP))\n",
    "\n",
    "    ## Loop through data to locate entities and save tags\n",
    "    pr_data = {}\n",
    "    for split in ['train', 'validation']:\n",
    "        pr_data[split] = []\n",
    "        for i, samp in enumerate(dataset[split]):\n",
    "            entities, tags = gen_entity_tags(samp)\n",
    "            sentence = ' '.join(entities)\n",
    "\n",
    "            ## Save entities and tags into openprompt InputExamples\n",
    "            for entity, tag in zip(entities,tags):\n",
    "                input_example = InputExample(text_a = sentence,\n",
    "                                             text_b = entity,\n",
    "                                             label = tag,\n",
    "                                             guid=i,\n",
    "                                             )\n",
    "                pr_data[split].append(input_example)\n",
    "\n",
    "    return pr_data\n",
    "\n",
    "prompt_dataset = prompt_data_prep(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6CIQDQqQiVXM"
   },
   "source": [
    "#### Prompt-tune the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jAiFkS1q4VAw",
    "outputId": "c506890d-647f-4d97-8767-f6d50f057fec"
   },
   "outputs": [],
   "source": [
    "# load the BERT prompt model\n",
    "plm, tokenizer, model_config, WrapperClass = load_plm(\"bert\", \"bert-base-cased\")\n",
    "wrapped_tokenizer = WrapperClass(max_seq_length=128,\n",
    "                                 decoder_max_length=3,\n",
    "                                 tokenizer=tokenizer,\n",
    "                                 truncate_method=\"head\")\n",
    "\n",
    "# create a cloze template for training and testing\n",
    "template_text = '{\"placeholder\": \"text_a\"}. {\"placeholder\": \"text_b\"} is a {\"mask\"} entity.'\n",
    "template = ManualTemplate(tokenizer=tokenizer, text=template_text)\n",
    "\n",
    "# define verbalizer corresponding to different named entity classes\n",
    "verbalizer = ManualVerbalizer(tokenizer, num_classes=5,\n",
    "                                label_words=[\n",
    "                                    ['non-'],\n",
    "                                    ['person'],\n",
    "                                    ['organization'],\n",
    "                                    ['location'],\n",
    "                                    ['other'],\n",
    "                                ])\n",
    "\n",
    "# create a validation dataloader\n",
    "val_dataloader = PromptDataLoader(prompt_dataset['validation'], template, tokenizer=tokenizer,\n",
    "                                  tokenizer_wrapper_class=WrapperClass, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0zUrV00f7eZ0"
   },
   "outputs": [],
   "source": [
    "# define F1-score metric\n",
    "def compute_prompt_f1(model, val_dataloader):\n",
    "    model.eval()\n",
    "    allpreds, alllabels = [], []\n",
    "    with torch.no_grad():\n",
    "        for step, inputs in enumerate(val_dataloader):\n",
    "            if use_cuda:\n",
    "                inputs = inputs.cuda()\n",
    "            logits = model(inputs)\n",
    "            labels = inputs['label']\n",
    "            alllabels.extend(labels.cpu().tolist())\n",
    "            allpreds.extend(torch.argmax(logits, dim=-1).cpu().tolist())\n",
    "            if step%10 ==0: print(step, 'of', len(val_dataloader), 'done')\n",
    "    labs_clean, preds_clean = np.array(alllabels), np.array(allpreds)\n",
    "    # precision\n",
    "    pmask = (preds_clean != 0)\n",
    "    p = (preds_clean[pmask] == labs_clean[pmask])\n",
    "    p = p.astype(float).sum()/sum(pmask)\n",
    "    # recall\n",
    "    rmask = (labs_clean != 0)\n",
    "    r = (preds_clean[rmask] == labs_clean[rmask])\n",
    "    r = r.astype(float).sum()/sum(rmask)\n",
    "    # f1\n",
    "    return 2.*p*r/(p+r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "RI5LWM775Pq-",
    "outputId": "559a7a8b-e81c-4e23-ee92-842ddfbed03a"
   },
   "outputs": [],
   "source": [
    "# loop over different values of k and save accuracy for each\n",
    "\n",
    "prompt_model = PromptForClassification(plm=copy.deepcopy(plm), template=template,\n",
    "                                        verbalizer=verbalizer, freeze_plm=False)\n",
    "prompt_model = prompt_model.cuda()\n",
    "\n",
    "prompt_scores = []\n",
    "sample_sizes = [0, 8, 16, 32, 64, 128, 256, 512, 1024]\n",
    "for ind, k in enumerate(sample_sizes):\n",
    "    ## If zero-shot, proceed to predictions. else, go through training loop\n",
    "    if k != 0:\n",
    "        ## define dataset and build dataloader\n",
    "        if ind == 0: k0 = 0\n",
    "        else: k0 = sample_sizes[ind-1]\n",
    "        train_sample = [samp for samp in prompt_dataset['train'] if k0 <= samp.guid < k]\n",
    "        train_dataloader = PromptDataLoader(train_sample, template, tokenizer=tokenizer,\n",
    "                                                                    tokenizer_wrapper_class=WrapperClass, shuffle=True,\n",
    "                                                                    batch_size=4, seed=seed)\n",
    "\n",
    "        loss_func = torch.nn.CrossEntropyLoss()\n",
    "        no_decay = ['bias', 'LayerNorm.weight']\n",
    "        optimizer_grouped_parameters = [\n",
    "            {'params': [p for n, p in prompt_model.named_parameters() if not any(nd in n for nd in no_decay)], 'weight_decay': 0.01},\n",
    "            {'params': [p for n, p in prompt_model.named_parameters() if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n",
    "        ]\n",
    "\n",
    "        optimizer = AdamW(optimizer_grouped_parameters, lr=2e-5)\n",
    "\n",
    "        for epoch in range(5):\n",
    "            tot_loss = 0\n",
    "            for step, inputs in enumerate(train_dataloader):\n",
    "                    if use_cuda:\n",
    "                            inputs = inputs.cuda()\n",
    "                    logits = prompt_model(inputs)\n",
    "                    labels = inputs['label']\n",
    "                    loss = loss_func(logits, labels)\n",
    "                    loss.backward()\n",
    "                    tot_loss += loss.item()\n",
    "                    optimizer.step()\n",
    "                    optimizer.zero_grad()\n",
    "\n",
    "    prompt_scores.append(compute_prompt_f1(prompt_model, val_dataloader))\n",
    "    print(prompt_scores)\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pErMqdKEiVXR"
   },
   "source": [
    "### Plot the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VIYK2hItiVXR"
   },
   "outputs": [],
   "source": [
    "# Scores for the plot in the paper\n",
    "\n",
    "finetune_scores = [0.06870229007633588,\n",
    "                   0.049224544841537425,\n",
    "                   0.0022522522522522522,\n",
    "                   0.003370786516853933,\n",
    "                   0.3323353293413174,\n",
    "                   0.5577981651376147,\n",
    "                   0.7157034442498541,\n",
    "                   0.7894438138479001,\n",
    "                   0.8526011560693642]\n",
    "\n",
    "prompt_scores = [0.07118375955707884,\n",
    "                 0.5788359788359789,\n",
    "                 0.6481854838709677,\n",
    "                 0.727367870225013,\n",
    "                 0.7866666666666666,\n",
    "                 0.8365437534397359,\n",
    "                 0.8671706263498921,\n",
    "                 0.8303769401330378,\n",
    "                 0.855135135135135]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 482
    },
    "id": "VvaMyJUnwjx4",
    "outputId": "b9a023c3-781b-496f-ff4a-ef34271c4e3b"
   },
   "outputs": [],
   "source": [
    "fig1 = plt.figure()\n",
    "ax11 = fig1.add_subplot(111)\n",
    "\n",
    "x = sample_sizes\n",
    "ax11.plot(range(len(x)), finetune_scores, color='#cc0000', lw=2, ls='--', label='Head-based tuning')\n",
    "ax11.scatter(range(len(x)), finetune_scores, color='#cc0000')\n",
    "ax11.plot(range(len(x)), prompt_scores, color='#6d9eeb', lw=2, label='Prompt-based tuning')\n",
    "ax11.scatter(range(len(x)), prompt_scores, color='#6d9eeb')\n",
    "\n",
    "ax11.set_xticks(range(len(x)))\n",
    "ax11.set_xticklabels(x,fontsize=16)\n",
    "ax11.set_xlabel('# training examples (per class)',fontsize=20)\n",
    "\n",
    "ax11.set_ylim(-0.04,1.08)\n",
    "ax11.set_yticks([0.0, 0.2, 0.4, 0.6, 0.8, 1.0])\n",
    "ax11.set_yticklabels(['0.0', '0.2', '0.4', '0.6', '0.8', '1.0'],fontsize=16)\n",
    "ax11.set_ylabel('Test Set F1-score',fontsize=20)\n",
    "\n",
    "ax11.xaxis.set_ticks_position('both')\n",
    "ax11.tick_params(axis='x', which='major', direction='in')\n",
    "ax11.tick_params(axis='x', which='minor', direction='in')\n",
    "ax11.yaxis.set_ticks_position('both')\n",
    "ax11.tick_params(axis='y', which='major', direction='in')\n",
    "ax11.tick_params(axis='y', which='minor', direction='in')\n",
    "\n",
    "plt.gca().yaxis.set_minor_locator(MultipleLocator(0.04))\n",
    "\n",
    "ax11.legend(fontsize=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TY7PlWl5-LKF"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "03a30e0f3629404fad68bca87cd7df83": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_bd09844151a54644b8a5c6cb7fc5ed29",
      "placeholder": "​",
      "style": "IPY_MODEL_901f44c6889744859993aaeb6100056f",
      "value": "Downloading: 100%"
     }
    },
    "0958a86012ae4b8eb2bccc8773959acd": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "0b84311849f84b2f8ead61bee9fe76ae": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "154f86fa79a24a878eab56c61be42a14": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "43eddbbd7b994bf280259963d8b90a0e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_471803e92edf48248a90d934549b2a89",
      "placeholder": "​",
      "style": "IPY_MODEL_7e710035aecf4dcdb9a89f2a94706f45",
      "value": "Map: 100%"
     }
    },
    "471803e92edf48248a90d934549b2a89": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "7e710035aecf4dcdb9a89f2a94706f45": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "8ab2c16098b0433a8ee2bffc4533183a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_91d59fe84a634ec293216cfaee9c333a",
      "max": 435779157,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_e6781e07a6314e51b3fd91e3f6ed4ae8",
      "value": 435779157
     }
    },
    "901f44c6889744859993aaeb6100056f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "91d59fe84a634ec293216cfaee9c333a": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "9f4371ee9536450e96dc2d404221e62f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_ad433567c0dc4b16a4c7a78fe9d1d2d7",
      "placeholder": "​",
      "style": "IPY_MODEL_b9bc96d64de94241a3727252642bf3ec",
      "value": " 872/872 [00:00&lt;00:00, 8097.81 examples/s]"
     }
    },
    "a2e31e39f305471f9ca3d12b7a5ca1bc": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_ae4146821f994598ab561b975ad2f9f2",
      "placeholder": "​",
      "style": "IPY_MODEL_f31f50d874a046b294537c1fe6950fb7",
      "value": " 416M/416M [00:08&lt;00:00, 49.1MB/s]"
     }
    },
    "ad433567c0dc4b16a4c7a78fe9d1d2d7": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "ae4146821f994598ab561b975ad2f9f2": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "b9bc96d64de94241a3727252642bf3ec": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "bd09844151a54644b8a5c6cb7fc5ed29": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "d3942f7ec3f44fb7b1cc36959b825420": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_0b84311849f84b2f8ead61bee9fe76ae",
      "max": 872,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_0958a86012ae4b8eb2bccc8773959acd",
      "value": 872
     }
    },
    "d84a270ea3ea4156acc42a1fb79a233d": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "dd9e2008ea70466d8f763e59a6e71e81": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_03a30e0f3629404fad68bca87cd7df83",
       "IPY_MODEL_8ab2c16098b0433a8ee2bffc4533183a",
       "IPY_MODEL_a2e31e39f305471f9ca3d12b7a5ca1bc"
      ],
      "layout": "IPY_MODEL_d84a270ea3ea4156acc42a1fb79a233d"
     }
    },
    "e6781e07a6314e51b3fd91e3f6ed4ae8": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "f31f50d874a046b294537c1fe6950fb7": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "fc3675c8cdd34ea38ac3f9c07fd0f8fd": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_43eddbbd7b994bf280259963d8b90a0e",
       "IPY_MODEL_d3942f7ec3f44fb7b1cc36959b825420",
       "IPY_MODEL_9f4371ee9536450e96dc2d404221e62f"
      ],
      "layout": "IPY_MODEL_154f86fa79a24a878eab56c61be42a14"
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
