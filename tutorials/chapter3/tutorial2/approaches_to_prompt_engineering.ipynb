{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qsTljco25Nyr"
   },
   "source": [
    "# Chapter 3 Tutorial #2: Approaches to Prompt Engineering\n",
    "\n",
    "In Chapter 3 we discussed various methods of template engineering with the goal of improving the analytic capabilities of prompt-tuned LLMs. As part of this discussion, we demonstrated online web applications that could be harnessed to do prompt-based inference and information retrieval, demonstrating the sensitivity of results to choices in template architecture and the fine details of prompt composition.\n",
    "\n",
    "While these web applications are valuable for exploring certain features of masked language prediction, they are limited in their capability for full prompt-based fine-tuning solutions. As such in this tutorial, we will expand on these exercises by exploring few- and many-shot prompt-tuning, discussing results for variable prompt template designs, with a goal towards grasping the critical importance of prompt template optimization.\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7gSSZFq6OvEG"
   },
   "source": [
    "## Installation and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nJu37RUGpwuP",
    "outputId": "1c9cb76a-b118-4ec5-81fe-f3b0085b6709"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers==4.19.0 in /home/maruanottoni/miniconda3/envs/nn/lib/python3.8/site-packages (4.19.0)\n",
      "Requirement already satisfied: filelock in /home/maruanottoni/miniconda3/envs/nn/lib/python3.8/site-packages (from transformers==4.19.0) (3.16.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.1.0 in /home/maruanottoni/miniconda3/envs/nn/lib/python3.8/site-packages (from transformers==4.19.0) (0.26.2)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/maruanottoni/miniconda3/envs/nn/lib/python3.8/site-packages (from transformers==4.19.0) (1.24.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/maruanottoni/miniconda3/envs/nn/lib/python3.8/site-packages (from transformers==4.19.0) (24.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/maruanottoni/miniconda3/envs/nn/lib/python3.8/site-packages (from transformers==4.19.0) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/maruanottoni/miniconda3/envs/nn/lib/python3.8/site-packages (from transformers==4.19.0) (2024.9.11)\n",
      "Requirement already satisfied: requests in /home/maruanottoni/miniconda3/envs/nn/lib/python3.8/site-packages (from transformers==4.19.0) (2.32.3)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.13,>=0.11.1 in /home/maruanottoni/miniconda3/envs/nn/lib/python3.8/site-packages (from transformers==4.19.0) (0.12.1)\n",
      "Requirement already satisfied: tqdm>=4.27 in /home/maruanottoni/miniconda3/envs/nn/lib/python3.8/site-packages (from transformers==4.19.0) (4.66.5)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /home/maruanottoni/miniconda3/envs/nn/lib/python3.8/site-packages (from huggingface-hub<1.0,>=0.1.0->transformers==4.19.0) (2024.9.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/maruanottoni/miniconda3/envs/nn/lib/python3.8/site-packages (from huggingface-hub<1.0,>=0.1.0->transformers==4.19.0) (4.12.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/maruanottoni/miniconda3/envs/nn/lib/python3.8/site-packages (from requests->transformers==4.19.0) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/maruanottoni/miniconda3/envs/nn/lib/python3.8/site-packages (from requests->transformers==4.19.0) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/maruanottoni/miniconda3/envs/nn/lib/python3.8/site-packages (from requests->transformers==4.19.0) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/maruanottoni/miniconda3/envs/nn/lib/python3.8/site-packages (from requests->transformers==4.19.0) (2024.8.30)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: datasets in /home/maruanottoni/miniconda3/envs/nn/lib/python3.8/site-packages (3.1.0)\n",
      "Requirement already satisfied: filelock in /home/maruanottoni/miniconda3/envs/nn/lib/python3.8/site-packages (from datasets) (3.16.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/maruanottoni/miniconda3/envs/nn/lib/python3.8/site-packages (from datasets) (1.24.4)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /home/maruanottoni/miniconda3/envs/nn/lib/python3.8/site-packages (from datasets) (16.1.0)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /home/maruanottoni/miniconda3/envs/nn/lib/python3.8/site-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: pandas in /home/maruanottoni/miniconda3/envs/nn/lib/python3.8/site-packages (from datasets) (2.0.3)\n",
      "Requirement already satisfied: requests>=2.32.2 in /home/maruanottoni/miniconda3/envs/nn/lib/python3.8/site-packages (from datasets) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.66.3 in /home/maruanottoni/miniconda3/envs/nn/lib/python3.8/site-packages (from datasets) (4.66.5)\n",
      "Requirement already satisfied: xxhash in /home/maruanottoni/miniconda3/envs/nn/lib/python3.8/site-packages (from datasets) (3.5.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in /home/maruanottoni/miniconda3/envs/nn/lib/python3.8/site-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2024.9.0,>=2023.1.0 in /home/maruanottoni/miniconda3/envs/nn/lib/python3.8/site-packages (from fsspec[http]<=2024.9.0,>=2023.1.0->datasets) (2024.9.0)\n",
      "Requirement already satisfied: aiohttp in /home/maruanottoni/miniconda3/envs/nn/lib/python3.8/site-packages (from datasets) (3.10.10)\n",
      "Requirement already satisfied: huggingface-hub>=0.23.0 in /home/maruanottoni/miniconda3/envs/nn/lib/python3.8/site-packages (from datasets) (0.26.2)\n",
      "Requirement already satisfied: packaging in /home/maruanottoni/miniconda3/envs/nn/lib/python3.8/site-packages (from datasets) (24.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/maruanottoni/miniconda3/envs/nn/lib/python3.8/site-packages (from datasets) (6.0.2)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /home/maruanottoni/miniconda3/envs/nn/lib/python3.8/site-packages (from aiohttp->datasets) (2.4.3)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /home/maruanottoni/miniconda3/envs/nn/lib/python3.8/site-packages (from aiohttp->datasets) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /home/maruanottoni/miniconda3/envs/nn/lib/python3.8/site-packages (from aiohttp->datasets) (24.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /home/maruanottoni/miniconda3/envs/nn/lib/python3.8/site-packages (from aiohttp->datasets) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /home/maruanottoni/miniconda3/envs/nn/lib/python3.8/site-packages (from aiohttp->datasets) (6.1.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.12.0 in /home/maruanottoni/miniconda3/envs/nn/lib/python3.8/site-packages (from aiohttp->datasets) (1.15.2)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0 in /home/maruanottoni/miniconda3/envs/nn/lib/python3.8/site-packages (from aiohttp->datasets) (4.0.3)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/maruanottoni/miniconda3/envs/nn/lib/python3.8/site-packages (from huggingface-hub>=0.23.0->datasets) (4.12.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/maruanottoni/miniconda3/envs/nn/lib/python3.8/site-packages (from requests>=2.32.2->datasets) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/maruanottoni/miniconda3/envs/nn/lib/python3.8/site-packages (from requests>=2.32.2->datasets) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/maruanottoni/miniconda3/envs/nn/lib/python3.8/site-packages (from requests>=2.32.2->datasets) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/maruanottoni/miniconda3/envs/nn/lib/python3.8/site-packages (from requests>=2.32.2->datasets) (2024.8.30)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /home/maruanottoni/miniconda3/envs/nn/lib/python3.8/site-packages (from pandas->datasets) (2.9.0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/maruanottoni/miniconda3/envs/nn/lib/python3.8/site-packages (from pandas->datasets) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /home/maruanottoni/miniconda3/envs/nn/lib/python3.8/site-packages (from pandas->datasets) (2024.2)\n",
      "Requirement already satisfied: six>=1.5 in /home/maruanottoni/miniconda3/envs/nn/lib/python3.8/site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /home/maruanottoni/miniconda3/envs/nn/lib/python3.8/site-packages (from yarl<2.0,>=1.12.0->aiohttp->datasets) (0.2.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: openprompt in /home/maruanottoni/miniconda3/envs/nn/lib/python3.8/site-packages (1.0.1)\n",
      "Requirement already satisfied: transformers>=4.10.0 in /home/maruanottoni/miniconda3/envs/nn/lib/python3.8/site-packages (from openprompt) (4.19.0)\n",
      "Requirement already satisfied: sentencepiece==0.1.96 in /home/maruanottoni/miniconda3/envs/nn/lib/python3.8/site-packages (from openprompt) (0.1.96)\n",
      "Requirement already satisfied: tqdm>=4.62.2 in /home/maruanottoni/miniconda3/envs/nn/lib/python3.8/site-packages (from openprompt) (4.66.5)\n",
      "Requirement already satisfied: tensorboardX in /home/maruanottoni/miniconda3/envs/nn/lib/python3.8/site-packages (from openprompt) (2.6.2.2)\n",
      "Requirement already satisfied: nltk in /home/maruanottoni/miniconda3/envs/nn/lib/python3.8/site-packages (from openprompt) (3.9.1)\n",
      "Requirement already satisfied: yacs in /home/maruanottoni/miniconda3/envs/nn/lib/python3.8/site-packages (from openprompt) (0.1.8)\n",
      "Requirement already satisfied: dill in /home/maruanottoni/miniconda3/envs/nn/lib/python3.8/site-packages (from openprompt) (0.3.8)\n",
      "Requirement already satisfied: datasets in /home/maruanottoni/miniconda3/envs/nn/lib/python3.8/site-packages (from openprompt) (3.1.0)\n",
      "Requirement already satisfied: rouge==1.0.0 in /home/maruanottoni/miniconda3/envs/nn/lib/python3.8/site-packages (from openprompt) (1.0.0)\n",
      "Requirement already satisfied: pyarrow in /home/maruanottoni/miniconda3/envs/nn/lib/python3.8/site-packages (from openprompt) (16.1.0)\n",
      "Requirement already satisfied: scipy in /home/maruanottoni/miniconda3/envs/nn/lib/python3.8/site-packages (from openprompt) (1.8.1)\n",
      "Requirement already satisfied: six in /home/maruanottoni/miniconda3/envs/nn/lib/python3.8/site-packages (from rouge==1.0.0->openprompt) (1.16.0)\n",
      "Requirement already satisfied: filelock in /home/maruanottoni/miniconda3/envs/nn/lib/python3.8/site-packages (from transformers>=4.10.0->openprompt) (3.16.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.1.0 in /home/maruanottoni/miniconda3/envs/nn/lib/python3.8/site-packages (from transformers>=4.10.0->openprompt) (0.26.2)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/maruanottoni/miniconda3/envs/nn/lib/python3.8/site-packages (from transformers>=4.10.0->openprompt) (1.24.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/maruanottoni/miniconda3/envs/nn/lib/python3.8/site-packages (from transformers>=4.10.0->openprompt) (24.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/maruanottoni/miniconda3/envs/nn/lib/python3.8/site-packages (from transformers>=4.10.0->openprompt) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/maruanottoni/miniconda3/envs/nn/lib/python3.8/site-packages (from transformers>=4.10.0->openprompt) (2024.9.11)\n",
      "Requirement already satisfied: requests in /home/maruanottoni/miniconda3/envs/nn/lib/python3.8/site-packages (from transformers>=4.10.0->openprompt) (2.32.3)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.13,>=0.11.1 in /home/maruanottoni/miniconda3/envs/nn/lib/python3.8/site-packages (from transformers>=4.10.0->openprompt) (0.12.1)\n",
      "Requirement already satisfied: pandas in /home/maruanottoni/miniconda3/envs/nn/lib/python3.8/site-packages (from datasets->openprompt) (2.0.3)\n",
      "Requirement already satisfied: xxhash in /home/maruanottoni/miniconda3/envs/nn/lib/python3.8/site-packages (from datasets->openprompt) (3.5.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in /home/maruanottoni/miniconda3/envs/nn/lib/python3.8/site-packages (from datasets->openprompt) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2024.9.0,>=2023.1.0 in /home/maruanottoni/miniconda3/envs/nn/lib/python3.8/site-packages (from fsspec[http]<=2024.9.0,>=2023.1.0->datasets->openprompt) (2024.9.0)\n",
      "Requirement already satisfied: aiohttp in /home/maruanottoni/miniconda3/envs/nn/lib/python3.8/site-packages (from datasets->openprompt) (3.10.10)\n",
      "Requirement already satisfied: click in /home/maruanottoni/miniconda3/envs/nn/lib/python3.8/site-packages (from nltk->openprompt) (8.1.7)\n",
      "Requirement already satisfied: joblib in /home/maruanottoni/miniconda3/envs/nn/lib/python3.8/site-packages (from nltk->openprompt) (1.4.2)\n",
      "Requirement already satisfied: protobuf>=3.20 in /home/maruanottoni/miniconda3/envs/nn/lib/python3.8/site-packages (from tensorboardX->openprompt) (4.25.5)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /home/maruanottoni/miniconda3/envs/nn/lib/python3.8/site-packages (from aiohttp->datasets->openprompt) (2.4.3)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /home/maruanottoni/miniconda3/envs/nn/lib/python3.8/site-packages (from aiohttp->datasets->openprompt) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /home/maruanottoni/miniconda3/envs/nn/lib/python3.8/site-packages (from aiohttp->datasets->openprompt) (24.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /home/maruanottoni/miniconda3/envs/nn/lib/python3.8/site-packages (from aiohttp->datasets->openprompt) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /home/maruanottoni/miniconda3/envs/nn/lib/python3.8/site-packages (from aiohttp->datasets->openprompt) (6.1.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.12.0 in /home/maruanottoni/miniconda3/envs/nn/lib/python3.8/site-packages (from aiohttp->datasets->openprompt) (1.15.2)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0 in /home/maruanottoni/miniconda3/envs/nn/lib/python3.8/site-packages (from aiohttp->datasets->openprompt) (4.0.3)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/maruanottoni/miniconda3/envs/nn/lib/python3.8/site-packages (from huggingface-hub<1.0,>=0.1.0->transformers>=4.10.0->openprompt) (4.12.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/maruanottoni/miniconda3/envs/nn/lib/python3.8/site-packages (from requests->transformers>=4.10.0->openprompt) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/maruanottoni/miniconda3/envs/nn/lib/python3.8/site-packages (from requests->transformers>=4.10.0->openprompt) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/maruanottoni/miniconda3/envs/nn/lib/python3.8/site-packages (from requests->transformers>=4.10.0->openprompt) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/maruanottoni/miniconda3/envs/nn/lib/python3.8/site-packages (from requests->transformers>=4.10.0->openprompt) (2024.8.30)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /home/maruanottoni/miniconda3/envs/nn/lib/python3.8/site-packages (from pandas->datasets->openprompt) (2.9.0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/maruanottoni/miniconda3/envs/nn/lib/python3.8/site-packages (from pandas->datasets->openprompt) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /home/maruanottoni/miniconda3/envs/nn/lib/python3.8/site-packages (from pandas->datasets->openprompt) (2024.2)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /home/maruanottoni/miniconda3/envs/nn/lib/python3.8/site-packages (from yarl<2.0,>=1.12.0->aiohttp->datasets->openprompt) (0.2.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: accelerate in /home/maruanottoni/miniconda3/envs/nn/lib/python3.8/site-packages (1.0.1)\n",
      "Requirement already satisfied: numpy<3.0.0,>=1.17 in /home/maruanottoni/miniconda3/envs/nn/lib/python3.8/site-packages (from accelerate) (1.24.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/maruanottoni/miniconda3/envs/nn/lib/python3.8/site-packages (from accelerate) (24.1)\n",
      "Requirement already satisfied: psutil in /home/maruanottoni/miniconda3/envs/nn/lib/python3.8/site-packages (from accelerate) (6.0.0)\n",
      "Requirement already satisfied: pyyaml in /home/maruanottoni/miniconda3/envs/nn/lib/python3.8/site-packages (from accelerate) (6.0.2)\n",
      "Requirement already satisfied: torch>=1.10.0 in /home/maruanottoni/miniconda3/envs/nn/lib/python3.8/site-packages (from accelerate) (2.4.1)\n",
      "Requirement already satisfied: huggingface-hub>=0.21.0 in /home/maruanottoni/miniconda3/envs/nn/lib/python3.8/site-packages (from accelerate) (0.26.2)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /home/maruanottoni/miniconda3/envs/nn/lib/python3.8/site-packages (from accelerate) (0.4.5)\n",
      "Requirement already satisfied: filelock in /home/maruanottoni/miniconda3/envs/nn/lib/python3.8/site-packages (from huggingface-hub>=0.21.0->accelerate) (3.16.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /home/maruanottoni/miniconda3/envs/nn/lib/python3.8/site-packages (from huggingface-hub>=0.21.0->accelerate) (2024.9.0)\n",
      "Requirement already satisfied: requests in /home/maruanottoni/miniconda3/envs/nn/lib/python3.8/site-packages (from huggingface-hub>=0.21.0->accelerate) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in /home/maruanottoni/miniconda3/envs/nn/lib/python3.8/site-packages (from huggingface-hub>=0.21.0->accelerate) (4.66.5)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/maruanottoni/miniconda3/envs/nn/lib/python3.8/site-packages (from huggingface-hub>=0.21.0->accelerate) (4.12.2)\n",
      "Requirement already satisfied: sympy in /home/maruanottoni/miniconda3/envs/nn/lib/python3.8/site-packages (from torch>=1.10.0->accelerate) (1.13.3)\n",
      "Requirement already satisfied: networkx in /home/maruanottoni/miniconda3/envs/nn/lib/python3.8/site-packages (from torch>=1.10.0->accelerate) (2.8.8)\n",
      "Requirement already satisfied: jinja2 in /home/maruanottoni/miniconda3/envs/nn/lib/python3.8/site-packages (from torch>=1.10.0->accelerate) (3.0.3)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /home/maruanottoni/miniconda3/envs/nn/lib/python3.8/site-packages (from torch>=1.10.0->accelerate) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /home/maruanottoni/miniconda3/envs/nn/lib/python3.8/site-packages (from torch>=1.10.0->accelerate) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /home/maruanottoni/miniconda3/envs/nn/lib/python3.8/site-packages (from torch>=1.10.0->accelerate) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /home/maruanottoni/miniconda3/envs/nn/lib/python3.8/site-packages (from torch>=1.10.0->accelerate) (9.1.0.70)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /home/maruanottoni/miniconda3/envs/nn/lib/python3.8/site-packages (from torch>=1.10.0->accelerate) (12.1.3.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /home/maruanottoni/miniconda3/envs/nn/lib/python3.8/site-packages (from torch>=1.10.0->accelerate) (11.0.2.54)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /home/maruanottoni/miniconda3/envs/nn/lib/python3.8/site-packages (from torch>=1.10.0->accelerate) (10.3.2.106)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /home/maruanottoni/miniconda3/envs/nn/lib/python3.8/site-packages (from torch>=1.10.0->accelerate) (11.4.5.107)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /home/maruanottoni/miniconda3/envs/nn/lib/python3.8/site-packages (from torch>=1.10.0->accelerate) (12.1.0.106)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.20.5 in /home/maruanottoni/miniconda3/envs/nn/lib/python3.8/site-packages (from torch>=1.10.0->accelerate) (2.20.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /home/maruanottoni/miniconda3/envs/nn/lib/python3.8/site-packages (from torch>=1.10.0->accelerate) (12.1.105)\n",
      "Requirement already satisfied: triton==3.0.0 in /home/maruanottoni/miniconda3/envs/nn/lib/python3.8/site-packages (from torch>=1.10.0->accelerate) (3.0.0)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12 in /home/maruanottoni/miniconda3/envs/nn/lib/python3.8/site-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.10.0->accelerate) (12.6.77)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/maruanottoni/miniconda3/envs/nn/lib/python3.8/site-packages (from jinja2->torch>=1.10.0->accelerate) (2.1.5)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/maruanottoni/miniconda3/envs/nn/lib/python3.8/site-packages (from requests->huggingface-hub>=0.21.0->accelerate) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/maruanottoni/miniconda3/envs/nn/lib/python3.8/site-packages (from requests->huggingface-hub>=0.21.0->accelerate) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/maruanottoni/miniconda3/envs/nn/lib/python3.8/site-packages (from requests->huggingface-hub>=0.21.0->accelerate) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/maruanottoni/miniconda3/envs/nn/lib/python3.8/site-packages (from requests->huggingface-hub>=0.21.0->accelerate) (2024.8.30)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /home/maruanottoni/miniconda3/envs/nn/lib/python3.8/site-packages (from sympy->torch>=1.10.0->accelerate) (1.3.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install transformers==4.19.0\n",
    "%pip install datasets\n",
    "%pip install openprompt\n",
    "%pip install accelerate -U"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "LNaSQwd2pwn_"
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import os\n",
    "import string\n",
    "import copy\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from transformers import  AdamW, get_linear_schedule_with_warmup\n",
    "from transformers.optimization import Adafactor, AdafactorSchedule\n",
    "from datasets import load_dataset\n",
    "\n",
    "from openprompt.plms import load_plm\n",
    "from openprompt import PromptDataLoader\n",
    "from openprompt.data_utils.utils import InputExample\n",
    "from openprompt.prompts import ManualTemplate, ManualVerbalizer, MixedTemplate, SoftTemplate\n",
    "from openprompt.prompts.prefix_tuning_template import PrefixTuningTemplate\n",
    "from openprompt import PromptForClassification\n",
    "from openprompt.utils.reproduciblity import set_seed\n",
    "set_seed(13)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.ticker import MultipleLocator\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DVzIg71SOzc0"
   },
   "source": [
    "## Data loading function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "j5CoaGh_XpRV"
   },
   "outputs": [],
   "source": [
    "## Load the SuperBLUE BoolQ dataset. THis consists of triplets of an informational paragraph,\n",
    "## a yes or no question related to the content of the paragraph, and the correct response.\n",
    "## Save these into test, train, and validation splits.\n",
    "\n",
    "def read_glue_boolq_dataset(tokenizer=None, max_len=512):\n",
    "    ogdataset = load_dataset('super_glue','boolq')\n",
    "\n",
    "    ogdataset['train'] = ogdataset['train'].shuffle(seed=1989).select(range(5000))\n",
    "    ogdataset['test'] = ogdataset['validation'].shuffle(seed=1989).select(range(1800,3270))\n",
    "    ogdataset['validation'] = ogdataset['validation'].shuffle(seed=1989).select(range(1800))\n",
    "\n",
    "    # create a dataset of opemprompt InputExamples from the training data\n",
    "    from openprompt.data_utils import InputExample\n",
    "    dataset = {}\n",
    "    for split in ['train', 'validation', 'test']:\n",
    "        if split in ('test','validation'): ipos, ineg = 0, 0\n",
    "        dataset[split] = []\n",
    "        for data in ogdataset[split]:\n",
    "            ## Throw away samples that are too long.\n",
    "            if tokenizer != None:\n",
    "                testlen = data['passage']+' '+data['question']\n",
    "                if len(tokenizer.tokenize(testlen)) > max_len:\n",
    "                    continue\n",
    "            ## Getting 50/50 for validation\n",
    "            if split in ('test','validation'):\n",
    "                if data['label'] in (1,'1'):\n",
    "                     if ipos >= 500: continue\n",
    "                     ipos += 1\n",
    "                if data['label'] in (0,'0'):\n",
    "                     if ineg >= 500: continue\n",
    "                     ineg += 1\n",
    "            ## Create input examples\n",
    "            input_example = InputExample(text_a = data['passage'],\n",
    "                                        text_b = data['question'],\n",
    "                                        label=data['label'],\n",
    "                                      # idx=data['idx'],\n",
    "                                        guid = \"%s-%s\" % (split, data['idx']))\n",
    "            dataset[split].append(input_example)\n",
    "\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pD3rbnluPBsI"
   },
   "source": [
    "## Prompt-tuning functions and training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "vnXrMTxg-ZJv"
   },
   "outputs": [],
   "source": [
    "## SOFT PROMPT TUNING\n",
    "\n",
    "class SoftPromptTuningArgs:\n",
    "    seed = 144\n",
    "    plm_eval_mode = True\n",
    "    tune_plm = False\n",
    "    max_steps = 20000\n",
    "    prompt_lr = 0.3\n",
    "    warmup_step_prompt = 500\n",
    "    soft_token_num = 20\n",
    "\n",
    "\n",
    "class FixedPromptTuningArgs:\n",
    "    seed = 144\n",
    "    plm_eval_mode = False\n",
    "    tune_plm = True\n",
    "    max_steps = 100 #20000\n",
    "    prompt_lr = 0.3\n",
    "    warmup_step_prompt = 500\n",
    "    soft_token_num = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "6teTpZggi8PZ"
   },
   "outputs": [],
   "source": [
    "## Create a two-parameter verbalizer mapped to \"yes\" and \"no\"\n",
    "def set_verbalizer_model(mytemplate, tokenizer, plm, use_cuda):\n",
    "    myverbalizer = ManualVerbalizer(tokenizer, label_words=[['no','No'],['yes','Yes']], num_classes = 2)\n",
    "    prompt_model = PromptForClassification(plm=plm, template=mytemplate, verbalizer=myverbalizer,\n",
    "                                           freeze_plm = False)\n",
    "    if use_cuda:\n",
    "        prompt_model = prompt_model.cuda()\n",
    "    return prompt_model\n",
    "\n",
    "\n",
    "## OpenPrompt dataloader routine\n",
    "def create_dataloader(dataset, mytemplate, tokenizer, WrapperClass, max_seq_l, batchsize_t,\n",
    "                      shuffle, seed):\n",
    "\n",
    "    the_dataloader = PromptDataLoader(dataset=dataset, template=mytemplate, tokenizer=tokenizer,\n",
    "        tokenizer_wrapper_class=WrapperClass, max_seq_length=max_seq_l, decoder_max_length=3,\n",
    "        batch_size=batchsize_t,shuffle=shuffle, teacher_forcing=False,\n",
    "        predict_eos_token=False, truncate_method=\"tail\",seed=seed)\n",
    "    return the_dataloader\n",
    "\n",
    "\n",
    "## Implement standard accuracy metric for assessing prediction quality.\n",
    "def evaluate(prompt_model, dataloader, use_cuda, desc):\n",
    "    prompt_model.eval()\n",
    "    allpreds = []\n",
    "    alllabels = []\n",
    "\n",
    "    for step, inputs in enumerate(dataloader):\n",
    "        if use_cuda:\n",
    "            inputs = inputs.cuda()\n",
    "        logits = prompt_model(inputs)\n",
    "        labels = inputs['label']\n",
    "        alllabels.extend(labels.cpu().tolist())\n",
    "        allpreds.extend(torch.argmax(logits, dim=-1).cpu().tolist())\n",
    "    acc = sum([int(i==j) for i,j in zip(allpreds, alllabels)])/len(allpreds)\n",
    "    return acc\n",
    "\n",
    "\n",
    "## Configuring LLM and prompt for fine-tuning.\n",
    "def set_plm_prompt_optimizers(prompt_model, tune_plm, tune_prompt, tot_step,\n",
    "                              learn_rate,args):\n",
    "    loss_func = torch.nn.CrossEntropyLoss()\n",
    "    optimizer1 = None\n",
    "    scheduler1 = None\n",
    "    optimizer2 = None\n",
    "    scheduler2 = None\n",
    "\n",
    "    if tune_plm:\n",
    "        no_decay = ['bias', 'LayerNorm.weight']\n",
    "        optimizer_grouped_parameters1 = [\n",
    "            {'params': [p for n, p in prompt_model.plm.named_parameters() if (not any(nd in n for nd in no_decay))], 'weight_decay': 0.01},\n",
    "            {'params': [p for n, p in prompt_model.plm.named_parameters() if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n",
    "        ]\n",
    "        optimizer1 = AdamW(optimizer_grouped_parameters1, lr=learn_rate, ## lr defaults to 3e-5\n",
    "                           no_deprecation_warning=True)\n",
    "        scheduler1 = get_linear_schedule_with_warmup(\n",
    "            optimizer1,\n",
    "            num_warmup_steps=0, num_training_steps=tot_step)\n",
    "\n",
    "    if tune_prompt:\n",
    "        optimizer_grouped_parameters2 = [{'params': [p for name, p in prompt_model.template.named_parameters() if 'raw_embedding' not in name]}]\n",
    "        optimizer2 = AdamW(optimizer_grouped_parameters2, lr=0.4, no_deprecation_warning=True) ## lr defaults to 0.4\n",
    "        scheduler2 = get_linear_schedule_with_warmup(\n",
    "                        optimizer2,\n",
    "                        num_warmup_steps=args.warmup_step_prompt, num_training_steps=tot_step)\n",
    "\n",
    "    return [loss_func, optimizer1, scheduler1, optimizer2, scheduler2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "bLjMMEFtY5Lh"
   },
   "outputs": [],
   "source": [
    "def full_program(task, llm, prompt_type, dataset_name, template, training_sizes,\n",
    "                 num_epoch, learn_rate=3e-5, allow_plm_to_vary = True, allow_prompt_to_vary = True,\n",
    "                 starting_mod = None, use_cuda = True):\n",
    "\n",
    "    ## Collect the relevant arguments\n",
    "    if prompt_type == 'fixed':\n",
    "        args = FixedPromptTuningArgs()\n",
    "    elif prompt_type in ('soft','mixed','prefix'):\n",
    "        args = SoftPromptTuningArgs()\n",
    "    else:\n",
    "        print('Unrecognized prompt_type')\n",
    "        return 0\n",
    "\n",
    "    ## Set parameters based on whether the LLM is being tuned or not ##\n",
    "    max_seq_l = 512\n",
    "    tot_step = args.max_steps\n",
    "    if args.tune_plm:\n",
    "        batchsize_t = 4\n",
    "        batchsize_e = 4\n",
    "        gradient_accumulation_steps = 8\n",
    "        model_parallelize = True\n",
    "    else:\n",
    "        batchsize_t = 8\n",
    "        batchsize_e = 4\n",
    "        gradient_accumulation_steps = 4\n",
    "        model_parallelize = False\n",
    "\n",
    "    ## Read in the model ##\n",
    "    if llm == 't5':\n",
    "        plm, tokenizer, model_config, WrapperClass = load_plm('t5', 't5-base')\n",
    "    else:\n",
    "        print('Unrecognized llm type')\n",
    "        return 0\n",
    "\n",
    "    ## Read in the data ##\n",
    "    dataset = read_glue_boolq_dataset(tokenizer=tokenizer,max_len=399)\n",
    "\n",
    "    ## Create the OpenPrompt objects ##\n",
    "    if prompt_type == 'fixed':\n",
    "        mytemplate = ManualTemplate(tokenizer=tokenizer, text=template)\n",
    "    elif prompt_type == 'soft':\n",
    "        mytemplate = SoftTemplate(model=plm, tokenizer=tokenizer, num_tokens=args.soft_token_num, text=template)\n",
    "    elif prompt_type == 'mixed':\n",
    "        mytemplate = MixedTemplate(model=plm, tokenizer=tokenizer, text=template)\n",
    "    elif prompt_type == 'prefix':\n",
    "        mytemplate = PrefixTuningTemplate(model=plm, tokenizer=tokenizer, text=template,\n",
    "                                          using_decoder_past_key_values=False).cuda(device='cuda:0')\n",
    "\n",
    "    ## Print out sample wrapped examples to make sure format is correct ##\n",
    "    wrapped_example = mytemplate.wrap_one_example(dataset['train'][0])\n",
    "    print('One wrapped example:', wrapped_example,'\\n')\n",
    "    wrapped_example = mytemplate.wrap_one_example(dataset['validation'][200])\n",
    "    print('One wrapped example:', wrapped_example,'\\n')\n",
    "\n",
    "    # Separate positives and negatives in the train sample and make dataloaders ##\n",
    "    ts_datasets = []\n",
    "    dataset['pos_train'] = [dataset['train'][i] for i in range(len(dataset['train'])) if dataset['train'][i].label == 1]\n",
    "    dataset['neg_train'] = [dataset['train'][i] for i in range(len(dataset['train'])) if dataset['train'][i].label == 0]\n",
    "\n",
    "    ## Create datasets for each training size, with even numbers of positive and negative ##\n",
    "    maxts = max(training_sizes)\n",
    "    dataset_comb = []\n",
    "    for a,b in zip(dataset['pos_train'][:maxts], dataset['neg_train'][:maxts]):\n",
    "        dataset_comb.append(a)\n",
    "        dataset_comb.append(b)\n",
    "    for ts in training_sizes:\n",
    "        if ts==0:\n",
    "            ts_datasets.append('nan')\n",
    "            continue\n",
    "        ts_datasets.append(dataset_comb[:2*ts])\n",
    "\n",
    "\n",
    "    ## Create validation and test dataloader ##\n",
    "    validation_dataloader = create_dataloader(dataset['validation'], mytemplate,\n",
    "                                              tokenizer, WrapperClass, max_seq_l,\n",
    "                                              batchsize_t,shuffle=False,seed=args.seed)\n",
    "    test_dataloader = create_dataloader(dataset['test'], mytemplate,\n",
    "                                        tokenizer, WrapperClass, max_seq_l,\n",
    "                                        batchsize_t,shuffle=False,seed=args.seed)\n",
    "\n",
    "    ## Training loop for specified train set sizes ##\n",
    "    val_accs = []\n",
    "    print(training_sizes)\n",
    "    for ii, ts in enumerate(training_sizes):\n",
    "\n",
    "        ## Create prompt model with template and model. If starting from an\n",
    "        ## existing model, read that in instead.\n",
    "        if starting_mod == None:\n",
    "            prompt_model = set_verbalizer_model(mytemplate,\n",
    "                                                tokenizer,\n",
    "                                                copy.deepcopy(plm),\n",
    "                                                use_cuda)\n",
    "        else:\n",
    "            print('USING MODEL THAT WAS PASSED IN')\n",
    "            prompt_model = copy.deepcopy(starting_mod)\n",
    "\n",
    "        ## Save number of model variables\n",
    "        prompt_params = sum(p.numel() for p in prompt_model.parameters())\n",
    "        llm_params = sum(p.numel() for p in plm.parameters())\n",
    "        print(f\"Number of parameters in prompt model: {prompt_params}\")\n",
    "        print(f\"Number of parameters in LLM: {llm_params}\")\n",
    "\n",
    "        ## Zero-shot evaluation against the validation set if 0 in ts.\n",
    "        if ts == 0:\n",
    "            val_accs.append(evaluate(prompt_model, validation_dataloader, use_cuda, desc=\"Valid\"))\n",
    "            print('Zero-shot val_acc =',val_accs)\n",
    "            continue\n",
    "\n",
    "        prompt_model.train()\n",
    "\n",
    "        ## Create training data loader\n",
    "        ts_dataloader = create_dataloader(ts_datasets[ii],mytemplate,\n",
    "                                          tokenizer, WrapperClass,\n",
    "                                          max_seq_l, batchsize_t,\n",
    "                                          shuffle=True,seed=args.seed)\n",
    "\n",
    "        tot_step = len(ts_dataloader)*num_epoch\n",
    "\n",
    "        ## Set optimization parameters for LLM and Prompt.\n",
    "        if prompt_type == 'fixed':\n",
    "            opt_params = set_plm_prompt_optimizers(prompt_model=prompt_model,\n",
    "                                                  tune_plm=True,\n",
    "                                                  tune_prompt=False,\n",
    "                                                  tot_step=tot_step,\n",
    "                                                  learn_rate=learn_rate,\n",
    "                                                  args=args)\n",
    "        elif prompt_type in ('soft','mixed','prefix'):\n",
    "            opt_params = set_plm_prompt_optimizers(prompt_model=prompt_model,\n",
    "                                                  tune_plm=allow_plm_to_vary,\n",
    "                                                  tune_prompt=allow_prompt_to_vary,\n",
    "                                                  tot_step=tot_step,\n",
    "                                                  learn_rate=learn_rate,\n",
    "                                                  args=args)\n",
    "        loss_func, optimizer1, scheduler1, optimizer2, scheduler2 = opt_params\n",
    "\n",
    "        actual_step = 0\n",
    "        tot_train_time = 0\n",
    "        best_val_acc = 0\n",
    "\n",
    "        ## Run the training and validation loop\n",
    "        for epoch in range(num_epoch):\n",
    "            tot_loss = 0\n",
    "            for step, inputs in enumerate(ts_dataloader):\n",
    "                prompt_model.train()\n",
    "                if use_cuda:\n",
    "                    inputs = inputs.cuda()\n",
    "                tot_train_time -= time.time()\n",
    "\n",
    "                logits = prompt_model(inputs)\n",
    "                labels = inputs['label']\n",
    "                loss = loss_func(logits, labels)\n",
    "                loss.backward()\n",
    "                tot_loss += loss.item()\n",
    "                actual_step += 1\n",
    "                torch.nn.utils.clip_grad_norm_(prompt_model.parameters(), 1.0)\n",
    "\n",
    "                if optimizer1 is not None:\n",
    "                    optimizer1.step()\n",
    "                    scheduler1.step()\n",
    "                    optimizer1.zero_grad()\n",
    "                if optimizer2 is not None:\n",
    "                    optimizer2.step()\n",
    "                    scheduler2.step()\n",
    "                    optimizer2.zero_grad()\n",
    "\n",
    "                tot_train_time += time.time()\n",
    "\n",
    "            ## Print out epoch-to-epoch changes if tuning prompt\n",
    "            if prompt_type == 'soft' and allow_prompt_to_vary and epoch+1 in (1,10,20,30,40,50,60):\n",
    "                val_acc = evaluate(prompt_model, validation_dataloader, use_cuda, desc=\"Valid\")\n",
    "                if val_acc >= best_val_acc:\n",
    "                    best_val_acc = val_acc\n",
    "                val_accs.append(val_acc)\n",
    "                print('In epoch ',epoch+1,' after step', actual_step,', best val acc = %.4f'%best_val_acc)\n",
    "\n",
    "        ## Predict with current model against train and validation sets for logging.\n",
    "        train_acc = evaluate(prompt_model, ts_dataloader, use_cuda, desc=\"Train\")\n",
    "        val_acc = evaluate(prompt_model, validation_dataloader, use_cuda, desc=\"Valid\")\n",
    "        val_accs.append(val_acc)\n",
    "        print('Train acc = ',train_acc,'  |  Val acc = ',val_acc)\n",
    "\n",
    "    ## With training loop done, run against indepedent test set for final check.\n",
    "    print('val_accs =', val_accs)\n",
    "    test_acc = evaluate(prompt_model, test_dataloader, use_cuda, desc=\"Valid\")\n",
    "    print('Test accuracy = ',test_acc)\n",
    "    return prompt_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BRcGMwhnRaOW"
   },
   "source": [
    "## Test #1\n",
    "Fine-tune a t5-base model with three different template shapes of increasing complexity and compare the zero-shot and few-shot results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "lsBiPc-kdGl3"
   },
   "outputs": [],
   "source": [
    "task = 'qa' ## \"Task\" = question/answer.\n",
    "dataset_name = 'glue_boolq' ## Using the Glue BoolQ dataset.\n",
    "prompt_type = 'fixed' ## Use a fixed, or \"manual\", template style.\n",
    "\n",
    "llm = 't5'\n",
    "\n",
    "train_sizes = (0, 16, 32, 64, 128, 256, 512)\n",
    "n_epochs = 5\n",
    "lr = 5e-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "b-jXg2RRRnTi"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: 100%|██████████| 1.18k/1.18k [00:00<00:00, 2.38MB/s]\n",
      "Downloading: 100%|██████████| 850M/850M [00:33<00:00, 26.9MB/s] \n",
      "/home/maruanottoni/miniconda3/envs/nn/lib/python3.8/site-packages/transformers/modeling_utils.py:392: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  return torch.load(checkpoint_file, map_location=\"cpu\")\n",
      "Downloading: 100%|██████████| 773k/773k [00:00<00:00, 1.57MB/s]\n",
      "/home/maruanottoni/miniconda3/envs/nn/lib/python3.8/site-packages/transformers/models/t5/tokenization_t5.py:163: FutureWarning: This tokenizer was incorrectly instantiated with a model max length of 512 which will be corrected in Transformers v5.\n",
      "For now, this behavior is kept to avoid breaking backwards compatibility when padding/encoding with `truncation is True`.\n",
      "- Be aware that you SHOULD NOT rely on t5-base automatically truncating your input to 512 when padding/encoding.\n",
      "- If you want to encode/pad to sequences longer than 512 you can either instantiate this tokenizer with `model_max_length` or pass `max_length` when encoding/padding.\n",
      "- To avoid this warning, please instantiate this tokenizer with `model_max_length` set to your preferred value.\n",
      "  warnings.warn(\n",
      "Downloading data: 100%|██████████| 4.12M/4.12M [00:00<00:00, 5.06MB/s]\n",
      "Generating train split: 100%|██████████| 9427/9427 [00:00<00:00, 9500.71 examples/s] \n",
      "Generating validation split: 100%|██████████| 3270/3270 [00:00<00:00, 11296.03 examples/s]\n",
      "Generating test split: 100%|██████████| 3245/3245 [00:00<00:00, 9491.46 examples/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "One wrapped example: [[{'text': '2018 FIFA World Cup knockout stage -- The knockout stage of the 2018 FIFA World Cup was the second and final stage of the competition, following the group stage. It began on 30 June with the round of 16 and ended on 15 July with the final match, held at the Luzhniki Stadium in Moscow. The top two teams from each group (16 in total) advanced to the knockout stage to compete in a single-elimination style tournament. A third place play-off was also played between the two losing teams of the semi-finals.', 'loss_ids': 0, 'shortenable_ids': 1}, {'text': ' is the round of 16 single elimination in world cup', 'loss_ids': 0, 'shortenable_ids': 1}, {'text': '<mask>', 'loss_ids': 1, 'shortenable_ids': 0}], {'guid': 'train-9103', 'label': 1}] \n",
      "\n",
      "One wrapped example: [[{'text': 'Major League Baseball All-Star Game -- Since 2017, home field advantage in the World Series goes to the league champion team with the higher regular season win-loss record.', 'loss_ids': 0, 'shortenable_ids': 1}, {'text': ' does the baseball all star game determine home field advantage', 'loss_ids': 0, 'shortenable_ids': 1}, {'text': '<mask>', 'loss_ids': 1, 'shortenable_ids': 0}], {'guid': 'validation-1639', 'label': 0}] \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tokenizing: 1000it [00:02, 364.15it/s]\n",
      "tokenizing: 1000it [00:02, 493.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, 16, 32, 64, 128, 256, 512)\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "The NVIDIA driver on your system is too old (found version 11080). Please update your GPU driver by downloading and installing a new version from the URL: http://www.nvidia.com/Download/index.aspx Alternatively, go to: https://pytorch.org to install a PyTorch version that has been compiled with your version of the CUDA driver.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m## Simplest template\u001b[39;00m\n\u001b[1;32m      2\u001b[0m templ1 \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'''\u001b[39m\u001b[38;5;124m{\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mplaceholder\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m:\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext_a\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m} \u001b[39m\u001b[38;5;124m{\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mplaceholder\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m:\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext_b\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m} \u001b[39m\u001b[38;5;124m{\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmask\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m}\u001b[39m\u001b[38;5;124m'''\u001b[39m\n\u001b[0;32m----> 4\u001b[0m prompt_mod \u001b[38;5;241m=\u001b[39m \u001b[43mfull_program\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtask\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mtask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m                          \u001b[49m\u001b[43mllm\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mllm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m                          \u001b[49m\u001b[43mprompt_type\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mprompt_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m                          \u001b[49m\u001b[43mdataset_name\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mdataset_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m                          \u001b[49m\u001b[43mtemplate\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mtempl1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m                          \u001b[49m\u001b[43mtraining_sizes\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mtrain_sizes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m                          \u001b[49m\u001b[43mnum_epoch\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mn_epochs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m                          \u001b[49m\u001b[43mlearn_rate\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mlr\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[43m                        \u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[7], line 89\u001b[0m, in \u001b[0;36mfull_program\u001b[0;34m(task, llm, prompt_type, dataset_name, template, training_sizes, num_epoch, learn_rate, allow_plm_to_vary, allow_prompt_to_vary, starting_mod, use_cuda)\u001b[0m\n\u001b[1;32m     84\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m ii, ts \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(training_sizes):\n\u001b[1;32m     85\u001b[0m \n\u001b[1;32m     86\u001b[0m     \u001b[38;5;66;03m## Create prompt model with template and model. If starting from an\u001b[39;00m\n\u001b[1;32m     87\u001b[0m     \u001b[38;5;66;03m## existing model, read that in instead.\u001b[39;00m\n\u001b[1;32m     88\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m starting_mod \u001b[38;5;241m==\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m---> 89\u001b[0m         prompt_model \u001b[38;5;241m=\u001b[39m \u001b[43mset_verbalizer_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmytemplate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     90\u001b[0m \u001b[43m                                            \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     91\u001b[0m \u001b[43m                                            \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdeepcopy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mplm\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     92\u001b[0m \u001b[43m                                            \u001b[49m\u001b[43muse_cuda\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     93\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     94\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mUSING MODEL THAT WAS PASSED IN\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "Cell \u001b[0;32mIn[6], line 7\u001b[0m, in \u001b[0;36mset_verbalizer_model\u001b[0;34m(mytemplate, tokenizer, plm, use_cuda)\u001b[0m\n\u001b[1;32m      4\u001b[0m prompt_model \u001b[38;5;241m=\u001b[39m PromptForClassification(plm\u001b[38;5;241m=\u001b[39mplm, template\u001b[38;5;241m=\u001b[39mmytemplate, verbalizer\u001b[38;5;241m=\u001b[39mmyverbalizer,\n\u001b[1;32m      5\u001b[0m                                        freeze_plm \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_cuda:\n\u001b[0;32m----> 7\u001b[0m     prompt_model \u001b[38;5;241m=\u001b[39m \u001b[43mprompt_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcuda\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m prompt_model\n",
      "File \u001b[0;32m~/miniconda3/envs/nn/lib/python3.8/site-packages/torch/nn/modules/module.py:916\u001b[0m, in \u001b[0;36mModule.cuda\u001b[0;34m(self, device)\u001b[0m\n\u001b[1;32m    899\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcuda\u001b[39m(\u001b[38;5;28mself\u001b[39m: T, device: Optional[Union[\u001b[38;5;28mint\u001b[39m, device]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m T:\n\u001b[1;32m    900\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Move all model parameters and buffers to the GPU.\u001b[39;00m\n\u001b[1;32m    901\u001b[0m \n\u001b[1;32m    902\u001b[0m \u001b[38;5;124;03m    This also makes associated parameters and buffers different objects. So\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    914\u001b[0m \u001b[38;5;124;03m        Module: self\u001b[39;00m\n\u001b[1;32m    915\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 916\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcuda\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/nn/lib/python3.8/site-packages/torch/nn/modules/module.py:780\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    778\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[1;32m    779\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m--> 780\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    782\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    783\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    784\u001b[0m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    785\u001b[0m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    790\u001b[0m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    791\u001b[0m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/nn/lib/python3.8/site-packages/torch/nn/modules/module.py:780\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    778\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[1;32m    779\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m--> 780\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    782\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    783\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    784\u001b[0m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    785\u001b[0m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    790\u001b[0m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    791\u001b[0m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/nn/lib/python3.8/site-packages/torch/nn/modules/module.py:780\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    778\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[1;32m    779\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m--> 780\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    782\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    783\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    784\u001b[0m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    785\u001b[0m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    790\u001b[0m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    791\u001b[0m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/nn/lib/python3.8/site-packages/torch/nn/modules/module.py:805\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    801\u001b[0m \u001b[38;5;66;03m# Tensors stored in modules are graph leaves, and we don't want to\u001b[39;00m\n\u001b[1;32m    802\u001b[0m \u001b[38;5;66;03m# track autograd history of `param_applied`, so we have to use\u001b[39;00m\n\u001b[1;32m    803\u001b[0m \u001b[38;5;66;03m# `with torch.no_grad():`\u001b[39;00m\n\u001b[1;32m    804\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m--> 805\u001b[0m     param_applied \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparam\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    806\u001b[0m p_should_use_set_data \u001b[38;5;241m=\u001b[39m compute_should_use_set_data(param, param_applied)\n\u001b[1;32m    808\u001b[0m \u001b[38;5;66;03m# subclasses may have multiple child tensors so we need to use swap_tensors\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/nn/lib/python3.8/site-packages/torch/nn/modules/module.py:916\u001b[0m, in \u001b[0;36mModule.cuda.<locals>.<lambda>\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m    899\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcuda\u001b[39m(\u001b[38;5;28mself\u001b[39m: T, device: Optional[Union[\u001b[38;5;28mint\u001b[39m, device]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m T:\n\u001b[1;32m    900\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Move all model parameters and buffers to the GPU.\u001b[39;00m\n\u001b[1;32m    901\u001b[0m \n\u001b[1;32m    902\u001b[0m \u001b[38;5;124;03m    This also makes associated parameters and buffers different objects. So\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    914\u001b[0m \u001b[38;5;124;03m        Module: self\u001b[39;00m\n\u001b[1;32m    915\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 916\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_apply(\u001b[38;5;28;01mlambda\u001b[39;00m t: \u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcuda\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m~/miniconda3/envs/nn/lib/python3.8/site-packages/torch/cuda/__init__.py:314\u001b[0m, in \u001b[0;36m_lazy_init\u001b[0;34m()\u001b[0m\n\u001b[1;32m    312\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCUDA_MODULE_LOADING\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m os\u001b[38;5;241m.\u001b[39menviron:\n\u001b[1;32m    313\u001b[0m     os\u001b[38;5;241m.\u001b[39menviron[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCUDA_MODULE_LOADING\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLAZY\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 314\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_C\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_cuda_init\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    315\u001b[0m \u001b[38;5;66;03m# Some of the queued calls may reentrantly call _lazy_init();\u001b[39;00m\n\u001b[1;32m    316\u001b[0m \u001b[38;5;66;03m# we need to just return without initializing in that case.\u001b[39;00m\n\u001b[1;32m    317\u001b[0m \u001b[38;5;66;03m# However, we must not let any *other* threads in!\u001b[39;00m\n\u001b[1;32m    318\u001b[0m _tls\u001b[38;5;241m.\u001b[39mis_initializing \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: The NVIDIA driver on your system is too old (found version 11080). Please update your GPU driver by downloading and installing a new version from the URL: http://www.nvidia.com/Download/index.aspx Alternatively, go to: https://pytorch.org to install a PyTorch version that has been compiled with your version of the CUDA driver."
     ]
    }
   ],
   "source": [
    "## Simplest template\n",
    "templ1 = '''{\"placeholder\":\"text_a\"} {\"placeholder\":\"text_b\"} {\"mask\"}'''\n",
    "\n",
    "prompt_mod = full_program(task = task,\n",
    "                          llm = llm,\n",
    "                          prompt_type = prompt_type,\n",
    "                          dataset_name = dataset_name,\n",
    "                          template = templ1,\n",
    "                          training_sizes = train_sizes,\n",
    "                          num_epoch = n_epochs,\n",
    "                          learn_rate = lr,\n",
    "                        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wG3sbXd7Rn0O"
   },
   "outputs": [],
   "source": [
    "## Simplest plus punctuation\n",
    "templ2 = '''{\"placeholder\":\"text_a\", \"shortenable\":False} . {\"placeholder\":\"text_b\"} ? {\"mask\"}'''\n",
    "\n",
    "prompt_mod = full_program(task = task,\n",
    "                          llm = llm,\n",
    "                          prompt_type = prompt_type,\n",
    "                          dataset_name = dataset_name,\n",
    "                          template = templ2,\n",
    "                          training_sizes = train_sizes,\n",
    "                          num_epoch = n_epochs,\n",
    "                          learn_rate = lr,\n",
    "                        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1nqi-Qh2RkCs"
   },
   "outputs": [],
   "source": [
    "## \"Suitable\" template\n",
    "templ3 = '''hypothesis: {\"placeholder\":\"text_a\", \"shortenable\":False, \"post_processing\": lambda x:x+\".\"} premise: {\"placeholder\":\"text_b\", \"post_processing\": lambda x:x+\"?\"} The answer was {\"mask\"} .'''\n",
    "\n",
    "prompt_mod = full_program(task = task,\n",
    "                          llm = llm,\n",
    "                          prompt_type = prompt_type,\n",
    "                          dataset_name = dataset_name,\n",
    "                          template = templ3,\n",
    "                          training_sizes = train_sizes,\n",
    "                          num_epoch = n_epochs,\n",
    "                          learn_rate = lr,\n",
    "                        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "J2Z7Ya9pN2Wt"
   },
   "outputs": [],
   "source": [
    "## Results from run used to create Ch3 tutorial 2, Figure 1\n",
    "fig1_x = (0, 16, 32, 64, 128, 256, 512)\n",
    "\n",
    "fig1_templ1 = [0.501, 0.511, 0.513, 0.518, 0.526, 0.578, 0.546]\n",
    "fig1_templ1test = 0.526\n",
    "\n",
    "fig1_templ2 = [0.526, 0.508, 0.482, 0.516, 0.598, 0.628, 0.638]\n",
    "fig1_templ2test = 0.609\n",
    "\n",
    "fig1_templ3 = [0.564, 0.489, 0.503, 0.535, 0.638, 0.686, 0.640]\n",
    "fig1_templ3test = 0.649"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "No-mUq7wR0c3"
   },
   "source": [
    "## Test #2\n",
    "Fine-tune a t5-base model with 10 different templates, representing variants of prefix, cloze, and information retrieval styles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OS0P7OUDR5RJ"
   },
   "outputs": [],
   "source": [
    "task = 'qa' ## \"Task\" = question/answer.\n",
    "dataset_name = 'glue_boolq' ## Using the Glue BoolQ dataset.\n",
    "prompt_type = 'fixed' ## Use a fixed, or \"manual\", template style.\n",
    "\n",
    "llm = 't5'\n",
    "\n",
    "train_sizes = (0, 16, 32, 64, 128, 256, 512)\n",
    "n_epochs = 5\n",
    "lr = 5e-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "r6br2xnC9Kjc"
   },
   "outputs": [],
   "source": [
    "templ = '''hypothesis: {\"placeholder\":\"text_a\", \"shortenable\":False, \"post_processing\": lambda x:x+\".\"} premise: {\"placeholder\":\"text_b\", \"post_processing\": lambda x:x+\"?\"} The answer was {\"mask\"} .'''\n",
    "#templ = '''{\"placeholder\":\"text_a\", \"shortenable\":False, \"post_processing\": lambda x:x+\".\"} Question: {\"placeholder\":\"text_b\"} ? The answer was {\"mask\"}'''\n",
    "#templ = '''Information: {\"placeholder\":\"text_a\", \"shortenable\":False, \"post_processing\": lambda x:x+\".\"} Question: {\"placeholder\":\"text_b\", \"post_processing\": lambda x:x+\"?\"} The answer is {\"mask\"} .'''\n",
    "#templ = '''Question: {\"placeholder\":\"text_b\"} ? Is the answer yes or no? {\"mask\"} . Context: {\"placeholder\":\"text_a\", \"shortenable\":False, \"post_processing\": lambda x:x+\".\"}'''\n",
    "#templ = '''Context: {\"placeholder\":\"text_a\", \"post_processing\": lambda x:x if x[-1] == '.' else x+\".\"} . Based on this, of the two options (yes or no), {\"mask\"} is the answer to this question: {\"placeholder\":\"text_b\", \"post_processing\": lambda x:x if x[-1] == '?' else x+\"?\"}'''\n",
    "#templ = '''Context: {\"placeholder\":\"text_a\", \"shortenable\":False, \"post_processing\": lambda x:x if x[-1] == '.' else x+\".\"} Based on the information in this paragraph, {\"placeholder\":\"text_b\", \"post_processing\": lambda x:x if x[-1] == '?' else x+\"?\"} {\"mask\"} .'''\n",
    "#templ = '''premise: {\"placeholder\":\"text_b\", \"post_processing\": lambda x:x+\"?\"} The answer was {\"mask\"} .'''\n",
    "#templ = '''Information: {\"placeholder\":\"text_a\", \"shortenable\":False, \"post_processing\": lambda x:x+\".\"} Question: {\"placeholder\":\"text_b\", \"post_processing\": lambda x:x+\"?\"} The answer is {\"mask\"} .'''\n",
    "#templ = '''The answer to the question \" {\"placeholder\":\"text_b\", \"post_processing\": lambda x:x+\"?\"} \" is {\"mask\"} .'''\n",
    "#templ = '''hypothesis: {\"placeholder\":\"text_a\", \"shortenable\":False, \"post_processing\": lambda x:x+\".\"} premise: {\"placeholder\":\"text_b\"} The answer was {\"mask\"}'''\n",
    "\n",
    "prompt_mod = full_program(task = task,\n",
    "                          llm = llm,\n",
    "                          prompt_type = prompt_type,\n",
    "                          dataset_name = dataset_name,\n",
    "                          template = templ,\n",
    "                          training_sizes = train_sizes,\n",
    "                          num_epoch = n_epochs,\n",
    "                          learn_rate = lr,\n",
    "                         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "X-42GIDuN02E"
   },
   "outputs": [],
   "source": [
    "## Results from run used to create Ch3 tutorial 2, Figure 2\n",
    "fig2_x = (0, 16, 32, 64, 128, 256, 512)\n",
    "fig2_templ1  = [0.564, 0.489, 0.503, 0.535, 0.638, 0.686, 0.64]\n",
    "fig2_templ1test = 0.649\n",
    "\n",
    "fig2_templ2  = [0.541, 0.504, 0.531, 0.544, 0.546, 0.596, 0.611]\n",
    "fig2_templ2test = 0.611\n",
    "\n",
    "fig2_templ3  = [0.573, 0.5, 0.551, 0.608, 0.531, 0.704, 0.671]\n",
    "fig2_templ3test = 0.670\n",
    "\n",
    "fig2_templ4  = [0.524, 0.499, 0.531, 0.499, 0.529, 0.616, 0.61]\n",
    "fig2_templ4test = 0.583\n",
    "\n",
    "fig2_templ5  = [0.531, 0.502, 0.519, 0.501, 0.526, 0.56, 0.676]\n",
    "fig2_templ5test = 0.667\n",
    "\n",
    "fig2_templ6  = [0.516, 0.485, 0.491, 0.525, 0.539, 0.686, 0.551]\n",
    "fig2_templ6test = 0.571\n",
    "\n",
    "fig2_templ7  = [0.495, 0.492, 0.457, 0.51, 0.508, 0.522, 0.557]\n",
    "fig2_templ7test = 0.539\n",
    "\n",
    "fig2_templ8  = [0.498, 0.487, 0.504, 0.5, 0.51, 0.54, 0.566]\n",
    "fig2_templ8test = 0.533\n",
    "\n",
    "fig2_templ9  = [0.531, 0.506, 0.512, 0.511, 0.518, 0.544, 0.516]\n",
    "fig2_templ9test = 0.497\n",
    "\n",
    "fig2_templ10 = [0.494, 0.496, 0.507, 0.507, 0.505, 0.537, 0.564]\n",
    "fig2_templ10test = 0.534"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uCHFw6fgUa_T"
   },
   "source": [
    "## Test #3.1\n",
    "Compare fine-tuned soft prompt models with two different starting points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "U8FdywTuUhta"
   },
   "outputs": [],
   "source": [
    "task = 'qa' ## \"Task\" = question/answer. \n",
    "dataset_name = 'glue_boolq' ## Using the Glue BoolQ dataset.\n",
    "prompt_type = 'soft' ## Use a soft prompt template style.\n",
    "\n",
    "llm = 't5'\n",
    "\n",
    "train_sizes = (128,)\n",
    "n_epochs = 60\n",
    "lr = 5e-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 809
    },
    "id": "h4njVMEu9Kic",
    "outputId": "3631aa09-06c7-4ec2-da65-207a0213b5ee"
   },
   "outputs": [],
   "source": [
    "templ1 = '''{\"placeholder\":\"text_a\"} {\"placeholder\":\"text_b\"} {\"mask\"}'''\n",
    "\n",
    "prompt_mod = full_program(task = task,\n",
    "                          llm = llm,\n",
    "                          prompt_type = prompt_type,\n",
    "                          dataset_name = dataset_name,\n",
    "                          template = templ1,\n",
    "                          training_sizes = train_sizes,\n",
    "                          num_epoch = n_epochs,\n",
    "                          learn_rate = lr,\n",
    "                          allow_plm_to_vary = False\n",
    "                         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 785
    },
    "id": "K365uNvxVIPh",
    "outputId": "9f9a5ff0-6c5b-45dd-992f-dfbda9891d1a"
   },
   "outputs": [],
   "source": [
    "templ2 = '''hypothesis: {\"placeholder\":\"text_a\", \"shortenable\":False} premise: {\"placeholder\":\"text_b\"} The answer was {\"mask\"} .'''\n",
    "\n",
    "prompt_mod = full_program(task = task,\n",
    "                          llm = llm,\n",
    "                          prompt_type = prompt_type,\n",
    "                          dataset_name = dataset_name,\n",
    "                          template = templ2,\n",
    "                          training_sizes = train_sizes,\n",
    "                          num_epoch = n_epochs,\n",
    "                          learn_rate = lr,\n",
    "                          allow_plm_to_vary = False\n",
    "                         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OWIhvNVpZSYE"
   },
   "outputs": [],
   "source": [
    "## Results from run used to create Ch3 tutorial 2, Figure 3 left panel\n",
    "fig31_x = (0, 1, 10, 20, 30, 40, 50, 60)\n",
    "fig31_templ1 = [0.501, 0.501, 0.511, 0.511, 0.511, 0.511, 0.511, 0.511]\n",
    "fig31_templ2 = [0.558, 0.565, 0.579, 0.584, 0.593, 0.593, 0.585, 0.603]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Mly6HPlFYtRv"
   },
   "source": [
    "## Test #3.2\n",
    "\n",
    "Compare four different tuning approaches:\n",
    "\n",
    "1) Soft prompt tuning only\n",
    "\n",
    "2) LLM tuning only\n",
    "\n",
    "3) Soft prompt tuning, then LLM tuning\n",
    "\n",
    "4) Simultaneous soft prompt and LLM tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "B260OCIIbR5C"
   },
   "outputs": [],
   "source": [
    "task = 'qa' ## \"Task\" = question/answer.\n",
    "dataset_name = 'glue_boolq' ## Using the Glue BoolQ dataset.\n",
    "llm = 't5'\n",
    "\n",
    "templ = '''hypothesis: {\"placeholder\":\"text_a\", \"shortenable\":False} premise: {\"placeholder\":\"text_b\"} The answer was {\"mask\"} .'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pvm5dv_ybYw3"
   },
   "outputs": [],
   "source": [
    "## 1) Soft prompt tuning only\n",
    "\n",
    "train_sizes = (0, 16, 32, 64, 128, 256, 512)\n",
    "n_epochs = 8\n",
    "lr = 1e-4\n",
    "prompt_type = 'soft' ## Use a soft prompt template style.\n",
    "prompt_mod = full_program(task = task,\n",
    "                          llm = llm,\n",
    "                          prompt_type = prompt_type,\n",
    "                          dataset_name = dataset_name,\n",
    "                          template = templ,\n",
    "                          training_sizes = train_sizes,\n",
    "                          num_epoch = n_epochs,\n",
    "                          learn_rate = lr,\n",
    "                          allow_plm_to_vary = False\n",
    "                        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6V9_h9mVbaNo"
   },
   "outputs": [],
   "source": [
    "## 2) LLM tuning only\n",
    "\n",
    "train_sizes = (0, 16, 32, 64, 128, 256, 512)\n",
    "n_epochs = 8\n",
    "lr = 1e-4\n",
    "prompt_type = 'fixed' ## Use a fixed, or \"manual\", template style.\n",
    "prompt_mod = full_program(task = task,\n",
    "                          llm = llm,\n",
    "                          prompt_type = prompt_type,\n",
    "                          dataset_name = dataset_name,\n",
    "                          template = templ,\n",
    "                          training_sizes = train_sizes,\n",
    "                          num_epoch = n_epochs,\n",
    "                          learn_rate = lr,\n",
    "                        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "MP2xuMfObc6_",
    "outputId": "6eec8b89-7e9d-4491-f23e-1512a0c027a2"
   },
   "outputs": [],
   "source": [
    "## 3) Soft prompt tuning, then LLM tuning\n",
    "\n",
    "train_sizes = (128,)\n",
    "n_epochs = 60\n",
    "lr = 1e-4\n",
    "prompt_type = 'soft' ## Use a soft prompt template style.\n",
    "prompt_mod = full_program(task = task,\n",
    "                          llm = llm,\n",
    "                          prompt_type = prompt_type,\n",
    "                          dataset_name = dataset_name,\n",
    "                          template = templ,\n",
    "                          training_sizes = train_sizes,\n",
    "                          num_epoch = n_epochs,\n",
    "                          learn_rate = lr,\n",
    "                          allow_plm_to_vary = False\n",
    "                        )\n",
    "\n",
    "\n",
    "train_sizes = (0, 16, 32, 64, 128, 256, 512)\n",
    "n_epochs = 8\n",
    "lr = 1e-4\n",
    "prompt_type = 'soft' ## Use a soft prompt template style.\n",
    "prompt_mod = full_program(task = task,\n",
    "                          llm = llm,\n",
    "                          prompt_type = prompt_type,\n",
    "                          dataset_name = dataset_name,\n",
    "                          template = templ,\n",
    "                          training_sizes = train_sizes,\n",
    "                          num_epoch = n_epochs,\n",
    "                          learn_rate = lr,\n",
    "                          allow_prompt_to_vary = False,\n",
    "                          starting_mod = prompt_mod\n",
    "                        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9daiFYSt9Kg_"
   },
   "outputs": [],
   "source": [
    "## 4) Simultaneous soft prompt and LLM tuning\n",
    "\n",
    "train_sizes = (0, 16, 32, 64, 128, 256, 512)\n",
    "n_epochs = 8\n",
    "lr = 1e-4\n",
    "prompt_type = 'soft' ## Use a soft prompt template style.\n",
    "prompt_mod = full_program(task = task,\n",
    "                          llm = llm,\n",
    "                          prompt_type = prompt_type,\n",
    "                          dataset_name = dataset_name,\n",
    "                          template = templ,\n",
    "                          training_sizes = train_sizes,\n",
    "                          num_epoch = n_epochs,\n",
    "                          learn_rate = lr,\n",
    "                          allow_plm_to_vary = True\n",
    "                        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QIQWXn2mvsO4"
   },
   "outputs": [],
   "source": [
    "## Results from run used to create Ch3 tutorial 2, Figure 3 right panel\n",
    "fig32_x = (0, 16, 32, 64, 128, 256, 512)\n",
    "fig32_test1 = [0.558, 0.557, 0.563, 0.585, 0.593, 0.622, 0.645] # Prompt\n",
    "fig32_test2 = [0.551, 0.563, 0.635, 0.664, 0.680, 0.747, 0.755] # LLM\n",
    "fig32_test3 = [0.600, 0.602, 0.555, 0.570, 0.674, 0.741, 0.729] # Prompt > LLM\n",
    "fig32_test4 = [0.558, 0.624, 0.635, 0.683, 0.726, 0.759, 0.761]# Prompt + LLM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zu9AMVHlZYBI"
   },
   "source": [
    "## Plot the results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b8kEK52YZab9"
   },
   "source": [
    "### Test 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 490
    },
    "id": "or1BqMXdiIfv",
    "outputId": "2dbf02a2-b38e-4f32-b4e6-a77f1df953f0"
   },
   "outputs": [],
   "source": [
    "## Ch 4 tutorial fig 1\n",
    "x = fig1_x\n",
    "y1 = fig1_templ1\n",
    "y2 = fig1_templ2\n",
    "y3 = fig1_templ3\n",
    "y1test = fig1_templ1test\n",
    "y2test = fig1_templ2test\n",
    "y3test = fig1_templ3test\n",
    "\n",
    "fig1 = plt.figure()\n",
    "ax11 = fig1.add_subplot(111)\n",
    "\n",
    "ax11.plot(range(len(x)), y1, color='#cc0000', ls='-', lw=2, label='Simplest')\n",
    "ax11.scatter(range(len(x)), y1, color='#cc0000')\n",
    "ax11.plot(range(len(x)), y2, color='#6d9eeb', ls='--', lw=2, label='Simplest + Punct')\n",
    "ax11.scatter(range(len(x)), y2, color='#6d9eeb')\n",
    "ax11.plot(range(len(x)), y3, color='#5CA894', ls=':', lw=2, label='Suitable Template')\n",
    "ax11.scatter(range(len(x)), y3, color='#5CA894')\n",
    "ax11.plot([0,len(x)-1],[0.5,0.5],'k--',alpha=0.5)\n",
    "ax11.text(5.9,0.495,'Coin-flip',ha='right',va='top',color='k',alpha=0.5,fontsize=16)\n",
    "\n",
    "ax11.scatter(6,y1test,color='#cc0000',marker='X',s=200)\n",
    "ax11.scatter(6,y2test,color='#6d9eeb',marker='X',s=200)\n",
    "ax11.scatter(6,y3test,color='#5CA894',marker='X',s=200)\n",
    "\n",
    "ax11.set_xticks(range(len(x)))\n",
    "ax11.set_xticklabels(x,fontsize=16)\n",
    "ax11.set_xlabel('# training examples (per class)',fontsize=20)\n",
    "\n",
    "ax11.set_yticks([0.5, 0.55, 0.6, 0.65, 0.7])\n",
    "ax11.set_yticklabels(['0.50', '0.55', '0.60', '0.65', '0.70'],fontsize=16)\n",
    "ax11.set_ylabel('Validation Set Accuracy',fontsize=20)\n",
    "\n",
    "ax11.xaxis.set_ticks_position('both')\n",
    "ax11.tick_params(axis='x', which='major', direction='in')\n",
    "ax11.tick_params(axis='x', which='minor', direction='in')\n",
    "ax11.yaxis.set_ticks_position('both')\n",
    "ax11.tick_params(axis='y', which='major', direction='in')\n",
    "ax11.tick_params(axis='y', which='minor', direction='in')\n",
    "\n",
    "plt.gca().yaxis.set_minor_locator(MultipleLocator(0.01))\n",
    "\n",
    "ax11.legend(fontsize=15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "InH6ygcSZc8p"
   },
   "source": [
    "### Test 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 407
    },
    "id": "PD00QmO9fUuR",
    "outputId": "fc10cd64-3f77-4849-f970-3353ba03a160"
   },
   "outputs": [],
   "source": [
    "# Ch4 tutorial fig2\n",
    "\n",
    "ts = fig2_x\n",
    "e1 = fig2_templ1\n",
    "e2 = fig2_templ2\n",
    "e3 = fig2_templ3\n",
    "e4 = fig2_templ4\n",
    "e5 = fig2_templ5\n",
    "e6 = fig2_templ6\n",
    "e7 = fig2_templ7\n",
    "e8 = fig2_templ8\n",
    "e9 = fig2_templ9\n",
    "e10 = fig2_templ10\n",
    "\n",
    "orig_test = fig2_templ1test\n",
    "prefix_test = [fig2_templ2test, fig2_templ3test, fig2_templ4test]\n",
    "cloze_test = [fig2_templ5test, fig2_templ6test]\n",
    "info_test = [fig2_templ7test, fig2_templ8test, fig2_templ9test, fig2_templ10test]\n",
    "\n",
    "\n",
    "fig2 = plt.figure(figsize=(12,4))\n",
    "ax21 = fig2.add_subplot(131)\n",
    "ax22 = fig2.add_subplot(132)\n",
    "ax23 = fig2.add_subplot(133)\n",
    "\n",
    "bshade_min1 = [min(e2[i],e3[i],e4[i]) for i in range(len(ts))]\n",
    "bshade_max1 = [max(e2[i],e3[i],e4[i]) for i in range(len(ts))]\n",
    "bshade_min2 = [min(e5[i],e6[i]) for i in range(len(ts))]\n",
    "bshade_max2 = [max(e5[i],e6[i]) for i in range(len(ts))]\n",
    "bshade_min3 = [min(e7[i],e8[i],e9[i],e10[i]) for i in range(len(ts))]\n",
    "bshade_max3 = [max(e7[i],e8[i],e9[i],e10[i]) for i in range(len(ts))]\n",
    "\n",
    "ax21.fill_between(range(len(ts)),[i-0.01 for i in e1],[i+0.01 for i in e1],color='k',alpha=0.8, label='Ex. 1')\n",
    "ax21.fill_between(range(len(ts)),bshade_min1,bshade_max1, color='r', alpha=0.6, label='Ex. 2-4')\n",
    "ax21.fill_between(range(len(ts)),bshade_min2,bshade_max2, color='b', alpha=0.1)\n",
    "ax21.fill_between(range(len(ts)),bshade_min3,bshade_max3, color='g', alpha=0.1)\n",
    "\n",
    "ax22.fill_between(range(len(ts)),[i-0.01 for i in e1],[i+0.01 for i in e1],color='k',alpha=0.8, label='Ex. 1')\n",
    "ax22.fill_between(range(len(ts)),bshade_min1,bshade_max1, color='r', alpha=0.1)\n",
    "ax22.fill_between(range(len(ts)),bshade_min2,bshade_max2, color='b', alpha=0.6, label='Ex. 5-6')\n",
    "ax22.fill_between(range(len(ts)),bshade_min3,bshade_max3, color='g', alpha=0.1)\n",
    "\n",
    "ax23.fill_between(range(len(ts)),[i-0.01 for i in e1],[i+0.01 for i in e1],color='k',alpha=0.8, label='Ex. 1')\n",
    "ax23.fill_between(range(len(ts)),bshade_min1,bshade_max1, color='r', alpha=0.1)\n",
    "ax23.fill_between(range(len(ts)),bshade_min2,bshade_max2, color='b', alpha=0.1)\n",
    "ax23.fill_between(range(len(ts)),bshade_min3,bshade_max3, color='g', alpha=0.6, label='Ex. 7-10')\n",
    "\n",
    "ax21.scatter(6,orig_test,color='k',marker='x',s=200)\n",
    "ax21.scatter([6,6,6],prefix_test,color='r',marker='x',s=200,label='Test set eval')\n",
    "ax22.scatter([6,6],cloze_test,color='b',marker='x',s=200,label='Test set eval')\n",
    "ax23.scatter([6,6,6,6],info_test,color='g',marker='x',s=200,label='Test set eval')\n",
    "\n",
    "ax21.plot([0,len(ts)-1],[0.5,0.5],'k--',alpha=0.5)\n",
    "ax21.text(5.9,0.501,'Coin-flip',ha='right',va='bottom',color='k',alpha=0.5)\n",
    "ax22.plot([0,len(ts)-1],[0.5,0.5],'k--',alpha=0.5)\n",
    "ax22.text(5.9,0.501,'Coin-flip',ha='right',va='bottom',color='k',alpha=0.5)\n",
    "ax23.plot([0,len(ts)-1],[0.5,0.5],'k--',alpha=0.5)\n",
    "ax23.text(5.9,0.501,'Coin-flip',ha='right',va='bottom',color='k',alpha=0.5)\n",
    "\n",
    "ax21.set_xticks(range(len(ts)))\n",
    "ax21.set_xticklabels(ts,fontsize=12)\n",
    "ax21.set_xlabel('Number of training examples',fontsize=15)\n",
    "ax21.set_ylabel('Validation Set Accuracy',fontsize=15)\n",
    "ax21.set_ylim(0.42, 0.72)\n",
    "ax21.set_yticks([0.45, 0.5, 0.55, 0.6, 0.65, 0.7])\n",
    "ax21.set_yticklabels(['0.45', '0.50', '0.55', '0.60', '0.65', '0.70'],fontsize=12)\n",
    "ax21.text(0., 0.71,'Prefix\\nPrompts',ha='left',va='top',fontsize=18)\n",
    "ax21.legend(loc=4)\n",
    "\n",
    "ax22.set_xticks(range(len(ts)))\n",
    "ax22.set_xticklabels(ts,fontsize=12)\n",
    "ax22.set_xlabel('Number of training examples',fontsize=15)\n",
    "ax22.set_ylabel('Validation Set Accuracy',fontsize=15)\n",
    "ax22.set_ylim(0.42, 0.72)\n",
    "ax22.set_yticks([0.45, 0.5, 0.55, 0.6, 0.65, 0.7])\n",
    "ax22.set_yticklabels(['0.45', '0.50', '0.55', '0.60', '0.65', '0.70'],fontsize=12)\n",
    "ax22.text(0., 0.71,'Cloze\\nPrompts',ha='left',va='top',fontsize=18)\n",
    "ax22.legend(loc=4)\n",
    "\n",
    "ax23.set_xticks(range(len(ts)))\n",
    "ax23.set_xticklabels(ts,fontsize=12)\n",
    "ax23.set_xlabel('Number of training examples',fontsize=15)\n",
    "ax23.set_ylabel('Validation Set Accuracy',fontsize=15)\n",
    "ax23.set_ylim(0.42, 0.72)\n",
    "ax23.set_yticks([0.45, 0.5, 0.55, 0.6, 0.65, 0.7])\n",
    "ax23.set_yticklabels(['0.45', '0.50', '0.55', '0.60', '0.65', '0.70'],fontsize=12)\n",
    "ax23.text(0., 0.71,'Information\\nRetrieval',ha='left',va='top',fontsize=18)\n",
    "ax23.legend(loc=4)\n",
    "\n",
    "ax21.xaxis.set_ticks_position('both')\n",
    "ax21.tick_params(axis='x', which='major', direction='in')\n",
    "ax21.tick_params(axis='x', which='minor', direction='in')\n",
    "ax21.yaxis.set_ticks_position('both')\n",
    "ax21.tick_params(axis='y', which='major', direction='in')\n",
    "ax21.tick_params(axis='y', which='minor', direction='in')\n",
    "ax21.yaxis.set_minor_locator(MultipleLocator(0.01))\n",
    "\n",
    "ax22.xaxis.set_ticks_position('both')\n",
    "ax22.tick_params(axis='x', which='major', direction='in')\n",
    "ax22.tick_params(axis='x', which='minor', direction='in')\n",
    "ax22.yaxis.set_ticks_position('both')\n",
    "ax22.tick_params(axis='y', which='major', direction='in')\n",
    "ax22.tick_params(axis='y', which='minor', direction='in')\n",
    "ax22.yaxis.set_minor_locator(MultipleLocator(0.01))\n",
    "\n",
    "ax23.xaxis.set_ticks_position('both')\n",
    "ax23.tick_params(axis='x', which='major', direction='in')\n",
    "ax23.tick_params(axis='x', which='minor', direction='in')\n",
    "ax23.yaxis.set_ticks_position('both')\n",
    "ax23.tick_params(axis='y', which='major', direction='in')\n",
    "ax23.tick_params(axis='y', which='minor', direction='in')\n",
    "ax23.yaxis.set_minor_locator(MultipleLocator(0.01))\n",
    "\n",
    "fig2.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YyvxwjtpZfSP"
   },
   "source": [
    "### Test 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 407
    },
    "id": "wx2iyyutWQJp",
    "outputId": "50514808-eca1-4ef8-ddf4-9920da21d2d4"
   },
   "outputs": [],
   "source": [
    "steps_1 = fig31_x\n",
    "c1 = fig31_templ1\n",
    "c2 = fig31_templ2\n",
    "\n",
    "steps_2 = fig32_x\n",
    "c3 = fig32_test1\n",
    "c4 = fig32_test2\n",
    "c5 = fig32_test3\n",
    "c6 = fig32_test4\n",
    "\n",
    "fig3 = plt.figure(figsize=(10,4))\n",
    "ax31 = fig3.add_subplot(121)\n",
    "ax32 = fig3.add_subplot(122)\n",
    "\n",
    "ax31.plot(range(len(steps_1)), c1, label = 'Featureless prompt', color='#cc0000', ls='--', lw=2)\n",
    "ax31.plot(range(len(steps_1)), c2, label = 'Engineered prompt',color='k',ls='-', lw=2)\n",
    "ax31.set_xticks(range(len(steps_1)))\n",
    "ax31.set_xticklabels(steps_1,fontsize=12)\n",
    "ax31.set_xlabel('Number of training epochs',fontsize=15)\n",
    "ax31.set_ylabel('Validation Set Accuracy',fontsize=15)\n",
    "ax31.text(0,0.795,'Prompt\\nTuning',ha='left',va='top',fontsize=20)\n",
    "ax31.set_ylim(0.48, 0.81)\n",
    "ax31.set_yticks([0.5, 0.55, 0.6, 0.65, 0.7, 0.75, 0.8])\n",
    "ax31.set_yticklabels(['0.50', '0.55', '0.60', '0.65', '0.70', '0.75', '0.80'],fontsize=12)\n",
    "ax31.legend(fontsize=13)\n",
    "\n",
    "ax32.plot(range(len(steps_2)), c3, label = 'Prompt', color='k', ls='-', lw=2)\n",
    "ax32.plot(range(len(steps_2)), c4, label = 'LLM', color='#5CA894', ls='--', lw=2)\n",
    "ax32.plot(range(len(steps_2)), c5, label = 'Prompt -> LLM', color='#cc0000',ls=':', lw=2)\n",
    "ax32.plot(range(len(steps_2)), c6, label = 'Prompt + LLM', color='#6d9eeb', ls='-.', lw=2)\n",
    "ax32.set_xticks(range(len(steps_2)))\n",
    "ax32.set_xticklabels(steps_2,fontsize=12)\n",
    "ax32.set_xlabel('Number of training epochs',fontsize=15)\n",
    "ax32.set_ylabel('Validation Set Accuracy',fontsize=15)\n",
    "ax32.text(0,0.795,'Prompt + LLM\\nTuning',ha='left',va='top',fontsize=20)\n",
    "ax32.set_ylim(0.48, 0.81)\n",
    "ax32.set_yticks([0.5, 0.55, 0.6, 0.65, 0.7, 0.75, 0.8])\n",
    "ax32.set_yticklabels(['0.50', '0.55', '0.60', '0.65', '0.70', '0.75', '0.80'],fontsize=12)\n",
    "ax32.legend(loc=4,fontsize=10)\n",
    "\n",
    "ax31.xaxis.set_ticks_position('both')\n",
    "ax31.tick_params(axis='x', which='major', direction='in')\n",
    "ax31.tick_params(axis='x', which='minor', direction='in')\n",
    "ax31.yaxis.set_ticks_position('both')\n",
    "ax31.tick_params(axis='y', which='major', direction='in')\n",
    "ax31.tick_params(axis='y', which='minor', direction='in')\n",
    "ax31.yaxis.set_minor_locator(MultipleLocator(0.01))\n",
    "\n",
    "ax32.xaxis.set_ticks_position('both')\n",
    "ax32.tick_params(axis='x', which='major', direction='in')\n",
    "ax32.tick_params(axis='x', which='minor', direction='in')\n",
    "ax32.yaxis.set_ticks_position('both')\n",
    "ax32.tick_params(axis='y', which='major', direction='in')\n",
    "ax32.tick_params(axis='y', which='minor', direction='in')\n",
    "ax32.yaxis.set_minor_locator(MultipleLocator(0.01))\n",
    "\n",
    "fig3.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fU80YRiGnJvV"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
