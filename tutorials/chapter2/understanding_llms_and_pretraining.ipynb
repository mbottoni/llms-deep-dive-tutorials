{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "# Chapter 2 Tutorial: Understanding LLMs and Pre-training\n",
        "\n",
        "In this tutorial, we will explore the mechanics of LLM architectures, with an emphasis on the differences between masked models and causal models. In the first section, we'll examine some existing pretrained models to understand how they produce their outputs. Once we've demonstrated how LLM's are able to do what they do, we will then run an abbreviated training loop to provide a glimpse into the training process."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7p66qIkzd7wz"
      },
      "source": [
        "## Installation and Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "zGW64FX-d7wz"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: datasets in /home/maruanottoni/miniconda3/envs/nn/lib/python3.8/site-packages (3.1.0)\n",
            "Requirement already satisfied: transformers[sentencepiece,torch] in /home/maruanottoni/miniconda3/envs/nn/lib/python3.8/site-packages (4.46.2)\n",
            "Requirement already satisfied: filelock in /home/maruanottoni/miniconda3/envs/nn/lib/python3.8/site-packages (from datasets) (3.16.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /home/maruanottoni/miniconda3/envs/nn/lib/python3.8/site-packages (from datasets) (1.24.4)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /home/maruanottoni/miniconda3/envs/nn/lib/python3.8/site-packages (from datasets) (16.1.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /home/maruanottoni/miniconda3/envs/nn/lib/python3.8/site-packages (from datasets) (0.3.1.1)\n",
            "Requirement already satisfied: pandas in /home/maruanottoni/miniconda3/envs/nn/lib/python3.8/site-packages (from datasets) (2.0.3)\n",
            "Requirement already satisfied: requests>=2.32.2 in /home/maruanottoni/miniconda3/envs/nn/lib/python3.8/site-packages (from datasets) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /home/maruanottoni/miniconda3/envs/nn/lib/python3.8/site-packages (from datasets) (4.66.5)\n",
            "Requirement already satisfied: xxhash in /home/maruanottoni/miniconda3/envs/nn/lib/python3.8/site-packages (from datasets) (3.5.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /home/maruanottoni/miniconda3/envs/nn/lib/python3.8/site-packages (from datasets) (0.70.16)\n",
            "Requirement already satisfied: fsspec<=2024.9.0,>=2023.1.0 in /home/maruanottoni/miniconda3/envs/nn/lib/python3.8/site-packages (from fsspec[http]<=2024.9.0,>=2023.1.0->datasets) (2024.9.0)\n",
            "Requirement already satisfied: aiohttp in /home/maruanottoni/miniconda3/envs/nn/lib/python3.8/site-packages (from datasets) (3.10.10)\n",
            "Requirement already satisfied: huggingface-hub>=0.23.0 in /home/maruanottoni/miniconda3/envs/nn/lib/python3.8/site-packages (from datasets) (0.26.2)\n",
            "Requirement already satisfied: packaging in /home/maruanottoni/miniconda3/envs/nn/lib/python3.8/site-packages (from datasets) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /home/maruanottoni/miniconda3/envs/nn/lib/python3.8/site-packages (from datasets) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /home/maruanottoni/miniconda3/envs/nn/lib/python3.8/site-packages (from transformers[sentencepiece,torch]) (2024.9.11)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /home/maruanottoni/miniconda3/envs/nn/lib/python3.8/site-packages (from transformers[sentencepiece,torch]) (0.4.5)\n",
            "Requirement already satisfied: tokenizers<0.21,>=0.20 in /home/maruanottoni/miniconda3/envs/nn/lib/python3.8/site-packages (from transformers[sentencepiece,torch]) (0.20.3)\n",
            "Requirement already satisfied: accelerate>=0.26.0 in /home/maruanottoni/miniconda3/envs/nn/lib/python3.8/site-packages (from transformers[sentencepiece,torch]) (1.0.1)\n",
            "Requirement already satisfied: torch in /home/maruanottoni/miniconda3/envs/nn/lib/python3.8/site-packages (from transformers[sentencepiece,torch]) (2.4.1)\n",
            "Requirement already satisfied: protobuf in /home/maruanottoni/miniconda3/envs/nn/lib/python3.8/site-packages (from transformers[sentencepiece,torch]) (4.25.5)\n",
            "Requirement already satisfied: sentencepiece!=0.1.92,>=0.1.91 in /home/maruanottoni/miniconda3/envs/nn/lib/python3.8/site-packages (from transformers[sentencepiece,torch]) (0.2.0)\n",
            "Requirement already satisfied: psutil in /home/maruanottoni/miniconda3/envs/nn/lib/python3.8/site-packages (from accelerate>=0.26.0->transformers[sentencepiece,torch]) (6.0.0)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /home/maruanottoni/miniconda3/envs/nn/lib/python3.8/site-packages (from aiohttp->datasets) (2.4.3)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /home/maruanottoni/miniconda3/envs/nn/lib/python3.8/site-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /home/maruanottoni/miniconda3/envs/nn/lib/python3.8/site-packages (from aiohttp->datasets) (24.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /home/maruanottoni/miniconda3/envs/nn/lib/python3.8/site-packages (from aiohttp->datasets) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /home/maruanottoni/miniconda3/envs/nn/lib/python3.8/site-packages (from aiohttp->datasets) (6.1.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.12.0 in /home/maruanottoni/miniconda3/envs/nn/lib/python3.8/site-packages (from aiohttp->datasets) (1.15.2)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /home/maruanottoni/miniconda3/envs/nn/lib/python3.8/site-packages (from aiohttp->datasets) (4.0.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/maruanottoni/miniconda3/envs/nn/lib/python3.8/site-packages (from huggingface-hub>=0.23.0->datasets) (4.12.2)\n",
            "Collecting dill<0.3.9,>=0.3.0 (from datasets)\n",
            "  Using cached dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /home/maruanottoni/miniconda3/envs/nn/lib/python3.8/site-packages (from requests>=2.32.2->datasets) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /home/maruanottoni/miniconda3/envs/nn/lib/python3.8/site-packages (from requests>=2.32.2->datasets) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/maruanottoni/miniconda3/envs/nn/lib/python3.8/site-packages (from requests>=2.32.2->datasets) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /home/maruanottoni/miniconda3/envs/nn/lib/python3.8/site-packages (from requests>=2.32.2->datasets) (2024.8.30)\n",
            "Requirement already satisfied: sympy in /home/maruanottoni/miniconda3/envs/nn/lib/python3.8/site-packages (from torch->transformers[sentencepiece,torch]) (1.13.3)\n",
            "Requirement already satisfied: networkx in /home/maruanottoni/miniconda3/envs/nn/lib/python3.8/site-packages (from torch->transformers[sentencepiece,torch]) (2.8.8)\n",
            "Requirement already satisfied: jinja2 in /home/maruanottoni/miniconda3/envs/nn/lib/python3.8/site-packages (from torch->transformers[sentencepiece,torch]) (3.0.3)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /home/maruanottoni/miniconda3/envs/nn/lib/python3.8/site-packages (from torch->transformers[sentencepiece,torch]) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /home/maruanottoni/miniconda3/envs/nn/lib/python3.8/site-packages (from torch->transformers[sentencepiece,torch]) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /home/maruanottoni/miniconda3/envs/nn/lib/python3.8/site-packages (from torch->transformers[sentencepiece,torch]) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /home/maruanottoni/miniconda3/envs/nn/lib/python3.8/site-packages (from torch->transformers[sentencepiece,torch]) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /home/maruanottoni/miniconda3/envs/nn/lib/python3.8/site-packages (from torch->transformers[sentencepiece,torch]) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /home/maruanottoni/miniconda3/envs/nn/lib/python3.8/site-packages (from torch->transformers[sentencepiece,torch]) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /home/maruanottoni/miniconda3/envs/nn/lib/python3.8/site-packages (from torch->transformers[sentencepiece,torch]) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /home/maruanottoni/miniconda3/envs/nn/lib/python3.8/site-packages (from torch->transformers[sentencepiece,torch]) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /home/maruanottoni/miniconda3/envs/nn/lib/python3.8/site-packages (from torch->transformers[sentencepiece,torch]) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.20.5 in /home/maruanottoni/miniconda3/envs/nn/lib/python3.8/site-packages (from torch->transformers[sentencepiece,torch]) (2.20.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /home/maruanottoni/miniconda3/envs/nn/lib/python3.8/site-packages (from torch->transformers[sentencepiece,torch]) (12.1.105)\n",
            "Requirement already satisfied: triton==3.0.0 in /home/maruanottoni/miniconda3/envs/nn/lib/python3.8/site-packages (from torch->transformers[sentencepiece,torch]) (3.0.0)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /home/maruanottoni/miniconda3/envs/nn/lib/python3.8/site-packages (from nvidia-cusolver-cu12==11.4.5.107->torch->transformers[sentencepiece,torch]) (12.6.77)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /home/maruanottoni/miniconda3/envs/nn/lib/python3.8/site-packages (from pandas->datasets) (2.9.0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /home/maruanottoni/miniconda3/envs/nn/lib/python3.8/site-packages (from pandas->datasets) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /home/maruanottoni/miniconda3/envs/nn/lib/python3.8/site-packages (from pandas->datasets) (2024.2)\n",
            "Requirement already satisfied: six>=1.5 in /home/maruanottoni/miniconda3/envs/nn/lib/python3.8/site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /home/maruanottoni/miniconda3/envs/nn/lib/python3.8/site-packages (from yarl<2.0,>=1.12.0->aiohttp->datasets) (0.2.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /home/maruanottoni/miniconda3/envs/nn/lib/python3.8/site-packages (from jinja2->torch->transformers[sentencepiece,torch]) (2.1.5)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /home/maruanottoni/miniconda3/envs/nn/lib/python3.8/site-packages (from sympy->torch->transformers[sentencepiece,torch]) (1.3.0)\n",
            "Using cached dill-0.3.8-py3-none-any.whl (116 kB)\n",
            "Installing collected packages: dill\n",
            "  Attempting uninstall: dill\n",
            "    Found existing installation: dill 0.3.1.1\n",
            "    Uninstalling dill-0.3.1.1:\n",
            "      Successfully uninstalled dill-0.3.1.1\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "apache-beam 2.60.0 requires dill<0.3.2,>=0.3.1.1, but you have dill 0.3.8 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed dill-0.3.8\n",
            "Requirement already satisfied: apache_beam in /home/maruanottoni/miniconda3/envs/nn/lib/python3.8/site-packages (2.60.0)\n",
            "Requirement already satisfied: crcmod<2.0,>=1.7 in /home/maruanottoni/miniconda3/envs/nn/lib/python3.8/site-packages (from apache_beam) (1.7)\n",
            "Requirement already satisfied: orjson<4,>=3.9.7 in /home/maruanottoni/miniconda3/envs/nn/lib/python3.8/site-packages (from apache_beam) (3.10.11)\n",
            "Collecting dill<0.3.2,>=0.3.1.1 (from apache_beam)\n",
            "  Using cached dill-0.3.1.1-py3-none-any.whl\n",
            "Requirement already satisfied: cloudpickle~=2.2.1 in /home/maruanottoni/miniconda3/envs/nn/lib/python3.8/site-packages (from apache_beam) (2.2.1)\n",
            "Requirement already satisfied: fastavro<2,>=0.23.6 in /home/maruanottoni/miniconda3/envs/nn/lib/python3.8/site-packages (from apache_beam) (1.9.7)\n",
            "Requirement already satisfied: fasteners<1.0,>=0.3 in /home/maruanottoni/miniconda3/envs/nn/lib/python3.8/site-packages (from apache_beam) (0.19)\n",
            "Requirement already satisfied: grpcio!=1.48.0,!=1.59.*,!=1.60.*,!=1.61.*,!=1.62.0,!=1.62.1,<1.66.0,<2,>=1.33.1 in /home/maruanottoni/miniconda3/envs/nn/lib/python3.8/site-packages (from apache_beam) (1.65.5)\n",
            "Requirement already satisfied: hdfs<3.0.0,>=2.1.0 in /home/maruanottoni/miniconda3/envs/nn/lib/python3.8/site-packages (from apache_beam) (2.7.3)\n",
            "Requirement already satisfied: httplib2<0.23.0,>=0.8 in /home/maruanottoni/miniconda3/envs/nn/lib/python3.8/site-packages (from apache_beam) (0.22.0)\n",
            "Requirement already satisfied: jsonschema<5.0.0,>=4.0.0 in /home/maruanottoni/miniconda3/envs/nn/lib/python3.8/site-packages (from apache_beam) (4.23.0)\n",
            "Requirement already satisfied: jsonpickle<4.0.0,>=3.0.0 in /home/maruanottoni/miniconda3/envs/nn/lib/python3.8/site-packages (from apache_beam) (3.4.2)\n",
            "Requirement already satisfied: numpy<1.27.0,>=1.14.3 in /home/maruanottoni/miniconda3/envs/nn/lib/python3.8/site-packages (from apache_beam) (1.24.4)\n",
            "Requirement already satisfied: objsize<0.8.0,>=0.6.1 in /home/maruanottoni/miniconda3/envs/nn/lib/python3.8/site-packages (from apache_beam) (0.7.0)\n",
            "Requirement already satisfied: packaging>=22.0 in /home/maruanottoni/miniconda3/envs/nn/lib/python3.8/site-packages (from apache_beam) (24.1)\n",
            "Requirement already satisfied: pymongo<5.0.0,>=3.8.0 in /home/maruanottoni/miniconda3/envs/nn/lib/python3.8/site-packages (from apache_beam) (4.10.1)\n",
            "Requirement already satisfied: proto-plus<2,>=1.7.1 in /home/maruanottoni/miniconda3/envs/nn/lib/python3.8/site-packages (from apache_beam) (1.25.0)\n",
            "Requirement already satisfied: protobuf!=4.0.*,!=4.21.*,!=4.22.0,!=4.23.*,!=4.24.*,<4.26.0,>=3.20.3 in /home/maruanottoni/miniconda3/envs/nn/lib/python3.8/site-packages (from apache_beam) (4.25.5)\n",
            "Requirement already satisfied: pydot<2,>=1.2.0 in /home/maruanottoni/miniconda3/envs/nn/lib/python3.8/site-packages (from apache_beam) (1.4.2)\n",
            "Requirement already satisfied: python-dateutil<3,>=2.8.0 in /home/maruanottoni/miniconda3/envs/nn/lib/python3.8/site-packages (from apache_beam) (2.9.0)\n",
            "Requirement already satisfied: pytz>=2018.3 in /home/maruanottoni/miniconda3/envs/nn/lib/python3.8/site-packages (from apache_beam) (2024.2)\n",
            "Requirement already satisfied: redis<6,>=5.0.0 in /home/maruanottoni/miniconda3/envs/nn/lib/python3.8/site-packages (from apache_beam) (5.2.0)\n",
            "Requirement already satisfied: regex>=2020.6.8 in /home/maruanottoni/miniconda3/envs/nn/lib/python3.8/site-packages (from apache_beam) (2024.9.11)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.24.0 in /home/maruanottoni/miniconda3/envs/nn/lib/python3.8/site-packages (from apache_beam) (2.32.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.0 in /home/maruanottoni/miniconda3/envs/nn/lib/python3.8/site-packages (from apache_beam) (4.12.2)\n",
            "Requirement already satisfied: zstandard<1,>=0.18.0 in /home/maruanottoni/miniconda3/envs/nn/lib/python3.8/site-packages (from apache_beam) (0.23.0)\n",
            "Requirement already satisfied: pyarrow<17.0.0,>=3.0.0 in /home/maruanottoni/miniconda3/envs/nn/lib/python3.8/site-packages (from apache_beam) (16.1.0)\n",
            "Requirement already satisfied: pyarrow-hotfix<1 in /home/maruanottoni/miniconda3/envs/nn/lib/python3.8/site-packages (from apache_beam) (0.6)\n",
            "Requirement already satisfied: docopt in /home/maruanottoni/miniconda3/envs/nn/lib/python3.8/site-packages (from hdfs<3.0.0,>=2.1.0->apache_beam) (0.6.2)\n",
            "Requirement already satisfied: six>=1.9.0 in /home/maruanottoni/miniconda3/envs/nn/lib/python3.8/site-packages (from hdfs<3.0.0,>=2.1.0->apache_beam) (1.16.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.0,!=3.0.1,!=3.0.2,!=3.0.3,<4,>=2.4.2 in /home/maruanottoni/miniconda3/envs/nn/lib/python3.8/site-packages (from httplib2<0.23.0,>=0.8->apache_beam) (3.1.4)\n",
            "Requirement already satisfied: attrs>=22.2.0 in /home/maruanottoni/miniconda3/envs/nn/lib/python3.8/site-packages (from jsonschema<5.0.0,>=4.0.0->apache_beam) (24.2.0)\n",
            "Requirement already satisfied: importlib-resources>=1.4.0 in /home/maruanottoni/miniconda3/envs/nn/lib/python3.8/site-packages (from jsonschema<5.0.0,>=4.0.0->apache_beam) (6.4.5)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /home/maruanottoni/miniconda3/envs/nn/lib/python3.8/site-packages (from jsonschema<5.0.0,>=4.0.0->apache_beam) (2023.12.1)\n",
            "Requirement already satisfied: pkgutil-resolve-name>=1.3.10 in /home/maruanottoni/miniconda3/envs/nn/lib/python3.8/site-packages (from jsonschema<5.0.0,>=4.0.0->apache_beam) (1.3.10)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /home/maruanottoni/miniconda3/envs/nn/lib/python3.8/site-packages (from jsonschema<5.0.0,>=4.0.0->apache_beam) (0.35.1)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /home/maruanottoni/miniconda3/envs/nn/lib/python3.8/site-packages (from jsonschema<5.0.0,>=4.0.0->apache_beam) (0.20.1)\n",
            "Requirement already satisfied: dnspython<3.0.0,>=1.16.0 in /home/maruanottoni/miniconda3/envs/nn/lib/python3.8/site-packages (from pymongo<5.0.0,>=3.8.0->apache_beam) (2.6.1)\n",
            "Requirement already satisfied: async-timeout>=4.0.3 in /home/maruanottoni/miniconda3/envs/nn/lib/python3.8/site-packages (from redis<6,>=5.0.0->apache_beam) (4.0.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /home/maruanottoni/miniconda3/envs/nn/lib/python3.8/site-packages (from requests<3.0.0,>=2.24.0->apache_beam) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /home/maruanottoni/miniconda3/envs/nn/lib/python3.8/site-packages (from requests<3.0.0,>=2.24.0->apache_beam) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/maruanottoni/miniconda3/envs/nn/lib/python3.8/site-packages (from requests<3.0.0,>=2.24.0->apache_beam) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /home/maruanottoni/miniconda3/envs/nn/lib/python3.8/site-packages (from requests<3.0.0,>=2.24.0->apache_beam) (2024.8.30)\n",
            "Requirement already satisfied: zipp>=3.1.0 in /home/maruanottoni/miniconda3/envs/nn/lib/python3.8/site-packages (from importlib-resources>=1.4.0->jsonschema<5.0.0,>=4.0.0->apache_beam) (3.20.2)\n",
            "Installing collected packages: dill\n",
            "  Attempting uninstall: dill\n",
            "    Found existing installation: dill 0.3.8\n",
            "    Uninstalling dill-0.3.8:\n",
            "      Successfully uninstalled dill-0.3.8\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "multiprocess 0.70.16 requires dill>=0.3.8, but you have dill 0.3.1.1 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed dill-0.3.1.1\n"
          ]
        }
      ],
      "source": [
        "!pip install datasets transformers[sentencepiece,torch]\n",
        "!pip install apache_beam"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "from datasets import load_dataset, DatasetDict\n",
        "\n",
        "from transformers import (\n",
        "    BertTokenizer,\n",
        "    BertForMaskedLM,\n",
        "    GPT2Tokenizer,\n",
        "    GPT2LMHeadModel,\n",
        "    DataCollatorForLanguageModeling,\n",
        "    AutoConfig,\n",
        "    AutoTokenizer,\n",
        "    Trainer,\n",
        "    TrainingArguments\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bGc8Si8CBbuI"
      },
      "source": [
        "## Understanding Masked LM's"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "99o28B2EGVh3"
      },
      "outputs": [],
      "source": [
        "## The first model we will look at is BERT, which is trained with masked tokens. As an example,\n",
        "## the text below masks the word \"box\" from a well-known movie quote.\n",
        "\n",
        "text = \"Life is like a [MASK] of chocolates.\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "ZVii0JgSBKpU"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "BertForMaskedLM has generative capabilities, as `prepare_inputs_for_generation` is explicitly overwritten. However, it doesn't directly inherit from `GenerationMixin`. From 👉v4.50👈 onwards, `PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.\n",
            "  - If you're using `trust_remote_code=True`, you can get rid of this warning by loading the model with an auto class. See https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes\n",
            "  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).\n",
            "  - If you are not the owner of the model architecture class, please contact the model code owner to update it.\n",
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
            "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        }
      ],
      "source": [
        "## We'll now see how BERT is able to predict the missing word. We can use HuggingFace to load\n",
        "## a copy of the pretrained model and tokenizer.\n",
        "\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "model = BertForMaskedLM.from_pretrained(\"bert-base-uncased\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "TWKpRFEFA2Oz"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "input_ids: tensor([[ 101, 2166, 2003, 2066, 1037,  103, 1997, 7967, 2015, 1012,  102]])\n",
            "attention_mask: tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])\n"
          ]
        }
      ],
      "source": [
        "## Next, we'll feed our example text into the tokenizer.\n",
        "\n",
        "encoded_input = tokenizer(text, return_tensors='pt')\n",
        "print('input_ids:', encoded_input['input_ids'])\n",
        "print('attention_mask:', encoded_input['attention_mask'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "o84muqUuLiwC"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "chocolate\n"
          ]
        }
      ],
      "source": [
        "## input_ids represents the tokenized output. Each integer can be mapped back to the corresponding string.\n",
        "\n",
        "print(tokenizer.decode([7967]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "1ut5R3IYbxxd"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "BertForMaskedLM(\n",
              "  (bert): BertModel(\n",
              "    (embeddings): BertEmbeddings(\n",
              "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
              "      (position_embeddings): Embedding(512, 768)\n",
              "      (token_type_embeddings): Embedding(2, 768)\n",
              "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (encoder): BertEncoder(\n",
              "      (layer): ModuleList(\n",
              "        (0-11): 12 x BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSdpaSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "  )\n",
              "  (cls): BertOnlyMLMHead(\n",
              "    (predictions): BertLMPredictionHead(\n",
              "      (transform): BertPredictionHeadTransform(\n",
              "        (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (transform_act_fn): GELUActivation()\n",
              "        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "      )\n",
              "      (decoder): Linear(in_features=768, out_features=30522, bias=True)\n",
              "    )\n",
              "  )\n",
              ")"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "## The model will then receive the output of the tokenizer. We can look at the BERT model to see exactly how\n",
        "## it was constructed and what the outputs will be like.\n",
        "\n",
        "model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "H-Xv2ujRAch4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([1, 11, 30522])\n"
          ]
        }
      ],
      "source": [
        "## The model starts with an embedding of each of the 30,522 possible tokens into 768 dimensions, which at this\n",
        "## point is simply a representation of each token without any additional information about their relationships\n",
        "## to one another in the text. Then the encoder attention blocks are applied, updating the embeddings such that\n",
        "## they now encode each token's contribution to the chunk of text and interactions with other tokens. Notably,\n",
        "## this includes the masked tokens as well. The final stage is the language model head, which takes the embeddings\n",
        "## from the masked positions back to 30,522 dimensions. Each index of this final vector corresponds to the\n",
        "## probability that the token in that position would be the correct choice to fill the mask.\n",
        "\n",
        "\n",
        "model_output = model(**encoded_input)\n",
        "output = model_output[\"logits\"]\n",
        "\n",
        "print(output.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "4hDXB-x5tzrh"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([30522])\n"
          ]
        }
      ],
      "source": [
        "tokens = encoded_input['input_ids'][0].tolist()\n",
        "masked_index = tokens.index(tokenizer.mask_token_id)\n",
        "logits = output[0, masked_index, :]\n",
        "\n",
        "print(logits.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "BaRYd_aVjeoR"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Top 5 predictions: box bag bowl jar cup\n",
            "tensor([0.1764, 0.1688, 0.0419, 0.0336, 0.0262], grad_fn=<TopkBackward0>)\n"
          ]
        }
      ],
      "source": [
        "probs = logits.softmax(dim=-1)\n",
        "values, predictions = probs.topk(5)\n",
        "sequence = tokenizer.decode(predictions)\n",
        "\n",
        "print('Top 5 predictions:', sequence)\n",
        "print(values)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1vVQ6Cqhuq9t"
      },
      "source": [
        "Printing the top 5 predictions and their respective scores, we see that BERT accurately chooses \"box\" as the most likely replacement for the mask token.\n",
        "\n",
        "## Understanding Causal LM's"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "azFAE8TVkPXv"
      },
      "outputs": [],
      "source": [
        "## We now repeat a similar exercise with the causal LLM GPT-2. This model generates\n",
        "## text following an input, instead of replacing a mask within the text.\n",
        "\n",
        "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
        "model = GPT2LMHeadModel.from_pretrained('gpt2')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "4TwmbE0bk1mC"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "GPT2LMHeadModel(\n",
              "  (transformer): GPT2Model(\n",
              "    (wte): Embedding(50257, 768)\n",
              "    (wpe): Embedding(1024, 768)\n",
              "    (drop): Dropout(p=0.1, inplace=False)\n",
              "    (h): ModuleList(\n",
              "      (0-11): 12 x GPT2Block(\n",
              "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        (attn): GPT2SdpaAttention(\n",
              "          (c_attn): Conv1D(nf=2304, nx=768)\n",
              "          (c_proj): Conv1D(nf=768, nx=768)\n",
              "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
              "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        (mlp): GPT2MLP(\n",
              "          (c_fc): Conv1D(nf=3072, nx=768)\n",
              "          (c_proj): Conv1D(nf=768, nx=3072)\n",
              "          (act): NewGELUActivation()\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "  )\n",
              "  (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
              ")"
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "## We can examine the model again, noting the similarities to BERT. An embedding, 12 attention blocks,\n",
        "## and a linear transformation bringing the output back to the size of the tokenizer. The tokenizer is\n",
        "## different from BERT so we see we have more tokens this time.\n",
        "\n",
        "model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "hwUCWU6TwTqZ"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'input_ids': tensor([[10462, 27428,   379,   262, 10481,   318]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1]])}"
            ]
          },
          "execution_count": 15,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "## We'll use a different text example, since this model works by producing tokens sequentially\n",
        "## rather than filling a mask.\n",
        "\n",
        "text = \"Swimming at the beach is\"\n",
        "model_inputs = tokenizer(text, return_tensors='pt')\n",
        "model_inputs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "Ua8YctwekqSm"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([257])\n"
          ]
        }
      ],
      "source": [
        "## After applying the model, the information needed to predict the next token is represented by\n",
        "## the last token. So we can access that vector by the index -1.\n",
        "\n",
        "output = model(**model_inputs)\n",
        "next_token_logits = output.logits[:, -1, :]\n",
        "next_token = torch.argmax(next_token_logits, dim=-1)\n",
        "print(next_token)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "OH05O28PnLW9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'input_ids': tensor([[10462, 27428,   379,   262, 10481,   318,   257]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1]])}\n"
          ]
        }
      ],
      "source": [
        "## Now add the new token to the end of the text, and feed all of it back to the model to continue\n",
        "## predicting more tokens.\n",
        "\n",
        "model_inputs['input_ids'] = torch.cat([model_inputs['input_ids'], next_token[:, None]], dim=-1)\n",
        "model_inputs[\"attention_mask\"] = torch.cat([model_inputs['attention_mask'], torch.tensor([[1]])], dim=-1)\n",
        "print(model_inputs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "-ja9Mk0DnpBe"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Swimming at the beach is a\n"
          ]
        }
      ],
      "source": [
        "## Here's what we have so far. The model added the word 'a' to the input text.\n",
        "\n",
        "print(tokenizer.decode(model_inputs['input_ids'][0]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "n-ygamSIoMKh"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Swimming at the beach is a great\n"
          ]
        }
      ],
      "source": [
        "## Repeating all the previous steps, we then add the word 'great'.\n",
        "\n",
        "output = model(**model_inputs)\n",
        "next_token_logits = output.logits[:, -1, :]\n",
        "next_token = torch.argmax(next_token_logits, dim=-1)\n",
        "model_inputs['input_ids'] = torch.cat([model_inputs['input_ids'], next_token[:, None]], dim=-1)\n",
        "model_inputs[\"attention_mask\"] = torch.cat([model_inputs['attention_mask'], torch.tensor([[1]])], dim=-1)\n",
        "print(tokenizer.decode(model_inputs['input_ids'][0]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "z9QIw--gIrh8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Swimming at the beach is a great way to get a little extra energy.\n",
            "\n",
            "The beach\n"
          ]
        }
      ],
      "source": [
        "## HuggingFace automates this iterative process. We'll use the quicker approach to finish our sentence.\n",
        "\n",
        "output_generate = model.generate(**model_inputs, max_length=20, pad_token_id=tokenizer.eos_token_id)\n",
        "print(tokenizer.decode(output_generate[0]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XL2rmSMVpkEu"
      },
      "source": [
        "## Pre-training a GPT-2 model from scratch\n",
        "\n",
        "Next we'll train a GPT-2 model from scratch using English Wikipedia data. Note that we're only using a tiny subset of the data to demonstrate that the model is capable of learning. The exact same approach could be followed on the full dataset to train a more functional model, but that would require a lot of compute."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "QTW0UmOJd7w2"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Downloading data:   2%|▏         | 1/41 [00:47<31:47, 47.69s/files]"
          ]
        }
      ],
      "source": [
        "dataset = load_dataset(\"wikipedia\", \"20220301.en\")\n",
        "ds_shuffle = dataset['train'].shuffle()\n",
        "\n",
        "raw_datasets = DatasetDict(\n",
        "    {\n",
        "        \"train\": ds_shuffle.select(range(50)),\n",
        "        \"valid\": ds_shuffle.select(range(50, 100))\n",
        "    }\n",
        ")\n",
        "\n",
        "raw_datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-qbgOvH3d7w2"
      },
      "outputs": [],
      "source": [
        "print(raw_datasets['train'][0]['text'][:200])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-HRmdiqbd7w3"
      },
      "outputs": [],
      "source": [
        "## We'll tokenize the text, setting the context size to 128 and thus breaking each document into chunks of 128 tokens.\n",
        "\n",
        "context_length = 128\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
        "\n",
        "outputs = tokenizer(\n",
        "    raw_datasets[\"train\"][:2][\"text\"],\n",
        "    truncation=True,\n",
        "    max_length=context_length,\n",
        "    return_overflowing_tokens=True,\n",
        "    return_length=True,\n",
        ")\n",
        "\n",
        "print(f\"Input IDs length: {len(outputs['input_ids'])}\")\n",
        "print(f\"Input chunk lengths: {(outputs['length'])}\")\n",
        "print(f\"Chunk mapping: {outputs['overflow_to_sample_mapping']}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iWvoQ3-td7w3"
      },
      "outputs": [],
      "source": [
        "def tokenize(element):\n",
        "    outputs = tokenizer(\n",
        "        element[\"text\"],\n",
        "        truncation=True,\n",
        "        max_length=context_length,\n",
        "        return_overflowing_tokens=True,\n",
        "        return_length=True,\n",
        "    )\n",
        "    input_batch = []\n",
        "    for length, input_ids in zip(outputs[\"length\"], outputs[\"input_ids\"]):\n",
        "        if length == context_length:\n",
        "            input_batch.append(input_ids)\n",
        "    return {\"input_ids\": input_batch}\n",
        "\n",
        "\n",
        "tokenized_datasets = raw_datasets.map(\n",
        "    tokenize, batched=True, remove_columns=raw_datasets[\"train\"].column_names\n",
        ")\n",
        "tokenized_datasets"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zfu1oJeQYiLa"
      },
      "source": [
        "Now we can set up the HuggingFace Trainer as follows. Since we're using such a small dataset, we'll need lots of epochs for the model to make progress because all of the parameters are randomly initialized at the outset. Typically, most LLM's are trained for only one epoch and more diverse examples."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7jf0Le4Yd7w3"
      },
      "outputs": [],
      "source": [
        "config = AutoConfig.from_pretrained(\n",
        "    \"gpt2\",\n",
        "    vocab_size=len(tokenizer),\n",
        "    n_ctx=context_length,\n",
        "    bos_token_id=tokenizer.bos_token_id,\n",
        "    eos_token_id=tokenizer.eos_token_id,\n",
        ")\n",
        "\n",
        "model = GPT2LMHeadModel(config)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O_OKDaPMd7w3"
      },
      "outputs": [],
      "source": [
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "data_collator = DataCollatorForLanguageModeling(tokenizer, mlm=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ex3MHkued7w4"
      },
      "outputs": [],
      "source": [
        "args = TrainingArguments(\n",
        "    output_dir=\"wiki-gpt2\",\n",
        "    evaluation_strategy=\"steps\",\n",
        "    num_train_epochs=100\n",
        ")\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    args=args,\n",
        "    data_collator=data_collator,\n",
        "    train_dataset=tokenized_datasets[\"train\"],\n",
        "    eval_dataset=tokenized_datasets[\"valid\"]\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WIj0n1fbd7w4"
      },
      "outputs": [],
      "source": [
        "trainer.train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S2avxzYR5U8Y"
      },
      "outputs": [],
      "source": [
        "trainer.evaluate()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UIkCLEFRc3h6"
      },
      "source": [
        "The training loss is low by the end, which means the model should perform very well on training examples it has seen. It does not generalize well to the validation set of course, since we deliberately overfit on a small train set.\n",
        "\n",
        "We can confirm with a couple of examples that were seen in training."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x-cxmRLLsGgB"
      },
      "outputs": [],
      "source": [
        "text = tokenizer.decode(tokenized_datasets[\"train\"][0]['input_ids'][:16])\n",
        "print(text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bbk_WBhMrj_F"
      },
      "outputs": [],
      "source": [
        "model_inputs = tokenizer(text, return_tensors='pt')\n",
        "print(model_inputs['input_ids'].shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ipgkkqn4sEAp"
      },
      "outputs": [],
      "source": [
        "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
        "model_inputs['input_ids'] = model_inputs['input_ids'].to(device)\n",
        "model_inputs['attention_mask'] = model_inputs['attention_mask'].to(device)\n",
        "\n",
        "output_generate = model.generate(**model_inputs, max_new_tokens=16)\n",
        "output_generate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JHUbsd-hxdTL"
      },
      "outputs": [],
      "source": [
        "sequence = tokenizer.decode(output_generate[0])\n",
        "print(sequence)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l04YrQLeghA5"
      },
      "source": [
        "The model should do quite well at reciting text after seeing it so many times. We can be convinced that the tokenizer, model architecture, and training objective are well-suited to learning Wikipedia data. For comparison, we'll try this model on text from the validation set."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MFK5FC57hwS9"
      },
      "outputs": [],
      "source": [
        "text = tokenizer.decode(tokenized_datasets[\"valid\"][0]['input_ids'][:32])\n",
        "print(text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-SlX_sfHfdTB"
      },
      "outputs": [],
      "source": [
        "model_inputs = tokenizer(text, return_tensors='pt')\n",
        "\n",
        "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
        "model_inputs['input_ids'] = model_inputs['input_ids'].to(device)\n",
        "model_inputs['attention_mask'] = model_inputs['attention_mask'].to(device)\n",
        "\n",
        "output_generate = model.generate(**model_inputs, max_new_tokens=16)\n",
        "sequence = tokenizer.decode(output_generate[0])\n",
        "print(sequence)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FassINSdfs9U"
      },
      "outputs": [],
      "source": [
        "raw_datasets['valid'][0]['text']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4ivQP0XciLl1"
      },
      "source": [
        "As expected, our model is completely confused this time. We'd need to train for much longer, and on much more diverse data, before we would have a model that can sensibly complete prompts it has never seen before. This is precisely why pre-training is such an important and powerful technique. If we had to train on all of Wikipedia for every NLP application to achieve optimal performance, it would be prohibitively expensive. But there's no need to do that when we can share and reuse existing pre-trained models as we did in the first part of this tutorial."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "V100",
      "include_colab_link": true,
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.20"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
