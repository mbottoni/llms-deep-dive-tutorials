{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "# Chapter 2 Tutorial: Understanding LLMs and Pre-training\n",
        "\n",
        "In this tutorial, we will explore the mechanics of LLM architectures, with an emphasis on the differences between masked models and causal models. In the first section, we'll examine some existing pretrained models to understand how they produce their outputs. Once we've demonstrated how LLM's are able to do what they do, we will then run an abbreviated training loop to provide a glimpse into the training process."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7p66qIkzd7wz"
      },
      "source": [
        "## Installation and Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "zGW64FX-d7wz"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: datasets in /home/maruanottoni/miniconda3/envs/nn/lib/python3.8/site-packages (3.1.0)\n",
            "Requirement already satisfied: transformers[sentencepiece,torch] in /home/maruanottoni/miniconda3/envs/nn/lib/python3.8/site-packages (4.46.2)\n",
            "Requirement already satisfied: filelock in /home/maruanottoni/miniconda3/envs/nn/lib/python3.8/site-packages (from datasets) (3.16.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /home/maruanottoni/miniconda3/envs/nn/lib/python3.8/site-packages (from datasets) (1.24.4)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /home/maruanottoni/miniconda3/envs/nn/lib/python3.8/site-packages (from datasets) (16.1.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /home/maruanottoni/miniconda3/envs/nn/lib/python3.8/site-packages (from datasets) (0.3.1.1)\n",
            "Requirement already satisfied: pandas in /home/maruanottoni/miniconda3/envs/nn/lib/python3.8/site-packages (from datasets) (2.0.3)\n",
            "Requirement already satisfied: requests>=2.32.2 in /home/maruanottoni/miniconda3/envs/nn/lib/python3.8/site-packages (from datasets) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /home/maruanottoni/miniconda3/envs/nn/lib/python3.8/site-packages (from datasets) (4.66.5)\n",
            "Requirement already satisfied: xxhash in /home/maruanottoni/miniconda3/envs/nn/lib/python3.8/site-packages (from datasets) (3.5.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /home/maruanottoni/miniconda3/envs/nn/lib/python3.8/site-packages (from datasets) (0.70.16)\n",
            "Requirement already satisfied: fsspec<=2024.9.0,>=2023.1.0 in /home/maruanottoni/miniconda3/envs/nn/lib/python3.8/site-packages (from fsspec[http]<=2024.9.0,>=2023.1.0->datasets) (2024.9.0)\n",
            "Requirement already satisfied: aiohttp in /home/maruanottoni/miniconda3/envs/nn/lib/python3.8/site-packages (from datasets) (3.10.10)\n",
            "Requirement already satisfied: huggingface-hub>=0.23.0 in /home/maruanottoni/miniconda3/envs/nn/lib/python3.8/site-packages (from datasets) (0.26.2)\n",
            "Requirement already satisfied: packaging in /home/maruanottoni/miniconda3/envs/nn/lib/python3.8/site-packages (from datasets) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /home/maruanottoni/miniconda3/envs/nn/lib/python3.8/site-packages (from datasets) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /home/maruanottoni/miniconda3/envs/nn/lib/python3.8/site-packages (from transformers[sentencepiece,torch]) (2024.9.11)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /home/maruanottoni/miniconda3/envs/nn/lib/python3.8/site-packages (from transformers[sentencepiece,torch]) (0.4.5)\n",
            "Requirement already satisfied: tokenizers<0.21,>=0.20 in /home/maruanottoni/miniconda3/envs/nn/lib/python3.8/site-packages (from transformers[sentencepiece,torch]) (0.20.3)\n",
            "Requirement already satisfied: protobuf in /home/maruanottoni/miniconda3/envs/nn/lib/python3.8/site-packages (from transformers[sentencepiece,torch]) (4.25.5)\n",
            "Requirement already satisfied: sentencepiece!=0.1.92,>=0.1.91 in /home/maruanottoni/miniconda3/envs/nn/lib/python3.8/site-packages (from transformers[sentencepiece,torch]) (0.2.0)\n",
            "Requirement already satisfied: accelerate>=0.26.0 in /home/maruanottoni/miniconda3/envs/nn/lib/python3.8/site-packages (from transformers[sentencepiece,torch]) (1.0.1)\n",
            "Requirement already satisfied: torch in /home/maruanottoni/miniconda3/envs/nn/lib/python3.8/site-packages (from transformers[sentencepiece,torch]) (2.4.1)\n",
            "Requirement already satisfied: psutil in /home/maruanottoni/miniconda3/envs/nn/lib/python3.8/site-packages (from accelerate>=0.26.0->transformers[sentencepiece,torch]) (6.0.0)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /home/maruanottoni/miniconda3/envs/nn/lib/python3.8/site-packages (from aiohttp->datasets) (2.4.3)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /home/maruanottoni/miniconda3/envs/nn/lib/python3.8/site-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /home/maruanottoni/miniconda3/envs/nn/lib/python3.8/site-packages (from aiohttp->datasets) (24.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /home/maruanottoni/miniconda3/envs/nn/lib/python3.8/site-packages (from aiohttp->datasets) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /home/maruanottoni/miniconda3/envs/nn/lib/python3.8/site-packages (from aiohttp->datasets) (6.1.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.12.0 in /home/maruanottoni/miniconda3/envs/nn/lib/python3.8/site-packages (from aiohttp->datasets) (1.15.2)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /home/maruanottoni/miniconda3/envs/nn/lib/python3.8/site-packages (from aiohttp->datasets) (4.0.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/maruanottoni/miniconda3/envs/nn/lib/python3.8/site-packages (from huggingface-hub>=0.23.0->datasets) (4.12.2)\n",
            "Collecting dill<0.3.9,>=0.3.0 (from datasets)\n",
            "  Using cached dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /home/maruanottoni/miniconda3/envs/nn/lib/python3.8/site-packages (from requests>=2.32.2->datasets) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /home/maruanottoni/miniconda3/envs/nn/lib/python3.8/site-packages (from requests>=2.32.2->datasets) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/maruanottoni/miniconda3/envs/nn/lib/python3.8/site-packages (from requests>=2.32.2->datasets) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /home/maruanottoni/miniconda3/envs/nn/lib/python3.8/site-packages (from requests>=2.32.2->datasets) (2024.8.30)\n",
            "Requirement already satisfied: sympy in /home/maruanottoni/miniconda3/envs/nn/lib/python3.8/site-packages (from torch->transformers[sentencepiece,torch]) (1.13.3)\n",
            "Requirement already satisfied: networkx in /home/maruanottoni/miniconda3/envs/nn/lib/python3.8/site-packages (from torch->transformers[sentencepiece,torch]) (2.8.8)\n",
            "Requirement already satisfied: jinja2 in /home/maruanottoni/miniconda3/envs/nn/lib/python3.8/site-packages (from torch->transformers[sentencepiece,torch]) (3.0.3)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /home/maruanottoni/miniconda3/envs/nn/lib/python3.8/site-packages (from torch->transformers[sentencepiece,torch]) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /home/maruanottoni/miniconda3/envs/nn/lib/python3.8/site-packages (from torch->transformers[sentencepiece,torch]) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /home/maruanottoni/miniconda3/envs/nn/lib/python3.8/site-packages (from torch->transformers[sentencepiece,torch]) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /home/maruanottoni/miniconda3/envs/nn/lib/python3.8/site-packages (from torch->transformers[sentencepiece,torch]) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /home/maruanottoni/miniconda3/envs/nn/lib/python3.8/site-packages (from torch->transformers[sentencepiece,torch]) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /home/maruanottoni/miniconda3/envs/nn/lib/python3.8/site-packages (from torch->transformers[sentencepiece,torch]) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /home/maruanottoni/miniconda3/envs/nn/lib/python3.8/site-packages (from torch->transformers[sentencepiece,torch]) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /home/maruanottoni/miniconda3/envs/nn/lib/python3.8/site-packages (from torch->transformers[sentencepiece,torch]) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /home/maruanottoni/miniconda3/envs/nn/lib/python3.8/site-packages (from torch->transformers[sentencepiece,torch]) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.20.5 in /home/maruanottoni/miniconda3/envs/nn/lib/python3.8/site-packages (from torch->transformers[sentencepiece,torch]) (2.20.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /home/maruanottoni/miniconda3/envs/nn/lib/python3.8/site-packages (from torch->transformers[sentencepiece,torch]) (12.1.105)\n",
            "Requirement already satisfied: triton==3.0.0 in /home/maruanottoni/miniconda3/envs/nn/lib/python3.8/site-packages (from torch->transformers[sentencepiece,torch]) (3.0.0)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /home/maruanottoni/miniconda3/envs/nn/lib/python3.8/site-packages (from nvidia-cusolver-cu12==11.4.5.107->torch->transformers[sentencepiece,torch]) (12.6.77)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /home/maruanottoni/miniconda3/envs/nn/lib/python3.8/site-packages (from pandas->datasets) (2.9.0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /home/maruanottoni/miniconda3/envs/nn/lib/python3.8/site-packages (from pandas->datasets) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /home/maruanottoni/miniconda3/envs/nn/lib/python3.8/site-packages (from pandas->datasets) (2024.2)\n",
            "Requirement already satisfied: six>=1.5 in /home/maruanottoni/miniconda3/envs/nn/lib/python3.8/site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /home/maruanottoni/miniconda3/envs/nn/lib/python3.8/site-packages (from yarl<2.0,>=1.12.0->aiohttp->datasets) (0.2.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /home/maruanottoni/miniconda3/envs/nn/lib/python3.8/site-packages (from jinja2->torch->transformers[sentencepiece,torch]) (2.1.5)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /home/maruanottoni/miniconda3/envs/nn/lib/python3.8/site-packages (from sympy->torch->transformers[sentencepiece,torch]) (1.3.0)\n",
            "Using cached dill-0.3.8-py3-none-any.whl (116 kB)\n",
            "Installing collected packages: dill\n",
            "  Attempting uninstall: dill\n",
            "    Found existing installation: dill 0.3.1.1\n",
            "    Uninstalling dill-0.3.1.1:\n",
            "      Successfully uninstalled dill-0.3.1.1\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "apache-beam 2.60.0 requires dill<0.3.2,>=0.3.1.1, but you have dill 0.3.8 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed dill-0.3.8\n",
            "Requirement already satisfied: apache_beam in /home/maruanottoni/miniconda3/envs/nn/lib/python3.8/site-packages (2.60.0)\n",
            "Requirement already satisfied: crcmod<2.0,>=1.7 in /home/maruanottoni/miniconda3/envs/nn/lib/python3.8/site-packages (from apache_beam) (1.7)\n",
            "Requirement already satisfied: orjson<4,>=3.9.7 in /home/maruanottoni/miniconda3/envs/nn/lib/python3.8/site-packages (from apache_beam) (3.10.11)\n",
            "Collecting dill<0.3.2,>=0.3.1.1 (from apache_beam)\n",
            "  Using cached dill-0.3.1.1-py3-none-any.whl\n",
            "Requirement already satisfied: cloudpickle~=2.2.1 in /home/maruanottoni/miniconda3/envs/nn/lib/python3.8/site-packages (from apache_beam) (2.2.1)\n",
            "Requirement already satisfied: fastavro<2,>=0.23.6 in /home/maruanottoni/miniconda3/envs/nn/lib/python3.8/site-packages (from apache_beam) (1.9.7)\n",
            "Requirement already satisfied: fasteners<1.0,>=0.3 in /home/maruanottoni/miniconda3/envs/nn/lib/python3.8/site-packages (from apache_beam) (0.19)\n",
            "Requirement already satisfied: grpcio!=1.48.0,!=1.59.*,!=1.60.*,!=1.61.*,!=1.62.0,!=1.62.1,<1.66.0,<2,>=1.33.1 in /home/maruanottoni/miniconda3/envs/nn/lib/python3.8/site-packages (from apache_beam) (1.65.5)\n",
            "Requirement already satisfied: hdfs<3.0.0,>=2.1.0 in /home/maruanottoni/miniconda3/envs/nn/lib/python3.8/site-packages (from apache_beam) (2.7.3)\n",
            "Requirement already satisfied: httplib2<0.23.0,>=0.8 in /home/maruanottoni/miniconda3/envs/nn/lib/python3.8/site-packages (from apache_beam) (0.22.0)\n",
            "Requirement already satisfied: jsonschema<5.0.0,>=4.0.0 in /home/maruanottoni/miniconda3/envs/nn/lib/python3.8/site-packages (from apache_beam) (4.23.0)\n",
            "Requirement already satisfied: jsonpickle<4.0.0,>=3.0.0 in /home/maruanottoni/miniconda3/envs/nn/lib/python3.8/site-packages (from apache_beam) (3.4.2)\n",
            "Requirement already satisfied: numpy<1.27.0,>=1.14.3 in /home/maruanottoni/miniconda3/envs/nn/lib/python3.8/site-packages (from apache_beam) (1.24.4)\n",
            "Requirement already satisfied: objsize<0.8.0,>=0.6.1 in /home/maruanottoni/miniconda3/envs/nn/lib/python3.8/site-packages (from apache_beam) (0.7.0)\n",
            "Requirement already satisfied: packaging>=22.0 in /home/maruanottoni/miniconda3/envs/nn/lib/python3.8/site-packages (from apache_beam) (24.1)\n",
            "Requirement already satisfied: pymongo<5.0.0,>=3.8.0 in /home/maruanottoni/miniconda3/envs/nn/lib/python3.8/site-packages (from apache_beam) (4.10.1)\n",
            "Requirement already satisfied: proto-plus<2,>=1.7.1 in /home/maruanottoni/miniconda3/envs/nn/lib/python3.8/site-packages (from apache_beam) (1.25.0)\n",
            "Requirement already satisfied: protobuf!=4.0.*,!=4.21.*,!=4.22.0,!=4.23.*,!=4.24.*,<4.26.0,>=3.20.3 in /home/maruanottoni/miniconda3/envs/nn/lib/python3.8/site-packages (from apache_beam) (4.25.5)\n",
            "Requirement already satisfied: pydot<2,>=1.2.0 in /home/maruanottoni/miniconda3/envs/nn/lib/python3.8/site-packages (from apache_beam) (1.4.2)\n",
            "Requirement already satisfied: python-dateutil<3,>=2.8.0 in /home/maruanottoni/miniconda3/envs/nn/lib/python3.8/site-packages (from apache_beam) (2.9.0)\n",
            "Requirement already satisfied: pytz>=2018.3 in /home/maruanottoni/miniconda3/envs/nn/lib/python3.8/site-packages (from apache_beam) (2024.2)\n",
            "Requirement already satisfied: redis<6,>=5.0.0 in /home/maruanottoni/miniconda3/envs/nn/lib/python3.8/site-packages (from apache_beam) (5.2.0)\n",
            "Requirement already satisfied: regex>=2020.6.8 in /home/maruanottoni/miniconda3/envs/nn/lib/python3.8/site-packages (from apache_beam) (2024.9.11)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.24.0 in /home/maruanottoni/miniconda3/envs/nn/lib/python3.8/site-packages (from apache_beam) (2.32.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.0 in /home/maruanottoni/miniconda3/envs/nn/lib/python3.8/site-packages (from apache_beam) (4.12.2)\n",
            "Requirement already satisfied: zstandard<1,>=0.18.0 in /home/maruanottoni/miniconda3/envs/nn/lib/python3.8/site-packages (from apache_beam) (0.23.0)\n",
            "Requirement already satisfied: pyarrow<17.0.0,>=3.0.0 in /home/maruanottoni/miniconda3/envs/nn/lib/python3.8/site-packages (from apache_beam) (16.1.0)\n",
            "Requirement already satisfied: pyarrow-hotfix<1 in /home/maruanottoni/miniconda3/envs/nn/lib/python3.8/site-packages (from apache_beam) (0.6)\n",
            "Requirement already satisfied: docopt in /home/maruanottoni/miniconda3/envs/nn/lib/python3.8/site-packages (from hdfs<3.0.0,>=2.1.0->apache_beam) (0.6.2)\n",
            "Requirement already satisfied: six>=1.9.0 in /home/maruanottoni/miniconda3/envs/nn/lib/python3.8/site-packages (from hdfs<3.0.0,>=2.1.0->apache_beam) (1.16.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.0,!=3.0.1,!=3.0.2,!=3.0.3,<4,>=2.4.2 in /home/maruanottoni/miniconda3/envs/nn/lib/python3.8/site-packages (from httplib2<0.23.0,>=0.8->apache_beam) (3.1.4)\n",
            "Requirement already satisfied: attrs>=22.2.0 in /home/maruanottoni/miniconda3/envs/nn/lib/python3.8/site-packages (from jsonschema<5.0.0,>=4.0.0->apache_beam) (24.2.0)\n",
            "Requirement already satisfied: importlib-resources>=1.4.0 in /home/maruanottoni/miniconda3/envs/nn/lib/python3.8/site-packages (from jsonschema<5.0.0,>=4.0.0->apache_beam) (6.4.5)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /home/maruanottoni/miniconda3/envs/nn/lib/python3.8/site-packages (from jsonschema<5.0.0,>=4.0.0->apache_beam) (2023.12.1)\n",
            "Requirement already satisfied: pkgutil-resolve-name>=1.3.10 in /home/maruanottoni/miniconda3/envs/nn/lib/python3.8/site-packages (from jsonschema<5.0.0,>=4.0.0->apache_beam) (1.3.10)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /home/maruanottoni/miniconda3/envs/nn/lib/python3.8/site-packages (from jsonschema<5.0.0,>=4.0.0->apache_beam) (0.35.1)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /home/maruanottoni/miniconda3/envs/nn/lib/python3.8/site-packages (from jsonschema<5.0.0,>=4.0.0->apache_beam) (0.20.1)\n",
            "Requirement already satisfied: dnspython<3.0.0,>=1.16.0 in /home/maruanottoni/miniconda3/envs/nn/lib/python3.8/site-packages (from pymongo<5.0.0,>=3.8.0->apache_beam) (2.6.1)\n",
            "Requirement already satisfied: async-timeout>=4.0.3 in /home/maruanottoni/miniconda3/envs/nn/lib/python3.8/site-packages (from redis<6,>=5.0.0->apache_beam) (4.0.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /home/maruanottoni/miniconda3/envs/nn/lib/python3.8/site-packages (from requests<3.0.0,>=2.24.0->apache_beam) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /home/maruanottoni/miniconda3/envs/nn/lib/python3.8/site-packages (from requests<3.0.0,>=2.24.0->apache_beam) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/maruanottoni/miniconda3/envs/nn/lib/python3.8/site-packages (from requests<3.0.0,>=2.24.0->apache_beam) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /home/maruanottoni/miniconda3/envs/nn/lib/python3.8/site-packages (from requests<3.0.0,>=2.24.0->apache_beam) (2024.8.30)\n",
            "Requirement already satisfied: zipp>=3.1.0 in /home/maruanottoni/miniconda3/envs/nn/lib/python3.8/site-packages (from importlib-resources>=1.4.0->jsonschema<5.0.0,>=4.0.0->apache_beam) (3.20.2)\n",
            "Installing collected packages: dill\n",
            "  Attempting uninstall: dill\n",
            "    Found existing installation: dill 0.3.8\n",
            "    Uninstalling dill-0.3.8:\n",
            "      Successfully uninstalled dill-0.3.8\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "multiprocess 0.70.16 requires dill>=0.3.8, but you have dill 0.3.1.1 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed dill-0.3.1.1\n"
          ]
        }
      ],
      "source": [
        "!pip install datasets transformers[sentencepiece,torch]\n",
        "!pip install apache_beam"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/maruanottoni/miniconda3/envs/nn/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "from datasets import load_dataset, DatasetDict\n",
        "\n",
        "from transformers import (\n",
        "    BertTokenizer,\n",
        "    BertForMaskedLM,\n",
        "    GPT2Tokenizer,\n",
        "    GPT2LMHeadModel,\n",
        "    DataCollatorForLanguageModeling,\n",
        "    AutoConfig,\n",
        "    AutoTokenizer,\n",
        "    Trainer,\n",
        "    TrainingArguments\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bGc8Si8CBbuI"
      },
      "source": [
        "## Understanding Masked LM's"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "99o28B2EGVh3"
      },
      "outputs": [],
      "source": [
        "## The first model we will look at is BERT, which is trained with masked tokens. As an example,\n",
        "## the text below masks the word \"box\" from a well-known movie quote.\n",
        "\n",
        "text = \"Life is like a [MASK] of chocolates.\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "ZVii0JgSBKpU"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "BertForMaskedLM has generative capabilities, as `prepare_inputs_for_generation` is explicitly overwritten. However, it doesn't directly inherit from `GenerationMixin`. From 👉v4.50👈 onwards, `PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.\n",
            "  - If you're using `trust_remote_code=True`, you can get rid of this warning by loading the model with an auto class. See https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes\n",
            "  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).\n",
            "  - If you are not the owner of the model architecture class, please contact the model code owner to update it.\n",
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
            "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        }
      ],
      "source": [
        "## We'll now see how BERT is able to predict the missing word. We can use HuggingFace to load\n",
        "## a copy of the pretrained model and tokenizer.\n",
        "\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "model = BertForMaskedLM.from_pretrained(\"bert-base-uncased\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "TWKpRFEFA2Oz"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "input_ids: tensor([[ 101, 2166, 2003, 2066, 1037,  103, 1997, 7967, 2015, 1012,  102]])\n",
            "attention_mask: tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])\n"
          ]
        }
      ],
      "source": [
        "## Next, we'll feed our example text into the tokenizer.\n",
        "\n",
        "encoded_input = tokenizer(text, return_tensors='pt')\n",
        "print('input_ids:', encoded_input['input_ids'])\n",
        "print('attention_mask:', encoded_input['attention_mask'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "o84muqUuLiwC"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "chocolate\n"
          ]
        }
      ],
      "source": [
        "## input_ids represents the tokenized output. Each integer can be mapped back to the corresponding string.\n",
        "\n",
        "print(tokenizer.decode([7967]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "1ut5R3IYbxxd"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "BertForMaskedLM(\n",
              "  (bert): BertModel(\n",
              "    (embeddings): BertEmbeddings(\n",
              "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
              "      (position_embeddings): Embedding(512, 768)\n",
              "      (token_type_embeddings): Embedding(2, 768)\n",
              "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (encoder): BertEncoder(\n",
              "      (layer): ModuleList(\n",
              "        (0-11): 12 x BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSdpaSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "  )\n",
              "  (cls): BertOnlyMLMHead(\n",
              "    (predictions): BertLMPredictionHead(\n",
              "      (transform): BertPredictionHeadTransform(\n",
              "        (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (transform_act_fn): GELUActivation()\n",
              "        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "      )\n",
              "      (decoder): Linear(in_features=768, out_features=30522, bias=True)\n",
              "    )\n",
              "  )\n",
              ")"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "## The model will then receive the output of the tokenizer. We can look at the BERT model to see exactly how\n",
        "## it was constructed and what the outputs will be like.\n",
        "\n",
        "model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "H-Xv2ujRAch4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([1, 11, 30522])\n"
          ]
        }
      ],
      "source": [
        "## The model starts with an embedding of each of the 30,522 possible tokens into 768 dimensions, which at this\n",
        "## point is simply a representation of each token without any additional information about their relationships\n",
        "## to one another in the text. Then the encoder attention blocks are applied, updating the embeddings such that\n",
        "## they now encode each token's contribution to the chunk of text and interactions with other tokens. Notably,\n",
        "## this includes the masked tokens as well. The final stage is the language model head, which takes the embeddings\n",
        "## from the masked positions back to 30,522 dimensions. Each index of this final vector corresponds to the\n",
        "## probability that the token in that position would be the correct choice to fill the mask.\n",
        "\n",
        "\n",
        "model_output = model(**encoded_input)\n",
        "output = model_output[\"logits\"]\n",
        "\n",
        "print(output.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "4hDXB-x5tzrh"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([30522])\n"
          ]
        }
      ],
      "source": [
        "tokens = encoded_input['input_ids'][0].tolist()\n",
        "masked_index = tokens.index(tokenizer.mask_token_id)\n",
        "logits = output[0, masked_index, :]\n",
        "\n",
        "print(logits.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "BaRYd_aVjeoR"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Top 5 predictions: box bag bowl jar cup\n",
            "tensor([0.1764, 0.1688, 0.0419, 0.0336, 0.0262], grad_fn=<TopkBackward0>)\n"
          ]
        }
      ],
      "source": [
        "probs = logits.softmax(dim=-1)\n",
        "values, predictions = probs.topk(5)\n",
        "sequence = tokenizer.decode(predictions)\n",
        "\n",
        "print('Top 5 predictions:', sequence)\n",
        "print(values)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1vVQ6Cqhuq9t"
      },
      "source": [
        "Printing the top 5 predictions and their respective scores, we see that BERT accurately chooses \"box\" as the most likely replacement for the mask token.\n",
        "\n",
        "## Understanding Causal LM's"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "azFAE8TVkPXv"
      },
      "outputs": [],
      "source": [
        "## We now repeat a similar exercise with the causal LLM GPT-2. This model generates\n",
        "## text following an input, instead of replacing a mask within the text.\n",
        "\n",
        "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
        "model = GPT2LMHeadModel.from_pretrained('gpt2')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "4TwmbE0bk1mC"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "GPT2LMHeadModel(\n",
              "  (transformer): GPT2Model(\n",
              "    (wte): Embedding(50257, 768)\n",
              "    (wpe): Embedding(1024, 768)\n",
              "    (drop): Dropout(p=0.1, inplace=False)\n",
              "    (h): ModuleList(\n",
              "      (0-11): 12 x GPT2Block(\n",
              "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        (attn): GPT2SdpaAttention(\n",
              "          (c_attn): Conv1D(nf=2304, nx=768)\n",
              "          (c_proj): Conv1D(nf=768, nx=768)\n",
              "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
              "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        (mlp): GPT2MLP(\n",
              "          (c_fc): Conv1D(nf=3072, nx=768)\n",
              "          (c_proj): Conv1D(nf=768, nx=3072)\n",
              "          (act): NewGELUActivation()\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "  )\n",
              "  (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
              ")"
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "## We can examine the model again, noting the similarities to BERT. An embedding, 12 attention blocks,\n",
        "## and a linear transformation bringing the output back to the size of the tokenizer. The tokenizer is\n",
        "## different from BERT so we see we have more tokens this time.\n",
        "\n",
        "model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "hwUCWU6TwTqZ"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'input_ids': tensor([[10462, 27428,   379,   262, 10481,   318]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1]])}"
            ]
          },
          "execution_count": 15,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "## We'll use a different text example, since this model works by producing tokens sequentially\n",
        "## rather than filling a mask.\n",
        "\n",
        "text = \"Swimming at the beach is\"\n",
        "model_inputs = tokenizer(text, return_tensors='pt')\n",
        "model_inputs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "Ua8YctwekqSm"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([257])\n"
          ]
        }
      ],
      "source": [
        "## After applying the model, the information needed to predict the next token is represented by\n",
        "## the last token. So we can access that vector by the index -1.\n",
        "\n",
        "output = model(**model_inputs)\n",
        "next_token_logits = output.logits[:, -1, :]\n",
        "next_token = torch.argmax(next_token_logits, dim=-1)\n",
        "print(next_token)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "OH05O28PnLW9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'input_ids': tensor([[10462, 27428,   379,   262, 10481,   318,   257]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1]])}\n"
          ]
        }
      ],
      "source": [
        "## Now add the new token to the end of the text, and feed all of it back to the model to continue\n",
        "## predicting more tokens.\n",
        "\n",
        "model_inputs['input_ids'] = torch.cat([model_inputs['input_ids'], next_token[:, None]], dim=-1)\n",
        "model_inputs[\"attention_mask\"] = torch.cat([model_inputs['attention_mask'], torch.tensor([[1]])], dim=-1)\n",
        "print(model_inputs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "-ja9Mk0DnpBe"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Swimming at the beach is a\n"
          ]
        }
      ],
      "source": [
        "## Here's what we have so far. The model added the word 'a' to the input text.\n",
        "\n",
        "print(tokenizer.decode(model_inputs['input_ids'][0]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "n-ygamSIoMKh"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Swimming at the beach is a great\n"
          ]
        }
      ],
      "source": [
        "## Repeating all the previous steps, we then add the word 'great'.\n",
        "\n",
        "output = model(**model_inputs)\n",
        "next_token_logits = output.logits[:, -1, :]\n",
        "next_token = torch.argmax(next_token_logits, dim=-1)\n",
        "model_inputs['input_ids'] = torch.cat([model_inputs['input_ids'], next_token[:, None]], dim=-1)\n",
        "model_inputs[\"attention_mask\"] = torch.cat([model_inputs['attention_mask'], torch.tensor([[1]])], dim=-1)\n",
        "print(tokenizer.decode(model_inputs['input_ids'][0]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "z9QIw--gIrh8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Swimming at the beach is a great way to get a little extra energy.\n",
            "\n",
            "The beach\n"
          ]
        }
      ],
      "source": [
        "## HuggingFace automates this iterative process. We'll use the quicker approach to finish our sentence.\n",
        "\n",
        "output_generate = model.generate(**model_inputs, max_length=20, pad_token_id=tokenizer.eos_token_id)\n",
        "print(tokenizer.decode(output_generate[0]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XL2rmSMVpkEu"
      },
      "source": [
        "## Pre-training a GPT-2 model from scratch\n",
        "\n",
        "Next we'll train a GPT-2 model from scratch using English Wikipedia data. Note that we're only using a tiny subset of the data to demonstrate that the model is capable of learning. The exact same approach could be followed on the full dataset to train a more functional model, but that would require a lot of compute."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "QTW0UmOJd7w2"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Downloading data: 100%|██████████| 41/41 [09:42<00:00, 14.20s/files]\n",
            "Generating train split: 100%|██████████| 6458670/6458670 [07:22<00:00, 14591.17 examples/s]\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "DatasetDict({\n",
              "    train: Dataset({\n",
              "        features: ['id', 'url', 'title', 'text'],\n",
              "        num_rows: 50\n",
              "    })\n",
              "    valid: Dataset({\n",
              "        features: ['id', 'url', 'title', 'text'],\n",
              "        num_rows: 50\n",
              "    })\n",
              "})"
            ]
          },
          "execution_count": 21,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "dataset = load_dataset(\"wikipedia\", \"20220301.en\")\n",
        "ds_shuffle = dataset['train'].shuffle()\n",
        "\n",
        "raw_datasets = DatasetDict(\n",
        "    {\n",
        "        \"train\": ds_shuffle.select(range(50)),\n",
        "        \"valid\": ds_shuffle.select(range(50, 100))\n",
        "    }\n",
        ")\n",
        "\n",
        "raw_datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "-qbgOvH3d7w2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Victoria Serena Pickett (born August 12, 1996) is a Filipino-Canadian soccer player who plays as a midfielder for Kansas City Current in the National Women's Soccer League (NWSL) and the Canada nation\n"
          ]
        }
      ],
      "source": [
        "print(raw_datasets['train'][0]['text'][:200])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "-HRmdiqbd7w3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Input IDs length: 7\n",
            "Input chunk lengths: [128, 128, 128, 103, 128, 128, 43]\n",
            "Chunk mapping: [0, 0, 0, 0, 1, 1, 1]\n"
          ]
        }
      ],
      "source": [
        "## We'll tokenize the text, setting the context size to 128 and thus breaking each document into chunks of 128 tokens.\n",
        "\n",
        "context_length = 128\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
        "\n",
        "outputs = tokenizer(\n",
        "    raw_datasets[\"train\"][:2][\"text\"],\n",
        "    truncation=True,\n",
        "    max_length=context_length,\n",
        "    return_overflowing_tokens=True,\n",
        "    return_length=True,\n",
        ")\n",
        "\n",
        "print(f\"Input IDs length: {len(outputs['input_ids'])}\")\n",
        "print(f\"Input chunk lengths: {(outputs['length'])}\")\n",
        "print(f\"Chunk mapping: {outputs['overflow_to_sample_mapping']}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "iWvoQ3-td7w3"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Map: 100%|██████████| 50/50 [00:00<00:00, 284.19 examples/s]\n",
            "Map: 100%|██████████| 50/50 [00:00<00:00, 244.39 examples/s]\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "DatasetDict({\n",
              "    train: Dataset({\n",
              "        features: ['input_ids'],\n",
              "        num_rows: 182\n",
              "    })\n",
              "    valid: Dataset({\n",
              "        features: ['input_ids'],\n",
              "        num_rows: 292\n",
              "    })\n",
              "})"
            ]
          },
          "execution_count": 24,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "def tokenize(element):\n",
        "    outputs = tokenizer(\n",
        "        element[\"text\"],\n",
        "        truncation=True,\n",
        "        max_length=context_length,\n",
        "        return_overflowing_tokens=True,\n",
        "        return_length=True,\n",
        "    )\n",
        "    input_batch = []\n",
        "    for length, input_ids in zip(outputs[\"length\"], outputs[\"input_ids\"]):\n",
        "        if length == context_length:\n",
        "            input_batch.append(input_ids)\n",
        "    return {\"input_ids\": input_batch}\n",
        "\n",
        "\n",
        "tokenized_datasets = raw_datasets.map(\n",
        "    tokenize, batched=True, remove_columns=raw_datasets[\"train\"].column_names\n",
        ")\n",
        "tokenized_datasets"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zfu1oJeQYiLa"
      },
      "source": [
        "Now we can set up the HuggingFace Trainer as follows. Since we're using such a small dataset, we'll need lots of epochs for the model to make progress because all of the parameters are randomly initialized at the outset. Typically, most LLM's are trained for only one epoch and more diverse examples."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "7jf0Le4Yd7w3"
      },
      "outputs": [],
      "source": [
        "config = AutoConfig.from_pretrained(\n",
        "    \"gpt2\",\n",
        "    vocab_size=len(tokenizer),\n",
        "    n_ctx=context_length,\n",
        "    bos_token_id=tokenizer.bos_token_id,\n",
        "    eos_token_id=tokenizer.eos_token_id,\n",
        ")\n",
        "\n",
        "model = GPT2LMHeadModel(config)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "O_OKDaPMd7w3"
      },
      "outputs": [],
      "source": [
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "data_collator = DataCollatorForLanguageModeling(tokenizer, mlm=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "Ex3MHkued7w4"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/maruanottoni/miniconda3/envs/nn/lib/python3.8/site-packages/transformers/training_args.py:1568: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
            "  warnings.warn(\n",
            "/home/maruanottoni/miniconda3/envs/nn/lib/python3.8/site-packages/torch/cuda/__init__.py:128: UserWarning: CUDA initialization: The NVIDIA driver on your system is too old (found version 11080). Please update your GPU driver by downloading and installing a new version from the URL: http://www.nvidia.com/Download/index.aspx Alternatively, go to: https://pytorch.org to install a PyTorch version that has been compiled with your version of the CUDA driver. (Triggered internally at ../c10/cuda/CUDAFunctions.cpp:108.)\n",
            "  return torch._C._cuda_getDeviceCount() > 0\n",
            "/tmp/ipykernel_18127/148413081.py:7: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
            "  trainer = Trainer(\n",
            "Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
          ]
        }
      ],
      "source": [
        "args = TrainingArguments(\n",
        "    output_dir=\"wiki-gpt2\",\n",
        "    evaluation_strategy=\"steps\",\n",
        "    num_train_epochs=100\n",
        ")\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    args=args,\n",
        "    data_collator=data_collator,\n",
        "    train_dataset=tokenized_datasets[\"train\"],\n",
        "    eval_dataset=tokenized_datasets[\"valid\"]\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "WIj0n1fbd7w4"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='1200' max='1200' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [1200/1200 2:17:04, Epoch 100/100]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>500</td>\n",
              "      <td>4.521200</td>\n",
              "      <td>8.916580</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1000</td>\n",
              "      <td>0.964600</td>\n",
              "      <td>9.287793</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": [
              "TrainOutput(global_step=1200, training_loss=2.340398515065511, metrics={'train_runtime': 8230.6977, 'train_samples_per_second': 2.211, 'train_steps_per_second': 0.146, 'total_flos': 1188878745600000.0, 'train_loss': 2.340398515065511, 'epoch': 100.0})"
            ]
          },
          "execution_count": 28,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "trainer.train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "S2avxzYR5U8Y"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='19' max='19' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [19/19 00:49]\n",
              "    </div>\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": [
              "{'eval_loss': 9.30921745300293,\n",
              " 'eval_runtime': 51.0598,\n",
              " 'eval_samples_per_second': 5.719,\n",
              " 'eval_steps_per_second': 0.372,\n",
              " 'epoch': 100.0}"
            ]
          },
          "execution_count": 29,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "trainer.evaluate()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UIkCLEFRc3h6"
      },
      "source": [
        "The training loss is low by the end, which means the model should perform very well on training examples it has seen. It does not generalize well to the validation set of course, since we deliberately overfit on a small train set.\n",
        "\n",
        "We can confirm with a couple of examples that were seen in training."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "x-cxmRLLsGgB"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Victoria Serena Pickett (born August 12, 1996) is a Filipino-\n"
          ]
        }
      ],
      "source": [
        "text = tokenizer.decode(tokenized_datasets[\"train\"][0]['input_ids'][:16])\n",
        "print(text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "bbk_WBhMrj_F"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([1, 16])\n"
          ]
        }
      ],
      "source": [
        "model_inputs = tokenizer(text, return_tensors='pt')\n",
        "print(model_inputs['input_ids'].shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "ipgkkqn4sEAp"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "tensor([[49898, 30175,  2616, 12346,  3087,   357,  6286,  2932,  1105,    11,\n",
              "          8235,     8,   318,   257, 35289,    12, 28203, 11783,  2137,   508,\n",
              "          5341,   355,   257, 18707,   329,  9470,  2254,  9236,   287,   262,\n",
              "          2351,  6926]])"
            ]
          },
          "execution_count": 32,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
        "model_inputs['input_ids'] = model_inputs['input_ids'].to(device)\n",
        "model_inputs['attention_mask'] = model_inputs['attention_mask'].to(device)\n",
        "\n",
        "output_generate = model.generate(**model_inputs, max_new_tokens=16)\n",
        "output_generate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "JHUbsd-hxdTL"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Victoria Serena Pickett (born August 12, 1996) is a Filipino-Canadian soccer player who plays as a midfielder for Kansas City Current in the National Women\n"
          ]
        }
      ],
      "source": [
        "sequence = tokenizer.decode(output_generate[0])\n",
        "print(sequence)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l04YrQLeghA5"
      },
      "source": [
        "The model should do quite well at reciting text after seeing it so many times. We can be convinced that the tokenizer, model architecture, and training objective are well-suited to learning Wikipedia data. For comparison, we'll try this model on text from the validation set."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "MFK5FC57hwS9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Charles II (29 May 1630 – 6 February 1685) was King of Scotland from 1649 until 1651, and King of Scotland, England and Ireland\n"
          ]
        }
      ],
      "source": [
        "text = tokenizer.decode(tokenized_datasets[\"valid\"][0]['input_ids'][:32])\n",
        "print(text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "-SlX_sfHfdTB"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Charles II (29 May 1630 – 6 February 1685) was King of Scotland from 1649 until 1651, and King of Scotland, England and Ireland also begun suffering with injury problems, also facing stiff left-back competition from younger\n"
          ]
        }
      ],
      "source": [
        "model_inputs = tokenizer(text, return_tensors='pt')\n",
        "\n",
        "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
        "model_inputs['input_ids'] = model_inputs['input_ids'].to(device)\n",
        "model_inputs['attention_mask'] = model_inputs['attention_mask'].to(device)\n",
        "\n",
        "output_generate = model.generate(**model_inputs, max_new_tokens=16)\n",
        "sequence = tokenizer.decode(output_generate[0])\n",
        "print(sequence)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "FassINSdfs9U"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'Charles II (29 May 1630 – 6 February 1685) was King of Scotland from 1649 until 1651, and King of Scotland, England and Ireland from the 1660 Restoration of the monarchy until his death in 1685.\\n\\nCharles II was the eldest surviving child of Charles I of England, Scotland and Ireland and Henrietta Maria of France. After Charles I\\'s execution at Whitehall on 30 January 1649, at the climax of the English Civil War, the Parliament of Scotland proclaimed Charles II king on 5 February 1649. But England  entered the period known as the English Interregnum or the English Commonwealth, and the country was a de facto republic led by Oliver Cromwell. Cromwell defeated Charles II at the Battle of Worcester on 3 September 1651, and Charles fled to mainland Europe. Cromwell became virtual dictator of England, Scotland and Ireland. Charles spent the next nine years in exile in France, the Dutch Republic and the Spanish Netherlands. The political crisis that followed Cromwell\\'s death in 1658 resulted in the restoration of the monarchy, and Charles was invited to return to Britain. On 29 May 1660, his 30th birthday, he was received in London to public acclaim. After 1660, all legal documents stating a regnal year did so as if he had succeeded his father as king in 1649.\\n\\nCharles\\'s English parliament enacted laws known as the Clarendon Code, designed to shore up the position of the re-established Church of England. Charles acquiesced to the Clarendon Code even though he favoured a policy of religious tolerance. The major foreign policy issue of his early reign was the Second Anglo-Dutch War. In 1670, he entered into the Treaty of Dover, an alliance with his cousin King Louis XIV of France. Louis agreed to aid him in the Third Anglo-Dutch War and pay him a pension, and Charles secretly promised to convert to Catholicism at an unspecified future date. Charles attempted to introduce religious freedom for Catholics and Protestant dissenters with his 1672 Royal Declaration of Indulgence, but the English Parliament forced him to withdraw it. In 1679, Titus Oates\\'s revelations of a supposed Popish Plot sparked the Exclusion Crisis when it was revealed that Charles\\'s brother and heir presumptive, James, Duke of York, had become a Roman Catholic. The crisis saw the birth of the pro-exclusion Whig and anti-exclusion Tory parties. Charles sided with the Tories, and after the discovery of the Rye House Plot to murder Charles and James in 1683, some Whig leaders were executed or forced into exile. Charles dissolved the English Parliament in 1681 and ruled alone until his death in 1685. He was allegedly received into the Catholic Church on his deathbed.\\n\\nTraditionally considered one of the most popular English kings, Charles is known as the Merry Monarch, a reference to the liveliness and hedonism of his court. He acknowledged at least 12 illegitimate children by various mistresses, but left no legitimate children and was succeeded by his brother, James.\\n\\nEarly life, civil war and exile \\n\\nCharles II was born at St James\\'s Palace on 29 May 1630. His parents were Charles I, who ruled the three kingdoms of England, Scotland and Ireland, and Henrietta Maria, the sister of the French king Louis XIII. Charles was their second child. Their first son was born about a year before Charles, but died within a day. England, Scotland, and Ireland were respectively predominantly Anglican, Presbyterian, and Catholic. Charles was baptised in the Chapel Royal, on 27 June, by the Anglican Bishop of London, William Laud. He was brought up in the care of the Protestant Countess of Dorset, though his godparents included his maternal uncle Louis XIII and his maternal grandmother, Marie de\\' Medici, the Dowager Queen of France, both of whom were Catholics. At birth, Charles automatically became Duke of Cornwall and Duke of Rothesay, along with several other associated titles. At or around his eighth birthday, he was designated Prince of Wales, though he was never formally invested.\\n\\nDuring the 1640s, when Charles was still young, his father fought Parliamentary and Puritan forces in the English Civil War. Charles accompanied his father during the Battle of Edgehill and, at the age of fourteen, participated in the campaigns of 1645, when he was made titular commander of the English forces in the West Country. By spring 1646, his father was losing the war, and Charles left England due to fears for his safety. Setting off from Falmouth after staying at Pendennis Castle, he went first to the Isles of Scilly, then to Jersey, and finally to France, where his mother was already living in exile and his first cousin, eight-year-old Louis XIV, was king. Charles I surrendered into captivity in May 1646.\\n\\nIn 1648, during the Second English Civil War, Charles moved to The Hague, where his sister Mary and his brother-in-law William II, Prince of Orange, seemed more likely to provide substantial aid to the royalist cause than his mother\\'s French relations. However, the royalist fleet that came under Charles\\'s control was not used to any advantage, and did not reach Scotland in time to join up with the royalist Engager army of the Duke of Hamilton before it was defeated at the Battle of Preston by the Parliamentarians.\\n\\nAt The Hague, Charles had a brief affair with Lucy Walter, who later falsely claimed that they had secretly married. Her son, James Crofts (afterwards Duke of Monmouth and Duke of Buccleuch), was one of Charles\\'s many illegitimate children who became prominent in British society.\\n\\nDespite his son\\'s diplomatic efforts to save him, King Charles I was beheaded in January 1649, and England became a republic. On 5 February, the Covenanter Parliament of Scotland proclaimed Charles II \"King of Great Britain, France and Ireland\" at the Mercat Cross, Edinburgh, but refused to allow him to enter Scotland unless he accepted the imposition of Presbyterianism throughout Britain and Ireland.\\n\\nWhen negotiations with the Scots stalled, Charles authorised General Montrose to land in the Orkney Islands with a small army to threaten the Scots with invasion, in the hope of forcing an agreement more to his liking. Montrose feared that Charles would accept a compromise, and so chose to invade mainland Scotland anyway. He was captured and executed. Charles reluctantly promised that he would abide by the terms of a treaty agreed between him and the Scots Parliament at Breda, and support the Solemn League and Covenant, which authorised Presbyterian church governance across Britain. Upon his arrival in Scotland on 23 June 1650, he formally agreed to the Covenant; his abandonment of Episcopal church governance, although winning him support in Scotland, left him unpopular in England. Charles himself soon came to despise the \"villainy\" and \"hypocrisy\" of the Covenanters. Charles was provided with a Scottish court, and the record of his food and household expenses at Falkland and Perth survives.\\n\\nOn 3 September 1650, the Covenanters were defeated at the Battle of Dunbar by a much smaller force led by Oliver Cromwell. The Scots forces were divided into royalist Engagers and Presbyterian Covenanters, who even fought each other. Disillusioned by the Covenanters, in October Charles attempted to escape from them and rode north to join with an Engager force, an event which became known as \"the Start\", but within two days the Presbyterians had caught up with and recovered him. Nevertheless, the Scots remained Charles\\'s best hope of restoration, and he was crowned King of Scotland at Scone Abbey on 1 January 1651. With Cromwell\\'s forces threatening Charles\\'s position in Scotland, it was decided to mount an attack on England. With many of the Scots (including Lord Argyll and other leading Covenanters) refusing to participate, and with few English royalists joining the force as it moved south into England, the invasion ended in defeat at the Battle of Worcester on 3 September 1651, after which Charles eluded capture by hiding in the Royal Oak at Boscobel House. Through six weeks of narrow escapes Charles managed to flee England in disguise, landing in Normandy on 16 October, despite a reward of £1,000 on his head, risk of death for anyone caught helping him and the difficulty in disguising Charles, who, at over , was unusually tall for the time.\\n\\nUnder the Instrument of Government passed by Parliament, Cromwell was appointed Lord Protector of England, Scotland and Ireland in 1653, effectively placing the British Isles under military rule. Charles lived a life of leisure at Saint-Germain-en-Laye near Paris, living on a grant from Louis XIV of 600 livres a month. Charles could not obtain sufficient finance or support to mount a serious challenge to Cromwell\\'s government. Despite the Stuart family connections through Henrietta Maria and the Princess of Orange, France and the Dutch Republic allied themselves with Cromwell\\'s government from 1654, forcing Charles to leave France and turn for aid to Spain, which at that time ruled the Southern Netherlands.\\n\\nCharles made the Treaty of Brussels with Spain in 1656. This gathered Spanish support for a restoration in return for Charles\\'s contribution to the war against France. Charles raised a ragtag army from his exiled subjects; this small, underpaid, poorly-equipped and ill-disciplined force formed the nucleus of the post-Restoration army. The Commonwealth made the Treaty of Paris with France in 1657 to join them in war against Spain in the Netherlands. Royalist supporters in the Spanish force were led by Charles\\'s younger brother James, Duke of York. At the Battle of the Dunes in 1658, as part of the larger Spanish force, Charles\\'s army of around 2,000 clashed with Commonwealth troops fighting with the French. By the end of the battle Charles\\'s force was about 1,000 and with Dunkirk given to the English the prospect of a Royalist expedition to England was dashed.\\n\\nRestoration \\n\\nAfter the death of Cromwell in 1658, Charles\\'s initial chances of regaining the Crown seemed slim; Cromwell was succeeded as Lord Protector by his son, Richard. However, the new Lord Protector had little experience of either military or civil administration. In 1659, the Rump Parliament was recalled and Richard resigned. During the civil and military unrest that followed, George Monck, the Governor of Scotland, was concerned that the nation would descend into anarchy. Monck and his army marched into the City of London, and forced the Rump Parliament to re-admit members of the Long Parliament who had been excluded in December 1648, during Pride\\'s Purge. The Long Parliament dissolved itself and there was a general election for the first time in almost 20 years. The outgoing Parliament defined the electoral qualifications intending to bring about the return of a Presbyterian majority.\\n\\nThe restrictions against royalist candidates and voters were widely ignored, and the elections resulted in a House of Commons that was fairly evenly divided on political grounds between Royalists and Parliamentarians and on religious grounds between Anglicans and Presbyterians. The new so-called Convention Parliament assembled on 25 April 1660, and soon afterwards welcomed the Declaration of Breda, in which Charles promised lenience and tolerance. There would be liberty of conscience and Anglican church policy would not be harsh. He would not exile past enemies nor confiscate their wealth. There would be pardons for nearly all his opponents except the regicides. Above all, Charles promised to rule in cooperation with Parliament. The English Parliament resolved to proclaim Charles king and invite him to return, a message that reached Charles at Breda on 8 May 1660. In Ireland, a convention had been called earlier in the year, and had already declared for Charles. On 14 May, he was proclaimed king in Dublin.\\n\\nHe set out for England from Scheveningen, arrived in Dover on 25 May 1660 and reached London on 29 May, his 30th birthday. Although Charles and Parliament granted amnesty to nearly all of Cromwell\\'s supporters in the Act of Indemnity and Oblivion, 50 people were specifically excluded. In the end nine of the regicides were executed: they were hanged, drawn and quartered, whereas others were given life imprisonment or simply excluded from office for life. The bodies of Oliver Cromwell, Henry Ireton and John Bradshaw were subjected to the indignity of posthumous decapitations.\\n\\nThe English Parliament granted him an annual income to run the government of £1.2\\xa0million, generated largely from customs and excise duties. The grant, however, proved to be insufficient for most of Charles\\'s reign. For the most part, the actual revenue was much lower, which led to attempts to economise at court by reducing the size and expenses of the royal household and raise money through unpopular innovations such as the hearth tax.\\n\\nIn the latter half of 1660, Charles\\'s joy at the Restoration was tempered by the deaths of his youngest brother, Henry, and sister, Mary, of smallpox. At around the same time, Anne Hyde, the daughter of the Lord Chancellor, Edward Hyde, revealed that she was pregnant by Charles\\'s brother, James, whom she had secretly married. Edward Hyde, who had not known of either the marriage or the pregnancy, was created Earl of Clarendon and his position as Charles\\'s favourite minister was strengthened.\\n\\nClarendon Code \\n\\nThe Convention Parliament was dissolved in December 1660, and, shortly after the coronation, the second English Parliament of the reign assembled. Dubbed the Cavalier Parliament, it was overwhelmingly Royalist and Anglican. It sought to discourage non-conformity to the Church of England and passed several acts to secure Anglican dominance. The Corporation Act 1661 required municipal officeholders to swear allegiance; the Act of Uniformity 1662 made the use of the Anglican Book of Common Prayer compulsory; the Conventicle Act 1664 prohibited religious assemblies of more than five people, except under the auspices of the Church of England; and the Five Mile Act 1665 prohibited expelled non-conforming clergymen from coming within five\\xa0miles (8\\xa0km) of a parish from which they had been banished. The Conventicle and Five Mile Acts remained in effect for the remainder of Charles\\'s reign. The Acts became known as the Clarendon Code, after Lord Clarendon, even though he was not directly responsible for them and even spoke against the Five Mile Act.\\n\\nThe Restoration was accompanied by social change. Puritanism lost its momentum. Theatres reopened after having been closed during the protectorship of Oliver Cromwell, and bawdy \"Restoration comedy\" became a recognisable genre. Theatre licences granted by Charles required that female parts be played by \"their natural performers\", rather than by boys as was often the practice before; and Restoration literature celebrated or reacted to the restored court, which included libertines such as John Wilmot, 2nd Earl of Rochester. Of Charles II, Wilmot supposedly said:\\n\\nTo which Charles is reputed to have replied \"that the matter was easily accounted for: For that his discourse was his own, his actions were the ministry\\'s\".\\n\\nGreat Plague and Great Fire \\nIn 1665, Charles was faced with a great health crisis: the Great Plague of London. The death toll reached a peak of 7,000 per week in the week of 17 September. Charles, with his family and court, fled London in July to Salisbury; Parliament met in Oxford. Plague cases ebbed over the winter, and Charles returned to London in February 1666.\\n\\nAfter a long spell of hot and dry weather through mid-1666, what later became known as the Great Fire of London started on 2 September 1666 in a bakehouse on Pudding Lane. Fanned by a strong easterly wind and fed by stockpiles of wood and fuel that had been prepared for the coming colder months, the fire eventually consumed about 13,200 houses and 87 churches, including St Paul\\'s Cathedral. Charles and his brother James joined and directed the fire-fighting effort. The public blamed Catholic conspirators for the fire, and one Frenchman, Robert Hubert, was hanged on the basis of a false confession even though he had no hand in starting the fire.\\n\\nForeign policy and marriage \\n\\nSince 1640, Portugal had been fighting a war against Spain to restore its independence after a dynastic union of sixty years between the crowns of Spain and Portugal. Portugal had been helped by France, but in the Treaty of the Pyrenees in 1659 Portugal was abandoned by its French ally. Negotiations with Portugal for Charles\\'s marriage to Catherine of Braganza began during his father\\'s reign and upon the restoration, Queen Luísa of Portugal, acting as regent, reopened negotiations with England that resulted in an alliance. On 23 June 1661, a marriage treaty was signed; England acquired Catherine\\'s dowry of Tangier (in North Africa) and the Seven islands of Bombay (the latter having a major influence on the development of the British Empire in India), together with trading privileges in Brazil and the East Indies, religious and commercial freedom in Portugal and two million Portuguese crowns (about £300,000); while Portugal obtained military and naval support against Spain and liberty of worship for Catherine. Catherine journeyed from Portugal to Portsmouth on 13–14 May 1662, but was not visited by Charles there until 20 May. The next day the couple were married at Portsmouth in two ceremonies—a Catholic one conducted in secret, followed by a public Anglican service.\\n\\nThe same year, in an unpopular move, Charles sold Dunkirk to his first cousin King Louis XIV of France for about £375,000. The channel port, although a valuable strategic outpost, was a drain on Charles\\'s limited finances.\\n\\nBefore Charles\\'s restoration, the Navigation Acts of 1650 had hurt Dutch trade by giving English vessels a monopoly, and had started the First Dutch War (1652–1654). To lay foundations for a new beginning, envoys of the States General appeared in November 1660 with the Dutch Gift. The Second Dutch War (1665–1667) was started by English attempts to muscle in on Dutch possessions in Africa and North America. The conflict began well for the English, with the capture of New Amsterdam (renamed New York in honour of Charles\\'s brother James, Duke of York) and a victory at the Battle of Lowestoft, but in 1667 the Dutch launched a surprise attack on England (the Raid on the Medway) when they sailed up the River Thames to where a major part of the English fleet was docked. Almost all of the ships were sunk except for the flagship, Royal Charles, which was taken back to the Netherlands as a prize. The Second Dutch War ended with the signing of the Treaty of Breda.\\n\\nAs a result of the Second Dutch War, Charles dismissed Lord Clarendon, whom he used as a scapegoat for the war. Clarendon fled to France when impeached for high treason (which carried the penalty of death). Power passed to five politicians known collectively by a whimsical acronym as the Cabal—Clifford, Arlington, Buckingham, Ashley (afterwards Earl of Shaftesbury) and Lauderdale. In fact, the Cabal rarely acted in concert, and the court was often divided between two factions led by Arlington and Buckingham, with Arlington the more successful.\\n\\nIn 1668, England allied itself with Sweden, and with its former enemy the Netherlands, to oppose Louis XIV in the War of Devolution. Louis made peace with the Triple Alliance, but he continued to maintain his aggressive intentions towards the Netherlands. In 1670, Charles, seeking to solve his financial troubles, agreed to the Treaty of Dover, under which Louis XIV would pay him £160,000 each year. In exchange, Charles agreed to supply Louis with troops and to announce his conversion to Catholicism \"as soon as the welfare of his kingdom will permit\". Louis was to provide him with 6,000 troops to suppress those who opposed the conversion. Charles endeavoured to ensure that the Treaty—especially the conversion clause—remained secret. It remains unclear if Charles ever seriously intended to convert.\\n\\nMeanwhile, by a series of five charters, Charles granted the East India Company the rights to autonomous government of its territorial acquisitions, to mint money, to command fortresses and troops, to form alliances, to make war and peace, and to exercise both civil and criminal jurisdiction over its possessions in the Indies. Earlier in 1668 he leased the islands of Bombay to the company for a nominal sum of £10 paid in gold. The Portuguese territories that Catherine brought with her as a dowry proved too expensive to maintain; Tangier was abandoned in 1684. In 1670, Charles granted control of the entire Hudson Bay drainage basin to the Hudson\\'s Bay Company by royal charter, and named the territory Rupert\\'s Land, after his cousin Prince Rupert of the Rhine, the company\\'s first governor.\\n\\nConflict with Parliament \\nAlthough previously favourable to the Crown, the Cavalier Parliament was alienated by the king\\'s wars and religious policies during the 1670s. In 1672, Charles issued the Royal Declaration of Indulgence, in which he purported to suspend all penal laws against Catholics and other religious dissenters. In the same year, he openly supported Catholic France and started the Third Anglo-Dutch War.\\n\\nThe Cavalier Parliament opposed the Declaration of Indulgence on constitutional grounds by claiming that the king had no right to arbitrarily suspend laws passed by Parliament. Charles withdrew the Declaration, and also agreed to the Test Act, which not only required public officials to receive the sacrament under the forms prescribed by the Church of England, but also later forced them to denounce transubstantiation and the Catholic Mass as \"superstitious and idolatrous\". Clifford, who had converted to Catholicism, resigned rather than take the oath, and died shortly after, possibly from suicide. \\n\\nBy 1674 England had gained nothing from the Anglo-Dutch War, and the Cavalier Parliament refused to provide further funds, forcing Charles to make peace. The power of the Cabal waned and that of Clifford\\'s replacement, Lord Danby grew, as did opposition towards him and the court. Politicians and peers believed that Charles II favoured a pro-French foreign policy that desired to emulate the absolutist (and Catholic) sovereignty of Louis XIV. In numerous pamphlets and parliamentary speeches between 1675 and 1678, \"popery and arbitrary government\" were decried for fear of the loss of English liberties and freedoms.\\n\\nCharles\\'s wife Queen Catherine was unable to produce an heir; her four pregnancies had ended in miscarriages and stillbirths in 1662, February 1666, May 1668 and June 1669. Charles\\'s heir presumptive was therefore his unpopular Catholic brother, James, Duke of York. Partly to assuage public fears that the royal family was too Catholic, Charles agreed that James\\'s daughter, Mary, should marry the Protestant William of Orange. In 1678, Titus Oates, who had been alternately an Anglican and Jesuit priest, falsely warned of a \"Popish Plot\" to assassinate the king, even accusing the queen of complicity. Charles did not believe the allegations, but ordered his chief minister Lord Danby to investigate. While Danby seems to have been rightly sceptical about Oates\\'s claims, the Cavalier Parliament took them seriously. The people were seized with an anti-Catholic hysteria; judges and juries across the land condemned the supposed conspirators; numerous innocent individuals were executed.\\n\\nLater in 1678, Danby was impeached by the House of Commons on the charge of high treason. Although much of the nation had sought war with Catholic France, Charles had secretly negotiated with Louis XIV, trying to reach an agreement under which England would remain neutral in return for money. Danby had publicly professed that he was hostile to France, but had reservedly agreed to abide by Charles\\'s wishes. Unfortunately for him, the House of Commons failed to view him as a reluctant participant in the scandal, instead believing that he was the author of the policy. To save Danby from the impeachment trial, Charles dissolved the Cavalier Parliament in January 1679.\\n\\nThe new English Parliament, which met in March of the same year, was quite hostile to Charles. Many members feared that he had intended to use the standing army to suppress dissent or impose Catholicism. However, with insufficient funds voted by Parliament, Charles was forced to gradually disband his troops. Having lost the support of Parliament, Danby resigned his post of Lord High Treasurer, but received a pardon from the king. In defiance of the royal will, the House of Commons declared that the dissolution of Parliament did not interrupt impeachment proceedings, and that the pardon was therefore invalid. When the House of Lords attempted to impose the punishment of exile—which the Commons thought too mild—the impeachment became stalled between the two Houses. As he had been required to do so many times during his reign, Charles bowed to the wishes of his opponents, committing Danby to the Tower of London, in which he was held for another five years.\\n\\nScience \\n\\nIn Charles II\\'s early childhood, William Cavendish, Earl of Newcastle was governor of the royal household and Brian Duppa, the Dean of Christchurch, was his tutor. Neither man thought that the study of science subjects was appropriate for a future king, and Newcastle even advised against studying any subject too seriously. However, as Charles grew older, the renowned surgeon William Harvey was appointed his tutor. He was famous for his work on blood circulation in the human body and already held the position of physician to Charles I; his studies were to influence Charles\\'s own attitude to science. As the king\\'s chief physician, Harvey accompanied Charles I to the Battle of Edgehill.  There, in the morning, he was placed in charge of the two princes, Charles and his brother James, but the boys were back with their father for the start of the battle.\\n\\nIn exile, Charles continued his education, including physics, chemistry and the mathematics of navigation. His tutors included the cleric John Earle, well known for his satirical book Microcosmographie, with whom he studied Latin and Greek, and Thomas Hobbes, the philosopher and author of Leviathon, with whom he studied mathematics. Even though some of his studies and experiments may have been a way of passing the time, by the time Charles returned to England he was already knowledgeable in the mathematics of navigation and was a competent chemist. The new concepts and discoveries being found at this time fascinated Charles. Soon after his coronation he had a sundial and 35\\' long telescope installed in the Privy garden.\\n\\nFrom the 1640s a group of scientists began to meet informally in Wadham College in Oxford or at Gresham College in London. At that time, free lectures were already being given each week at Gresham College, on a variety of topics, and the new group wished to give a more academic and learned approach to science and to conduct experiments in physics and mathematics. Included in this group were Harvey, Christopher Wren, Robert Hooke and Robert Boyle. Activities almost ceased during the civil war but following the Restoration, it was revived and Charles agreed to give it royal patronage as the Royal Society in 1662. Hooke was appointed as a salaried Curator of Experiments and organised scientific demonstrations on a regular basis, helped by a laboratory assistant. Charles was aware of Hooke\\'s weekly demonstrations and, in July 1663, to the Society\\'s consternation, he threatened to attend in person. Wren was consulted for advice, to ensure that the display would be appropriate for the king. In the event, Charles never visited the society, although his cousin Prince Rupert did.\\n\\nAs time passed, Charles lost interest in the activities of the society and left it to its own devices, but he continued to support scientific and commercial endeavours. He founded the Mathematical School at Christ\\'s Hospital in 1673 and, two years later, following concerns over French advances in astronomy, he founded the Royal Observatory at Greenwich. He maintained an interest in chemistry and had a private laboratory. There, dissections were occasionally carried out, and observed by the king. Samuel Pepys noted in his diary that on the morning of Friday, 15 January 1669, while he was walking to Whitehall, he met the king who invited him to view his chemistry laboratory. Pepys\\'s scientific knowledge was not great and he confessed to finding what he saw there beyond him.\\n\\nCharles developed painful gout in later life which limited the daily walks that he took regularly when younger. His keenness was now channelled to his laboratory where he would devote himself to his experiments, for hours at a time. Charles became particularly obsessed with mercury and often spent whole mornings attempting to distill it. Unfortunately, heating mercury in an open crucible releases mercury vapour, which is toxic and may have contributed to his later ill health.\\n\\nLater years \\nCharles faced a political storm over his brother James, a Catholic, being next in line to the throne. The prospect of a Catholic monarch was vehemently opposed by Anthony Ashley Cooper, 1st Earl of Shaftesbury (previously Baron Ashley and a member of the Cabal, which had fallen apart in 1673). Shaftesbury\\'s power base was strengthened when the House of Commons of 1679 introduced the Exclusion Bill, which sought to exclude the Duke of York from the line of succession. Some even sought to confer the Crown on the Protestant Duke of Monmouth, the eldest of Charles\\'s illegitimate children. The Abhorrers—those who thought the Exclusion Bill was abhorrent—were named Tories (after a term for dispossessed Irish Catholic bandits), while the Petitioners—those who supported a petitioning campaign in favour of the Exclusion Bill—were called Whigs (after a term for rebellious Scottish Presbyterians).\\n\\nAbsolute monarch\\nFearing that the Exclusion Bill would be passed, and bolstered by some acquittals in the continuing Plot trials, which seemed to him to indicate a more favourable public mood towards Catholicism, Charles dissolved the English Parliament, for a second time that year, in mid-1679. Charles\\'s hopes for a more moderate Parliament were not fulfilled; within a few months he had dissolved Parliament yet again, after it sought to pass the Exclusion Bill. When a new Parliament assembled at Oxford in March 1681, Charles dissolved it for a fourth time after just a few days. During the 1680s, however, popular support for the Exclusion Bill ebbed, and Charles experienced a nationwide surge of loyalty. Lord Shaftesbury was prosecuted (albeit unsuccessfully) for treason in 1681 and later fled to Holland, where he died. For the remainder of his reign, Charles ruled without Parliament.\\n\\nCharles\\'s opposition to the Exclusion Bill angered some Protestants. Protestant conspirators formulated the Rye House Plot, a plan to murder him and the Duke of York as they returned to London after horse races in Newmarket. A great fire, however, destroyed Charles\\'s lodgings at Newmarket, which forced him to leave the races early, thus inadvertently avoiding the planned attack. News of the failed plot was leaked. Protestant politicians such as the Earl of Essex, Algernon Sydney, Lord Russell and the Duke of Monmouth were implicated in the plot. Essex slit his own throat while imprisoned in the Tower of London; Sydney and Russell were executed for high treason on very flimsy evidence; and the Duke of Monmouth went into exile at the court of William of Orange. Lord Danby and the surviving Catholic lords held in the Tower were released and the king\\'s Catholic brother, James, acquired greater influence at court. Titus Oates was convicted and imprisoned for defamation.\\n\\nThus through the last years of Charles\\'s reign, his approach towards his opponents changed, and he was compared by Whigs to the contemporary Louis XIV of France, with his form of government in those years termed \"slavery\". Many of them were prosecuted and their estates seized, with Charles replacing judges and sheriffs at will and packing juries to achieve conviction. To destroy opposition in London, Charles first disenfranchised many Whigs in the 1682 municipal elections, and in 1683 the London charter was forfeited. In retrospect, the use of the judicial system by Charles (and later his brother and heir James) as a tool against opposition, helped establish the idea of separation of powers between the judiciary and the Crown in Whig thought.\\n\\nDeath \\nCharles suffered a sudden apoplectic fit on the morning of 2 February 1685, and died aged 54 at 11:45\\xa0am, four days later, at the Palace of Whitehall. The suddenness of his illness and death led to suspicion of poison in the minds of many, including one of the royal doctors; however, a more modern medical analysis has held that the symptoms of his final illness are similar to those of uraemia (a clinical syndrome due to kidney dysfunction). Charles had a laboratory among his many interests, where prior to his illness he had been experimenting with mercury. Mercuric poisoning can produce irreversible kidney damage; but the case for this being a cause of his death is unproven. In the days between his collapse and his death, Charles endured a variety of torturous treatments including bloodletting, purging and cupping in hopes of effecting a recovery, which may have exacerbated his uraemia through dehydration instead of helping alleviate it.\\n\\nOn his deathbed Charles asked his brother, James, to look after his mistresses: \"be well to Portsmouth, and let not poor Nelly starve\". He told his courtiers, \"I am sorry, gentlemen, for being such a time a-dying\", and expressed regret at his treatment of his wife. On the last evening of his life he was received into the Catholic Church in the presence of Father John Huddleston, though the extent to which he was fully conscious or committed, and with whom the idea originated, is unclear. He was buried in Westminster Abbey \"without any manner of pomp\" on 14 February.\\n\\nCharles was succeeded by his brother James II and VII.\\n\\nLegacy \\n\\nThe escapades of Charles after his defeat at the Battle of Worcester remained important to him throughout his life. He delighted and bored listeners with tales of his escape for many years. Numerous accounts of his adventures were published, particularly in the immediate aftermath of the Restoration. Though not averse to his escape being ascribed to divine providence, Charles himself seems to have delighted most in his ability to sustain his disguise as a man of ordinary origins, and to move unrecognised through his realm. Ironic and cynical, Charles took pleasure in retailing stories which demonstrated the undetectable nature of any inherent majesty he possessed.\\n\\nCharles had no legitimate children, but acknowledged a dozen by seven mistresses, including five by Barbara Villiers, Lady Castlemaine, for whom the Dukedom of Cleveland was created. His other mistresses included Moll Davis, Nell Gwyn, Elizabeth Killigrew, Catherine Pegge, Lucy Walter and Louise de Kérouaille, Duchess of Portsmouth. As a result, in his lifetime he was often nicknamed \"Old Rowley\", the name of his favourite racehorse, notable as a stallion.\\n\\nHis subjects resented paying taxes that were spent on his mistresses and their children, many of whom received dukedoms or earldoms. The present Dukes of Buccleuch, Richmond, Grafton and St Albans descend from Charles in unbroken male line. Diana, Princess of Wales, was descended from two of Charles\\'s illegitimate sons: the Dukes of Grafton and Richmond. Diana\\'s son, Prince William, Duke of Cambridge, second in line to the British throne, is likely to be the first British monarch descended from Charles II.\\n\\nCharles\\'s eldest son, the Duke of Monmouth, led a rebellion against James II, but was defeated at the Battle of Sedgemoor on 6 July 1685, captured and executed. James was eventually dethroned in 1688, in the course of the Glorious Revolution.\\n\\nLooking back on Charles\\'s reign, Tories tended to view it as a time of benevolent monarchy whereas Whigs perceived it as a terrible despotism. Today it is possible to assess him without the taint of partisanship, and he is seen as more of a lovable rogue—in the words of his contemporary John Evelyn, \"a prince of many virtues and many great imperfections, debonair, easy of access, not bloody or cruel\". John Wilmot, 2nd Earl of Rochester, wrote more lewdly of Charles:\\n\\nProfessor Ronald Hutton summarises the polarised historiography:\\n\\nHutton says Charles was a popular king in his own day and a \"legendary figure\" in British history.\\n\\nThe anniversary of the Restoration (which was also Charles\\'s birthday)—29 May—was recognised in England until the mid-nineteenth century as Oak Apple Day, after the Royal Oak in which Charles hid during his escape from the forces of Oliver Cromwell. Traditional celebrations involved the wearing of oak leaves but these have now died out. Charles II is depicted extensively in art, literature and media. Charleston, South Carolina, and South Kingstown, Rhode Island, are named after him.\\n\\nTitles, styles, honours and arms\\n\\nTitles and styles \\nThe official style of Charles II was \"Charles the Second, by the Grace of God, King of England, Scotland, France and Ireland, Defender of the Faith, etc.\" The claim to France was only nominal, and had been asserted by every English monarch since Edward III, regardless of the amount of French territory actually controlled.\\n\\nHonours \\n KG: Knight of the Garter, 21 May 1638\\n\\nArms \\nCharles\\'s coat of arms as Prince of Wales was the royal arms (which he later inherited), differenced by a label of three points Argent. His arms as monarch were: Quarterly, I and IV Grandquarterly, Azure three fleurs-de-lis Or (for France) and Gules three lions passant guardant in pale Or (for England); II Or a lion rampant within a double tressure flory-counter-flory Gules (for Scotland); III Azure a harp Or stringed Argent (for Ireland).\\n\\nIssue\\nBy Lucy Walter (c. 1630 – 1658):\\n James Crofts, later Scott (1649–1685), created Duke of Monmouth (1663) in England and Duke of Buccleuch (1663) in Scotland. Monmouth was born nine months after Walter and Charles II first met, and was acknowledged as his son by Charles II, but James II suggested that he was the son of another of her lovers, Colonel Robert Sidney, rather than Charles. Lucy Walter had a daughter, Mary Crofts, born after James in 1651, but Charles II was not the father, since he and Walter parted in September 1649.\\n\\nBy Elizabeth Killigrew (1622–1680), daughter of Sir Robert Killigrew, married Francis Boyle, 1st Viscount Shannon, in 1660:\\n Charlotte Jemima Henrietta Maria FitzRoy (1650–1684), married firstly James Howard and secondly William Paston, 2nd Earl of Yarmouth\\n\\nBy Catherine Pegge:\\n Charles FitzCharles (1657–1680), known as \"Don Carlo\", created Earl of Plymouth (1675)\\n Catherine FitzCharles (born 1658; she either died young or became a nun at Dunkirk)\\n\\nBy Barbara Villiers (1641–1709), wife of Roger Palmer, 1st Earl of Castlemaine, and created Duchess of Cleveland in her own right:\\n Lady Anne Palmer (Fitzroy) (1661–1722), married Thomas Lennard, 1st Earl of Sussex. She may have been the daughter of Roger Palmer, but Charles accepted her.\\n Charles Fitzroy (1662–1730), created Duke of Southampton (1675), became 2nd Duke of Cleveland (1709)\\n Henry Fitzroy (1663–1690), created Earl of Euston (1672), Duke of Grafton (1675)\\n Charlotte Fitzroy (1664–1717), married Edward Lee, 1st Earl of Lichfield\\n George Fitzroy (1665–1716), created Earl of Northumberland (1674), Duke of Northumberland (1678)\\n (Barbara (Benedicta) Fitzroy (1672–1737) – She was probably the child of John Churchill, later Duke of Marlborough, who was another of Cleveland\\'s many lovers, and was never acknowledged by Charles as his own daughter.)\\n\\nBy Nell Gwyn (1650–1687):\\n Charles Beauclerk (1670–1726), created Duke of St Albans (1684)\\n James, Lord Beauclerk (1671–1680)\\n\\nBy Louise Renée de Penancoet de Kérouaille (1649–1734), created Duchess of Portsmouth in her own right (1673):\\n Charles Lennox (1672–1723), created Duke of Richmond (1675) in England and Duke of Lennox (1675) in Scotland.\\n\\nBy Mary \\'Moll\\' Davis, courtesan and actress of repute:\\n Lady Mary Tudor (1673–1726), married Edward Radclyffe, 2nd Earl of Derwentwater; after Edward\\'s death, she married Henry Graham (of Levens), and upon his death she married James Rooke.\\n\\nOther probable mistresses include:\\n Christabella Wyndham\\n Hortense Mancini, Duchess of Mazarin\\n Winifred Wells – one of Queen Catherine\\'s Maids of Honour\\n Jane Roberts – the daughter of a clergyman\\n Mrs Knight – a famous singer\\n Elizabeth Berkeley, née Bagot, Dowager Countess of Falmouth – the widow of Charles Berkeley, 1st Earl of Falmouth\\n Elizabeth Fitzgerald, Countess of Kildare\\nLetters claiming that Marguerite or Margaret de Carteret bore Charles a son named James de la Cloche in 1646 are dismissed by historians as forgeries.\\n\\nGenealogical table\\n\\nNotes\\n\\nReferences\\n\\nBibliography\\n\\nFurther reading\\n\\nExternal links \\n\\n \\n \\n\\n|-\\n\\n|-\\n\\n|-\\n\\n \\n1630 births\\n1685 deaths\\n17th-century English monarchs\\n17th-century Scottish monarchs\\n17th-century Irish monarchs\\n17th-century English nobility\\n17th-century Scottish peers\\nBritish expatriates in the Dutch Republic\\nBurials at Westminster Abbey\\nConverts to Roman Catholicism from Anglicanism\\nDukes of Cornwall\\nDukes of Rothesay\\nEnglish pretenders to the French throne\\nEnglish Roman Catholics\\nFellows of the Royal Society\\nHouse of Stuart\\nHigh Stewards of Scotland\\nKnights of the Garter\\nPeople from Westminster\\nPeople of the English Civil War\\nPrinces of England\\nPrinces of Scotland\\nPrinces of Wales\\nLord High Admirals of England\\nChildren of Charles I of England'"
            ]
          },
          "execution_count": 36,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "raw_datasets['valid'][0]['text']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4ivQP0XciLl1"
      },
      "source": [
        "As expected, our model is completely confused this time. We'd need to train for much longer, and on much more diverse data, before we would have a model that can sensibly complete prompts it has never seen before. This is precisely why pre-training is such an important and powerful technique. If we had to train on all of Wikipedia for every NLP application to achieve optimal performance, it would be prohibitively expensive. But there's no need to do that when we can share and reuse existing pre-trained models as we did in the first part of this tutorial."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "V100",
      "include_colab_link": true,
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.20"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
